import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import plotly.graph_objects as go
import math
import numpy as np
import time

from threading import Thread
from matplotlib.animation import FuncAnimation
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten
from keras.optimizers import Adam, SGD, RMSprop
from keras.datasets import mnist, fashion_mnist, cifar10
from keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Config

def new_line():
    st.write("\n")
 
# with st.sidebar:
#    st.image("./assets/sb-quick.png",  use_container_width=True)


st.markdown("<h1 style='text-align: center; '>丘멡euraTrain游</h1>", unsafe_allow_html=True)
st.markdown("游빏Modelo de clasificaci칩n de imagenes", unsafe_allow_html=True)
st.markdown("游녤游낕Utiliza el panel lateral para comenzar a crear tu red neuronal artifical!", unsafe_allow_html=True)
st.divider()


# Inicializar estado de la sesi칩n
if 'graph_positions' not in st.session_state:
    st.session_state['graph_positions'] = None
if 'graph' not in st.session_state:
    st.session_state['graph'] = None
if 'layer_config' not in st.session_state:
    st.session_state['layer_config'] = []
if 'hyperparams' not in st.session_state:
    st.session_state['hyperparams'] = {
        'optimizer': 'Adam',
        'learning_rate': 0.001,
        'epochs': 5,
        'batch_size': 32,
        'loss_function': 'categorical_crossentropy'
    }
if 'logs' not in st.session_state:
    st.session_state['logs'] = []
if 'training_in_progress' not in st.session_state:
    st.session_state['training_in_progress'] = False
if 'selected_dataset' not in st.session_state:
    st.session_state['selected_dataset'] = 'MNIST'
if 'training_finished' not in st.session_state:
    st.session_state['training_finished'] = False
if 'modelDownload' not in st.session_state:
    st.session_state['modelDownload'] = None
# Inicializar estado de la sesi칩n para el bot칩n de m칠tricas
if 'show_metrics' not in st.session_state:
    st.session_state['show_metrics'] = False    
if 'train_ratio' not in st.session_state:
    st.session_state['train_ratio'] = 80  # Valor predeterminado para el entrenamiento
if 'val_ratio' not in st.session_state:
    st.session_state['val_ratio'] = 10  # Valor predeterminado para la validaci칩n
if 'test_ratio' not in st.session_state:
    st.session_state['test_ratio'] = 10  # Valor predeterminado para la prueba
if 'shuffle_data' not in st.session_state:
    st.session_state['shuffle_data'] = True  # Valor predeterminado para aleatorizar datos




# Inyectar CSS para capturar el color de fondo
st.markdown(
    """
    <style>
    .stApp {
        background-color: var(--background-color);
    }
    </style>
    """,
    unsafe_allow_html=True
)


# Capturar el color de fondo actual
def get_background_color():
    # Verificar si es modo claro o oscuro
    return st.session_state.get("background_color", "#ffffff")




# Mostrar gr치fico y bot칩n para entrenar
preview_placeholder = st.empty()
dynamic_placeholder = st.empty()

# Funci칩n para inicializar configuraci칩n de capas seg칰n el dataset
def initialize_layer_config(dataset):
    # Verificar si el dataset ya fue inicializado
    if "initialized_dataset" not in st.session_state or st.session_state["initialized_dataset"] != dataset:
        st.session_state["initialized_dataset"] = dataset  # Marcar el dataset como inicializado

        if dataset == 'CIFAR-10':
            # Capas base espec칤ficas de CIFAR-10
            st.session_state['layer_config'] = [
                {"name": "Conv2D1", "type": "Conv2D", "filters": 32, "kernel_size": 3, "activation": "relu", "base": True},
                {"name": "Conv2D2", "type": "Conv2D", "filters": 64, "kernel_size": 3, "activation": "relu", "base": True},
                {"name": "MaxPooling2D1", "type": "MaxPooling2D", "pool_size": 2, "base": True},
                {"name": "Flatten1", "type": "Flatten", "base": True},
                {"name": "Dense1", "type": "Dense", "neurons": 128, "activation": "relu", "base": True},
                {"name": "Dropout1", "type": "Dropout", "dropout_rate": 0.5, "base": True},
            ]
        # Reiniciar capas intermedias al cambiar dataset
        st.session_state['num_intermediate_layers'] = 0


# Funci칩n para manejar actualizaci칩n de capas intermedias
def update_intermediate_layers():
    num_layers = st.session_state['num_intermediate_layers']
    current_intermediate_layers = len([
        layer for layer in st.session_state['layer_config']
        if not layer.get("base", False)
    ])

    # Actualizar solo si hay un cambio
    if num_layers != current_intermediate_layers:
        # Eliminar capas intermedias actuales
        st.session_state['layer_config'] = [
            layer for layer in st.session_state['layer_config'] if layer.get("base", False)
        ]

        # Agregar nuevas capas intermedias
        for i in range(num_layers):
            st.session_state['layer_config'].insert(
                -2,  # Insertar antes de las capas finales (Flatten y Output)
                {
                    "name": f"IntermediateLayer{i + 1}",
                    "type": "Dense",
                    "neurons": 64,
                    "activation": "relu",
                    "base": False,
                }
            )


def load_dataset(name, train_ratio=0.8, val_ratio=0.1, shuffle=True, normalize=True):
    if name == 'MNIST':
        (x, y), (x_test, y_test) = mnist.load_data()
        if normalize:
            x = x.astype("float32") / 255.0
            x_test = x_test.astype("float32") / 255.0
        x = x.reshape(-1, 28, 28, 1)
        x_test = x_test.reshape(-1, 28, 28, 1)
        y = to_categorical(y, 10)
        y_test = to_categorical(y_test, 10)
        input_shape = (28, 28, 1)
        num_classes = 10
    elif name == 'Fashion MNIST':
        (x, y), (x_test, y_test) = fashion_mnist.load_data()
        if normalize:
            x = x.astype("float32") / 255.0
            x_test = x_test.astype("float32") / 255.0
        x = x.reshape(-1, 28, 28, 1)
        x_test = x_test.reshape(-1, 28, 28, 1)
        y = to_categorical(y, 10)
        y_test = to_categorical(y_test, 10)
        input_shape = (28, 28, 1)
        num_classes = 10
    elif name == 'CIFAR-10':
        (x, y), (x_test, y_test) = cifar10.load_data()
        if normalize:
            x = x.astype("float32") / 255.0
            x_test = x_test.astype("float32") / 255.0
        y = to_categorical(y, 10)
        y_test = to_categorical(y_test, 10)
        input_shape = (32, 32, 3)
        num_classes = 10
    else:
        raise ValueError("Dataset no soportado")

    # Dividir el conjunto de datos seg칰n las proporciones
    indices = np.arange(len(x))
    if shuffle:
        np.random.shuffle(indices)

    train_end = int(len(x) * train_ratio)
    val_end = int(len(x) * (train_ratio + val_ratio))
    x_train, y_train = x[:train_end], y[:train_end]
    x_val, y_val = x[train_end:val_end], y[train_end:val_end]
    x_test, y_test = x_test, y_test

    return x_train, y_train, x_val, y_val, x_test, y_test, input_shape, num_classes



def initialize_graph(layers):
    """
    Inicializa una representaci칩n de la red neuronal con Input y Output 칰nicos.
    """
    if not layers:
        st.session_state['graph'] = []
        return

    # Redefinir gr치fico con Input, capas intermedias y Output
    st.session_state['graph'] = [
        {'name': 'Input', 'num_neurons': 1, 'type': 'Input', 'layer_index': 0}
    ]

    for i, layer in enumerate(layers):
        st.session_state['graph'].append({
            'name': layer['name'],
            'num_neurons': layer.get('neurons', 4),  # Valor predeterminado
            'type': layer['type'],
            'layer_index': i + 1
        })

    st.session_state['graph'].append({
        'name': 'Output', 'num_neurons': 1, 'type': 'Output', 'layer_index': len(layers) + 1
    })



def generate_graph_fig(layers, active_layer=None, epoch=None, neurons_per_point=12, animated_layer=None, progress_step=0):
    """
    Genera el gr치fico unificado para preview y din치mico con animaci칩n.
    """
    fig = go.Figure()
    layer_positions = {}

    background_color = get_background_color()

    # Determinar tama침o de texto basado en la cantidad de capas
    num_layers = len(layers)
    text_size = 12 if num_layers <= 10 else max(8, 12 - (num_layers - 10) // 2)

    # Calcular posiciones y conexiones primero
    for i, layer in enumerate(st.session_state['graph']):
        x_position = i * 300
        if layer['type'] in ["Dense", "Input", "Output"]:
            total_neurons = layer['num_neurons']
            num_points = math.ceil(total_neurons / neurons_per_point)
            y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]
        else:
            y_positions = [0]  # Una sola posici칩n para capas simb칩licas

        # Guardar posiciones
        for j, y in enumerate(y_positions):
            layer_positions[(i, j)] = (x_position, y)

    # Dibujar conexiones primero (en el fondo)
    for i in range(len(st.session_state['graph']) - 1):
        for j1 in layer_positions:
            if j1[0] == i:
                for j2 in layer_positions:
                    if j2[0] == i + 1:
                        x0, y0 = layer_positions[j1]
                        x1, y1 = layer_positions[j2]
                        is_current_connection = i == animated_layer
                        line_color = "lightblue" if not is_current_connection else f"rgba(0, 200, 200, {0.5 + progress_step / 2})"
                        line_width = 2 if not is_current_connection else 3
                        fig.add_trace(go.Scatter(
                            x=[x0, x1], y=[y0, y1],
                            mode="lines",
                            line=dict(width=line_width, color=line_color),
                            hoverinfo="none"
                        ))

    # Dibujar formas (capas) encima
    for i, layer in enumerate(st.session_state['graph']):
        x_position = i * 300
        if layer['type'] in ["Dense", "Input", "Output"]:
            total_neurons = layer['num_neurons']
            num_points = math.ceil(total_neurons / neurons_per_point)
            y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]
            if layer['type'] != "Dense":   
                text1=[f"{layer['name']}"]
                # Dibujar neuronas como puntos
                node_color = "gray" if i != animated_layer else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                for j, y in enumerate(y_positions):
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[y],
                        mode="markers",
                        marker=dict(size=20, color=node_color),
                        hoverinfo="none",
                        text=text1
                    ))
            else:
                text1=[f"{layer['type']}<br>{layer['num_neurons']} Neuronas"]
                # Dibujar neuronas como puntos
                node_color = "blue" if i != animated_layer else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                for j, y in enumerate(y_positions):
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[y],
                        mode="markers",
                        marker=dict(size=10, color=node_color),
                        hoverinfo="none",
                        text=text1
                    ))



        else:
            # Capas sin neuronas: formas simb칩licas
            if layer['type'] == "Conv2D":
                color = "green"
            elif layer['type'] == "MaxPooling2D":
                color = "purple"
            elif layer['type'] == "Flatten":
                color = "gray"
            elif layer['type'] == "Dropout":
                color = "orange"
            elif layer['type'] == "BatchNormalization":
                color = "pink"
            else:
                color = "black"

            fig.add_trace(go.Scatter(
                x=[x_position], y=[0],
                mode="markers",
                marker=dict(size=20, color=color),
                hoverinfo="none",
                text=[f"{layer['name']}<br>{layer['type']}"]
            ))

        # Etiquetas para Input/Output y capas
        label_text = (
            f"{layer['type']}"
            if layer['type'] in ["Dense", "Input", "Output"]
            else layer['type']
        )

        # Ajustar posici칩n de etiquetas alternadas
        if i % 2 == 0:
            label_y_offset = max(y_positions) + 1.5  # Posici칩n arriba
            label_position = "top center"
        else:
            label_y_offset = min(y_positions) - 2.5  # Posici칩n abajo (m치s alejado)

        fig.add_trace(go.Scatter(
            x=[x_position], 
            y=[label_y_offset],
            mode="text",
            text=label_text,
            textposition=label_position,
            textfont=dict(size=text_size),
            hoverinfo="none"
        ))

    # Configuraci칩n del gr치fico
    title = f"Training Progress - Epoch {epoch + 1}" if epoch is not None else "Network Architecture Preview"
    fig.update_layout(
        title=title,
        showlegend=False,
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        plot_bgcolor=background_color,  # Fondo del gr치fico
        paper_bgcolor=background_color,  # Fondo del 치rea alrededor del gr치fico
        margin=dict(l=10, r=10, t=30, b=10)
    )

    return fig



def update_graph_with_smooth_color_transition(layers, epoch, placeholder, neurons_per_point=12, animation_steps=30):
    """
    Muestra una animaci칩n visual fluida de flujo entre capas mediante cambios progresivos de color.
    """
    total_layers = len(layers)
    background_color = get_background_color()

    for layer_index in range(total_layers - 1):  # No incluye la capa final
        for step in range(animation_steps):
            progress_step = step / animation_steps  # Progreso gradual del color
            
            fig = go.Figure()
            layer_positions = {}

            # Calcular posiciones de capas
            for i, layer in enumerate(layers):
                x_position = i * 300
                total_neurons = layer['num_neurons'] if layer['type'] in ["Dense", "Input", "Output"] else 1

                # Calcular cu치ntos puntos representar
                num_points = math.ceil(total_neurons / neurons_per_point)
                y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]

                # Guardar posiciones de la capa
                for j, y in enumerate(y_positions):
                    layer_positions[(i, j)] = (x_position, y)

            # Dibujar conexiones primero (fondo)
            for i in range(total_layers - 1):  # No conecta despu칠s de Output
                for j1 in layer_positions:
                    if j1[0] == i:
                        for j2 in layer_positions:
                            if j2[0] == i + 1:
                                x0, y0 = layer_positions[j1]
                                x1, y1 = layer_positions[j2]

                                # Color de las conexiones
                                if i == layer_index:
                                    line_color = f"rgba(255, {int(progress_step * 255)}, 0, 1)"  # De naranja a azul
                                else:
                                    line_color = "lightblue"

                                fig.add_trace(go.Scatter(
                                    x=[x0, x1], y=[y0, y1],
                                    mode="lines",
                                    line=dict(width=2, color=line_color),
                                    hoverinfo="none"
                                ))

            # Dibujar puntos o formas de las capas (frente)
            for i, layer in enumerate(layers):
                x_position = i * 300
                total_neurons = layer['num_neurons'] if layer['type'] in ["Dense", "Input", "Output"] else 1
                num_points = math.ceil(total_neurons / neurons_per_point)
                y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]

                # Determinar color y forma de la capa
                node_color = "blue" if i != layer_index and i != layer_index + 1 else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                if layer['type'] in ["Dense"]:
                    # Dibujar nodos (puntos)
                    for j, y in enumerate(y_positions):
                        fig.add_trace(go.Scatter(
                            x=[x_position], y=[y],
                            mode="markers",
                            marker=dict(size=10, color=node_color),
                            hoverinfo="none"
                        ))
                else:
                    # Dibujar formas simb칩licas para otras capas
                    color = {
                        "Conv2D": ( "green"),
                        "MaxPooling2D": ( "purple"),
                        "Flatten": ("gray"),
                        "Dropout": ("orange"),
                        "BatchNormalization": ("pink")
                    }.get(layer['type'], ("black"))
                    
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[-1],
                        mode="markers",
                        marker=dict(size=20, color=color),
                        hoverinfo="none"
                    ))

                # A침adir etiquetas
                label_text = (
                    f"{layer['type']}"
                    if layer['type'] in ["Dense", "Input", "Output"]
                    else layer['type']
                )

                # Ajustar posici칩n de etiquetas alternadas
                if i % 2 == 0:
                    label_y_offset = max(y_positions) + 1.5  # Posici칩n arriba
                    label_position = "top center"
                else:
                    label_y_offset = min(y_positions) - 2.5  # Posici칩n abajo (m치s alejado)

                fig.add_trace(go.Scatter(
                    x=[x_position], 
                    y=[label_y_offset],
                    mode="text",
                    text=label_text,
                    textposition=label_position,
                    hoverinfo="none"
                ))

            # Configuraci칩n del gr치fico
            fig.update_layout(
                title=f"Progreso del entrenamiento - 칄poca {epoch + 1}" if epoch is not None else "Arquitectura del modelo",
                showlegend=False,
                xaxis=dict(visible=False),
                yaxis=dict(visible=False),
                plot_bgcolor=background_color,  # Fondo del gr치fico
                paper_bgcolor=background_color,  # Fondo del 치rea alrededor del gr치fico
                margin=dict(l=10, r=10, t=30, b=10)
            )

            # Renderizar el gr치fico
            placeholder.plotly_chart(
                fig,
                use_container_width=True,
                key=f"training_{epoch}_{layer_index}_{step}_{time.time()}"
            )
            time.sleep(0.03)  # Ajusta para controlar la fluidez



def preview_graph(layers, placeholder, neurons_per_point=12):
    """
    Genera un gr치fico est치tico con Input, capas y Output.
    """
    initialize_graph(layers)
    if not st.session_state['graph']:
        placeholder.empty()
        st.warning("No hay capas configuradas. Agregue una capa para visualizar el gr치fico.")
        return

    fig = generate_graph_fig(layers, neurons_per_point=neurons_per_point)
    placeholder.plotly_chart(fig, use_container_width=True, key=f"preview_{time.time()}")





    
# Funci칩n para registrar logs
def log_event(log_placeholder, message):
    st.session_state['logs'].append(message)
    log_placeholder.text_area("Registro del entrenamiento", "\n".join(st.session_state['logs']), height=300)

# Funci칩n para entrenar el modelo
def train_model(layers, hyperparams, preview_placeholder, dynamic_placeholder):
    st.session_state['logs'] = []
    preview_placeholder.empty()
    loss_chart_placeholder = st.empty()
    accuracy_chart_placeholder = st.empty()

    initialize_graph(layers)

    dataset_name = st.session_state['selected_dataset']
    
    x_train, y_train, x_val, y_val, x_test, y_test, input_shape, num_classes = load_dataset(
        st.session_state['selected_dataset'],
        st.session_state['train_ratio'] / 100,
        st.session_state['val_ratio'] / 100,
        shuffle=st.session_state['shuffle_data'],
        normalize=st.session_state['normalize_data']
    )



    # Configuraci칩n de Data Augmentation (si est치 activado)
    if st.session_state['selected_dataset'] == 'CIFAR-10' and st.session_state.get('data_augmentation', False):
        datagen = ImageDataGenerator(
            rotation_range=st.session_state['rotation_range'],
            width_shift_range=st.session_state['width_shift_range'],
            height_shift_range=st.session_state['height_shift_range'],
            horizontal_flip=st.session_state['horizontal_flip']
        )
        datagen.fit(x_train)  # Ajustar el generador al conjunto de entrenamiento
    else:
        datagen = None  # No usar data augmentation si no est치 activado

    optimizers = {
        "Adam": Adam(learning_rate=hyperparams['learning_rate']),
        "SGD": SGD(learning_rate=hyperparams['learning_rate']),
        "RMSprop": RMSprop(learning_rate=hyperparams['learning_rate'])
    }
    optimizer = optimizers[hyperparams['optimizer']]

    model = Sequential()

    # Early Stopping Configuration
    callbacks = []
    if 'enable_early_stopping' in hyperparams and hyperparams['enable_early_stopping']:
        early_stopping = EarlyStopping(
            monitor=hyperparams['monitor_metric'],
            patience=hyperparams['patience'],
            restore_best_weights=True
        )
        callbacks.append(early_stopping)

    if st.session_state['selected_dataset'] == 'CIFAR-10':
        # Capas base iniciales para CIFAR-10
        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, name='Conv2D_1'))
        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, name='Conv2D_2'))
        model.add(MaxPooling2D(pool_size=(2, 2), name='MaxPooling2D_1'))

        # Capas intermedias (configurables por el usuario)
        for layer in layers:
            if not layer.get('base', False):
                if layer['type'] == "Conv2D":
                    model.add(Conv2D(
                        layer['filters'], 
                        layer['kernel_size'], 
                        activation=layer['activation'], 
                        name=layer['name']
                    ))
                    if layer.get('batch_norm', False):
                        model.add(BatchNormalization(name=f"BatchNorm_{layer['name']}"))
                elif layer['type'] == "MaxPooling2D":
                    model.add(MaxPooling2D(pool_size=layer['pool_size'], name=layer['name']))
                elif layer['type'] == "Dropout":
                    model.add(Dropout(layer['rate'], name=layer['name']))
                elif layer['type'] == "Dense":
                    model.add(Dense(layer['neurons'], activation=layer['activation'], name=layer['name']))

        # Capas base finales para CIFAR-10
        model.add(Flatten(name='Flatten'))
        model.add(Dense(128, activation='relu', name='Dense'))
        model.add(Dropout(0.2, name='Dropout'))

    else:
        # Modelo para MNIST y Fashion MNIST
        model.add(Flatten(input_shape=input_shape, name='Flatten_Base'))
        for layer in layers:
            if layer['type'] == "Dense":
                model.add(Dense(layer['neurons'], activation=layer['activation'], name=layer['name']))
                if layer.get('batch_norm', False):
                    model.add(BatchNormalization(name=f"BatchNorm_{layer['name']}"))
            elif layer['type'] == "Dropout":
                model.add(Dropout(layer['dropout_rate'], name=layer['name']))

    model.add(Dense(num_classes, activation='softmax', name="Output"))

    model.compile(optimizer=optimizer, loss=hyperparams['loss_function'], metrics=['accuracy'])

    epochs = hyperparams['epochs']
    batch_size = hyperparams['batch_size']

    # Crear la disposici칩n en una fila
    col_dynamic, col_metrics = st.columns([6, 1])  # Anchos personalizados: Log, Din치mico, M칠tricas


    log_placeholder = st.empty()

    # Placeholder para el gr치fico din치mico
    with col_dynamic:
        dynamic_placeholder = st.empty()

    # Placeholders para m칠tricas
    loss_chart_placeholder = col_metrics.empty()
    accuracy_chart_placeholder = col_metrics.empty()

    # Inicializar listas para almacenar las m칠tricas
    loss_values = []
    accuracy_values = []

    for epoch in range(epochs):
        
        if not st.session_state['training_in_progress']:
            break



        epoch_start_time = time.time()
        log_event(log_placeholder, f"칄poca {epoch + 1}/{epochs} - Iniciando entrenamiento... ({dataset_name})")


        
        # Lanzar animaci칩n sincronizada por capas
          # Llamar a la animaci칩n sincronizada
        update_graph_with_smooth_color_transition(
            st.session_state['graph'], 
            epoch, 
            dynamic_placeholder, 
            neurons_per_point=12, 
            animation_steps=15
        )

        # Entrenamiento del modelo
        if datagen:
            history = model.fit(
                datagen.flow(x_train, y_train, batch_size=batch_size),
                steps_per_epoch=len(x_train) // batch_size,
                epochs=1,
                validation_data=(x_val, y_val),
                callbacks=callbacks,
                verbose=0
            )
        else:
            history = model.fit(
                x_train, y_train,
                batch_size=batch_size,
                epochs=1,
                validation_data=(x_val, y_val),
                callbacks=callbacks,
                verbose=0
            )


        
        # Almacenar las m칠tricas
        loss = history.history['loss'][0]
        accuracy = history.history['accuracy'][0]
        loss_values.append(loss)
        accuracy_values.append(accuracy)

        # Actualizar gr치ficos de m칠tricas
        if st.session_state['show_metrics']: 
            with col_metrics:
                # Gr치fico de P칠rdida
                fig_loss, ax_loss = plt.subplots(figsize=(4, 2))
                ax_loss.plot(range(1, len(loss_values) + 1), loss_values, label="P칠rdida", marker='o', color='blue')
                ax_loss.set_title("P칠rdida")
                ax_loss.set_xlabel("칄pocas")
                ax_loss.set_ylabel("P칠rdida")
                ax_loss.grid(True)
                loss_chart_placeholder.pyplot(fig_loss, clear_figure=True)

                # Gr치fico de Precisi칩n
                fig_accuracy, ax_accuracy = plt.subplots(figsize=(4, 2))
                ax_accuracy.plot(range(1, len(accuracy_values) + 1), accuracy_values, label="Precisi칩n", marker='o', color='green')
                ax_accuracy.set_title("Precisi칩n")
                ax_accuracy.set_xlabel("칄pocas")
                ax_accuracy.set_ylabel("Precisi칩n")
                ax_accuracy.grid(True)
                accuracy_chart_placeholder.pyplot(fig_accuracy, clear_figure=True)

        # Log de la 칠poca
        epoch_end_time = time.time()

        with log_placeholder:
            log_event(
                log_placeholder,
                f"칄poca {epoch + 1}/{epochs} - P칠rdida: {loss:.4f}, Precisi칩n: {accuracy:.4f}, "
                f"P칠rdida Validaci칩n: {history.history['val_loss'][0]:.4f}, Precisi칩n Validaci칩n: {history.history['val_accuracy'][0]:.4f}\n"
                f"Tiempo por 칄poca: {epoch_end_time - epoch_start_time:.2f}s"
            )

        # Early stopping check
        if 'enable_early_stopping' in hyperparams and hyperparams['enable_early_stopping']:
            if early_stopping.stopped_epoch > 0:
                log_event(log_placeholder, "Entrenamiento detenido por Early Stopping.")
                break 

    # Guardar el modelo despu칠s del entrenamiento
    st.session_state['modelDownload'] = model







current_intermediate_layers = max(0, len(st.session_state['layer_config']) - 6)

# Inicializar estado para el n칰mero de capas
if 'num_layers_selected' not in st.session_state:
    st.session_state['num_layers_selected'] = len(st.session_state['layer_config'])
if 'previous_layer_config' not in st.session_state:
    st.session_state['previous_layer_config'] = []
# Inicializar el estado para capas intermedias si no existe
if 'num_intermediate_layers' not in st.session_state:
    st.session_state['num_intermediate_layers'] = current_intermediate_layers

# Funci칩n para manejar la actualizaci칩n de capas
def update_layer_config():
    num_layers = st.session_state['num_layers_selected']
    current_num_layers = len(st.session_state['layer_config'])

    if num_layers > current_num_layers:
        # A침adir capas
        for i in range(current_num_layers, num_layers):
            st.session_state['layer_config'].append({
                "name": f"Layer{i + 1}",
                "type": "Dense",
                "neurons": 64,
                "dropout_rate": 0.2
            })
    elif num_layers < current_num_layers:
        # Reducir capas
        st.session_state['layer_config'] = st.session_state['layer_config'][:num_layers]

    # Guardar una copia del estado actual para evitar rebotes
    st.session_state['previous_layer_config'] = st.session_state['layer_config'][:]



# Diccionario de arquitecturas predefinidas
architectures = {
    "MNIST": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True},
            {"name": "Dropout1", "type": "Dropout", "dropout_rate": 0.2, "base": True},
            {"name": "Dense2", "type": "Dense", "neurons": 10, "activation": "softmax", "base": True}
        ],
        "Deep MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 128, "activation": "relu", "base": True},
            {"name": "Dense2", "type": "Dense", "neurons": 64, "activation": "relu", "base": True},
            {"name": "Dropout1", "type": "Dropout", "dropout_rate": 0.2, "base": True},
            {"name": "Dense3", "type": "Dense", "neurons": 10, "activation": "softmax", "base": True}
        ]
    },
    "Fashion MNIST": {
        "Simple CNN": [
            {"name": "Conv2D1", "type": "Conv2D", "filters": 32, "kernel_size": 3, "activation": "relu", "base": True},
            {"name": "MaxPooling2D1", "type": "MaxPooling2D", "pool_size": 2, "base": True},
            {"name": "Flatten1", "type": "Flatten", "base": True},
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True},
            {"name": "Dense2", "type": "Dense", "neurons": 10, "activation": "softmax", "base": True}
        ],
        "Deep CNN": [
            {"name": "Conv2D1", "type": "Conv2D", "filters": 64, "kernel_size": 3, "activation": "relu", "base": True},
            {"name": "Conv2D2", "type": "Conv2D", "filters": 32, "kernel_size": 3, "activation": "relu", "base": True},
            {"name": "MaxPooling2D1", "type": "MaxPooling2D", "pool_size": 2, "base": True},
            {"name": "Flatten1", "type": "Flatten", "base": True},
            {"name": "Dense1", "type": "Dense", "neurons": 128, "activation": "relu", "base": True},
            {"name": "Dense2", "type": "Dense", "neurons": 10, "activation": "softmax", "base": True}
        ]
    },
    "CIFAR-10": {
        "VGG-like": [
            {"name": "Conv2D1", "type": "Conv2D", "filters": 64, "kernel_size": 3, "activation": "relu", "base": True},
            {"name": "Conv2D2", "type": "Conv2D", "filters": 64, "kernel_size": 3, "activation": "relu", "base": True},
            {"name": "MaxPooling2D1", "type": "MaxPooling2D", "pool_size": 2, "base": True},
            {"name": "Conv2D3", "type": "Conv2D", "filters": 128, "kernel_size": 3, "activation": "relu", "base": True},
            {"name": "MaxPooling2D2", "type": "MaxPooling2D", "pool_size": 2, "base": True},
            {"name": "Flatten1", "type": "Flatten", "base": True},
            {"name": "Dense1", "type": "Dense", "neurons": 256, "activation": "relu", "base": True},
            {"name": "Dense2", "type": "Dense", "neurons": 10, "activation": "softmax", "base": True}
        ],
        "Custom CNN": [
            {"name": "Conv2D1", "type": "Conv2D", "filters": 32, "kernel_size": 3, "activation": "relu", "base": True},
            {"name": "MaxPooling2D1", "type": "MaxPooling2D", "pool_size": 2, "base": True},
            {"name": "Flatten1", "type": "Flatten", "base": True},
            {"name": "Dense1", "type": "Dense", "neurons": 128, "activation": "relu", "base": True},
            {"name": "Dense2", "type": "Dense", "neurons": 10, "activation": "softmax", "base": True}
        ]
    }
}


# Funci칩n para manejar el cambio de dataset
if "selected_dataset_previous" not in st.session_state or st.session_state["selected_dataset_previous"] != st.session_state["selected_dataset"]:
    st.session_state["selected_dataset_previous"] = st.session_state["selected_dataset"]
    
    # Reinicia configuraci칩n de capas y arquitectura
    initialize_layer_config(st.session_state["selected_dataset"])




# Configuraci칩n con pesta침as
tabs = st.sidebar.radio("Configuraci칩n:", ["Dataset","Capas", "Hiperpar치metros", "Early Stopping"])


# Configuraci칩n del Dataset
if tabs == "Dataset":
    st.sidebar.subheader("Configuraci칩n del Dataset")
    # Mostrar select box para dataset
    st.sidebar.subheader("Dataset")
    st.session_state['selected_dataset'] = st.sidebar.selectbox(
        "Seleccione un Dataset para Entrenar",
        options=['MNIST', 'Fashion MNIST', 'CIFAR-10']
    )

    # Inicializar configuraci칩n de capas seg칰n el dataset seleccionado
    initialize_layer_config(st.session_state['selected_dataset'])

    # Proporci칩n de datos: activaci칩n de configuraci칩n avanzada
    advanced_split = st.sidebar.checkbox(
        "Configurar proporciones espec칤ficas",
        value=False,
        help="Permite ajustar individualmente las proporciones de entrenamiento, validaci칩n y prueba."
    )

    if advanced_split:
        # Ajustes de proporciones: Entrenamiento, Validaci칩n y Prueba
        train_ratio = st.sidebar.slider(
            "Entrenamiento (%)",
            min_value=10,
            max_value=90,
            value=st.session_state.get('train_ratio', 80),
            step=1,
            help="Porcentaje de datos para entrenamiento. La suma debe ser 100%."
        )
        val_ratio = st.sidebar.slider(
            "Validaci칩n (%)",
            min_value=5,
            max_value=50,
            value=st.session_state.get('val_ratio', 10),
            step=1,
            help="Porcentaje de datos para validaci칩n. La suma debe ser 100%."
        )
        test_ratio = 100 - train_ratio - val_ratio
        st.sidebar.text(f"Prueba (%): {test_ratio}")

        if train_ratio + val_ratio > 100:
            st.warning("Las proporciones de entrenamiento y validaci칩n exceden el 100%. Ajusta los valores.")
            st.stop()
    else:
        train_ratio = 80
        val_ratio = 10
        test_ratio = 10

    # Guardar las proporciones en `st.session_state`
    st.session_state['train_ratio'] = train_ratio
    st.session_state['val_ratio'] = val_ratio
    st.session_state['test_ratio'] = test_ratio

    # Aleatorizaci칩n de datos
    st.session_state['shuffle_data'] = st.sidebar.checkbox(
        "Aleatorizar Datos",
        value=st.session_state.get('shuffle_data', True),
        help="Determina si los datos deben ser aleatorizados antes de dividirlos."
    )

    # Normalizaci칩n
    st.session_state['normalize_data'] = st.sidebar.checkbox(
        "Normalizar Datos",
        value=st.session_state.get('normalize_data', True),
        help="Si est치 activado, escala los datos entre 0 y 1."
    )

    # Data Augmentation (Solo para CIFAR-10)
    if st.session_state['selected_dataset'] == 'CIFAR-10':
        st.session_state['data_augmentation'] = st.sidebar.checkbox(
            "Aplicar Data Augmentation",
            value=st.session_state.get('data_augmentation', False),
            help="Realiza aumentos de datos como rotaciones o traslaciones para mejorar la robustez del modelo."
        )
        if st.session_state['data_augmentation']:
            st.session_state['rotation_range'] = st.sidebar.slider(
                "Rango de Rotaci칩n (춿)",
                min_value=0,
                max_value=45,
                value=st.session_state.get('rotation_range', 10),
                step=1,
                help="Rango de rotaciones aleatorias aplicadas a las im치genes."
            )
            st.session_state['width_shift_range'] = st.sidebar.slider(
                "Desplazamiento Horizontal (%)",
                min_value=0.0,
                max_value=0.5,
                value=st.session_state.get('width_shift_range', 0.1),
                step=0.01,
                help="Desplazamiento horizontal m치ximo como porcentaje del ancho de la imagen."
            )
            st.session_state['height_shift_range'] = st.sidebar.slider(
                "Desplazamiento Vertical (%)",
                min_value=0.0,
                max_value=0.5,
                value=st.session_state.get('height_shift_range', 0.1),
                step=0.01,
                help="Desplazamiento vertical m치ximo como porcentaje de la altura de la imagen."
            )
            st.session_state['horizontal_flip'] = st.sidebar.checkbox(
                "Volteo Horizontal",
                value=st.session_state.get('horizontal_flip', True),
                help="Permite voltear horizontalmente las im치genes de forma aleatoria."
            )

    # Tama침o de batch
    st.session_state['batch_size'] = st.sidebar.number_input(
        "Tama침o de Batch",
        min_value=8,
        max_value=256,
        value=st.session_state.get('batch_size', 32),
        step=8,
        help="Tama침o de los lotes que se utilizar치n durante el entrenamiento."
    )

    # Mostrar ejemplos del dataset
    x_train, y_train, x_val, y_val, x_test, y_test, input_shape, num_classes = load_dataset(
        st.session_state['selected_dataset'],
        st.session_state['train_ratio'] / 100,
        st.session_state['val_ratio'] / 100,
        shuffle=st.session_state['shuffle_data'],
        normalize=st.session_state['normalize_data']
    )
    st.sidebar.subheader("Ejemplo de Dataset")
    for i in range(5):
        st.sidebar.image(
            x_train[i].reshape(input_shape[:2]) if st.session_state['selected_dataset'] != 'CIFAR-10' else x_train[i],
            caption=f"Etiqueta: {y_train[i].argmax()}",
            width=100
        )



# Visualizar ejemplos del dataset
elif tabs == "Capas":
    st.sidebar.subheader("Configuraci칩n de Capas")
    # Mostrar select box para arquitectura

    
    st.sidebar.subheader("Arquitectura")
    selected_architecture = st.sidebar.selectbox(
        "Seleccione una Arquitectura",
        options=list(architectures[st.session_state['selected_dataset']].keys()),
        help="Elija la arquitectura del modelo basada en el dataset seleccionado."
    )

    # Actualizar configuraci칩n de capas seg칰n la arquitectura seleccionada
    if st.sidebar.button("Aplicar Arquitectura"):
        # Obtener las capas de la arquitectura seleccionada
        selected_architecture_layers = architectures[st.session_state['selected_dataset']][selected_architecture]

        # Reiniciar las configuraciones de capas
        st.session_state['layer_config'] = selected_architecture_layers[:]
        st.session_state['num_layers_selected'] = len(selected_architecture_layers)

        # Validar tipos de capas seg칰n el dataset
        if st.session_state['selected_dataset'] in ['MNIST']:
            valid_types = ["Dense", "Dropout"]
        else:
            valid_types = ["Conv2D", "MaxPooling2D", "Flatten", "Dense", "Dropout", "BatchNormalization"]

        # Asegurarse de que las capas sean v치lidas
        for layer in st.session_state['layer_config']:
            if layer['type'] not in valid_types:
                st.warning(f"Tipo de capa no v치lido detectado: {layer['type']}.")
                st.session_state['layer_config'] = []  # Limpia configuraciones inv치lidas
                break

        st.sidebar.success(f"Arquitectura '{selected_architecture}' aplicada correctamente.")

    # Validar si las configuraciones cambian en otros lugares
    if st.session_state['layer_config'] != st.session_state.get('previous_layer_config', []):
        st.session_state['previous_layer_config'] = st.session_state['layer_config'][:]

    if st.session_state['selected_dataset'] == 'CIFAR-10':
        # Interfaz de usuario para n칰mero de capas intermedias
        st.sidebar.subheader("Capas Intermedias")
        st.sidebar.number_input(
            "N칰mero de Capas Intermedias",
            min_value=0,
            max_value=15,
            value=st.session_state['num_intermediate_layers'],
            step=1,
            key='num_intermediate_layers',
            help="Define cu치ntas capas intermedias desea incluir entre las capas iniciales y finales. Ajuste seg칰n las necesidades de su modelo.",
            on_change=update_intermediate_layers
        )

        # Mostrar configuraci칩n de las capas
        for i, layer in enumerate(st.session_state['layer_config']):
            if layer.get('base', False):
                st.sidebar.text_input(
                    f"{layer['name']} (Base, No editable)",
                    value=layer['type'],
                    disabled=True,
                    key=f"text_input_{i}",
                    help="Esta es una capa base predefinida y no puede ser modificada."
                )
            else:
                with st.sidebar.expander(f"Configuraci칩n de la Capa {i + 1}", expanded=True):
                    layer['type'] = st.selectbox(
                        "Tipo de Capa",
                        ["Conv2D", "MaxPooling2D", "Dense", "Dropout", "BatchNormalization"],
                        key=f"layer_type_{i}",
                        help="Seleccione el tipo de capa que desea a침adir al modelo."
                    )
                    if layer['type'] == "Conv2D":
                        layer['filters'] = st.number_input(
                            "N칰mero de Filtros",
                            min_value=1,
                            value=layer.get('filters', 32),
                            key=f"filters_{i}",
                            help="Define cu치ntos filtros utilizar치 esta capa de convoluci칩n."
                        )
                        layer['kernel_size'] = st.slider(
                            "Tama침o del Kernel",
                            1,
                            5,
                            value=3,
                            step=1,
                            key=f"kernel_{i}",
                            help="Tama침o del filtro o kernel que se mover치 sobre la imagen en la capa de convoluci칩n."
                        )
                        layer['activation'] = st.selectbox(
                            "Activaci칩n",
                            ["relu", "sigmoid", "tanh","softmax"],
                            index=0,
                            key=f"activation_{i}",
                            help="Funci칩n de activaci칩n que transforma la salida de la capa."
                        )
                        layer['batch_normalization'] = st.checkbox(
                            "Agregar Batch Normalization",
                            value=layer.get('batch_normalization', False),
                            key=f"batch_norm_{i}",
                            help="Si est치 activado, agrega una normalizaci칩n de lotes despu칠s de la capa."
                        )
                    elif layer['type'] == "MaxPooling2D":
                        layer['pool_size'] = st.slider(
                            "Tama침o del Pool",
                            1,
                            3,
                            value=2,
                            step=1,
                            key=f"pool_{i}",
                            help="Tama침o de la ventana para realizar la operaci칩n de pooling."
                        )
                    elif layer['type'] == "Dense":
                        layer['neurons'] = st.number_input(
                            "N칰mero de Neuronas",
                            min_value=1,
                            value=layer.get('neurons', 64),
                            key=f"neurons_{i}",
                            help="N칰mero de neuronas en la capa densa."
                        )
                        layer['activation'] = st.selectbox(
                            "Activaci칩n",
                            ["relu", "sigmoid", "tanh","softmax"],
                            index=0,
                            key=f"activation_dense_{i}",
                            help="Funci칩n de activaci칩n para la capa densa."
                        )
                        layer['dropout_rate'] = st.slider(
                            "Tasa de Dropout",
                            0.0,
                            0.9,
                            value=layer.get('dropout_rate', 0.2),
                            step=0.1,
                            key=f"dropout_dense_{i}",
                            help="Proporci칩n de unidades que se desactivar치n aleatoriamente en la capa."
                        )
                    elif layer['type'] == "Dropout":
                        layer['dropout_rate'] = st.slider(
                            "Tasa de Dropout",
                            0.0,
                            0.9,
                            value=layer.get('dropout_rate', 0.2),
                            step=0.1,
                            key=f"dropout_{i}",
                            help="Proporci칩n de unidades que se desactivar치n aleatoriamente en esta capa."
                        )
                    elif layer['type'] == "BatchNormalization":
                        layer['batch_normalization'] = True
                    layer['name'] = f"{layer['type']}{i + 1}"


    # Configuraci칩n para otros datasets
    else: 
        # UI para n칰mero de capas
        st.sidebar.subheader("Capas")
        st.sidebar.number_input(
            "N칰mero de Capas",
            min_value=0,
            max_value=30,
            value=st.session_state['num_layers_selected'],
            step=1,
            key='num_layers_selected',
            help="N칰mero total de capas que desea incluir en su modelo.",
            on_change=update_layer_config
        )

        # Validar si la configuraci칩n de capas cambi칩 externamente
        if st.session_state['layer_config'] != st.session_state['previous_layer_config']:
            st.session_state['layer_config'] = st.session_state['previous_layer_config'][:]

        if st.session_state['selected_dataset'] == 'MNIST':
            valid_types = ["Dense", "Dropout", "Flatten", "BatchNormalization"]
        else:
            valid_types = ["Dense", "Dropout", "Conv2D", "MaxPooling2D", "Flatten", "BatchNormalization"]

        # Validar si la configuraci칩n actual de capas es v치lida para el dataset seleccionado
        invalid_layers = any(layer.get("type") not in valid_types for layer in st.session_state['layer_config'])

        
        if invalid_layers:
        # Reiniciar las capas si hay configuraciones inv치lidas
            st.session_state['layer_config'] = []
            st.warning("La configuraci칩n previa de capas no es v치lida para el dataset seleccionado. Las capas se han reiniciado.")

        # Mostrar configuraci칩n de cada capa
        for i, layer in enumerate(st.session_state['layer_config']):
            with st.sidebar.expander(f"Configuraci칩n de la Capa {i + 1}", expanded=True):
                layer['type'] = st.selectbox(
                    "Tipo de Capa",
                    valid_types,
                    index=valid_types.index(layer.get("type", "Dense")),
                    key=f"layer_type_{i}",
                    help="Seleccione el tipo de capa que desea a침adir."
                )
                # Configuraci칩n espec칤fica de cada tipo de capa
                if layer['type'] == "Dense":
                    layer['neurons'] = st.number_input(
                        "N칰mero de Neuronas",
                        min_value=1,
                        value=layer.get('neurons', 64),
                        key=f"neurons_{i}",
                        help="N칰mero de neuronas en la capa densa."
                    )
                    layer['activation'] = st.selectbox(
                        "Activaci칩n",
                        ["relu", "sigmoid", "tanh", "softmax"],
                        index=["relu", "sigmoid", "tanh", "softmax"].index(layer.get('activation', "relu")),
                        key=f"activation_{i}",
                        help="Funci칩n de activaci칩n para la capa densa."
                    )
                elif layer['type'] == "Dropout":
                    layer['dropout_rate'] = st.slider(
                        "Tasa de Dropout",
                        0.0,
                        0.9,
                        value=layer.get('dropout_rate', 0.2),
                        step=0.1,
                        key=f"dropout_{i}",
                        help="Proporci칩n de unidades que se desactivar치n aleatoriamente."
                    )
                elif layer['type'] == "Conv2D":
                    layer['filters'] = st.number_input(
                        "N칰mero de Filtros",
                        min_value=1,
                        value=layer.get('filters', 32),
                        key=f"filters_{i}",
                        help="Define cu치ntos filtros utilizar치 esta capa de convoluci칩n."
                    )
                    layer['kernel_size'] = st.slider(
                        "Tama침o del Kernel",
                        min_value=1,
                        max_value=5,
                        value=layer.get('kernel_size', 3),
                        step=1,
                        key=f"kernel_{i}",
                        help="Tama침o del filtro o kernel que se mover치 sobre la imagen."
                    )
                    layer['activation'] = st.selectbox(
                        "Activaci칩n",
                        ["relu", "sigmoid", "tanh", "softmax"],
                        index=["relu", "sigmoid", "tanh", "softmax"].index(layer.get('activation', "relu")),
                        key=f"activation_conv_{i}",
                        help="Funci칩n de activaci칩n para la capa de convoluci칩n."
                    )
                elif layer['type'] == "MaxPooling2D":
                    layer['pool_size'] = st.slider(
                        "Tama침o del Pool",
                        min_value=1,
                        max_value=5,
                        value=layer.get('pool_size', 2),
                        step=1,
                        key=f"pool_size_{i}",
                        help="Tama침o de la ventana para la operaci칩n de pooling."
                    )
                elif layer['type'] == "Flatten":
                    st.info("Capa que aplana la entrada para conectarla con capas densas.")
                elif layer['type'] == "BatchNormalization":
                    st.info("Capa para normalizar las salidas de la capa anterior y estabilizar el entrenamiento.")
                elif layer['type'] == "GlobalAveragePooling2D":
                    st.info("Reduce cada canal de caracter칤sticas a un 칰nico valor promedio.")

                # Actualizar el nombre de la capa
                layer['name'] = f"{layer['type']}{i + 1}"


# Configurar hiperpar치metros
elif tabs == "Hiperpar치metros":
    st.sidebar.subheader("Hiperpar치metros")
    st.session_state['hyperparams']['optimizer'] = st.sidebar.selectbox(
        "Optimizador",
        ["Adam", "SGD", "RMSprop"],
        help="Elija el optimizador que se utilizar치 para ajustar los pesos del modelo."
    )
    st.session_state['hyperparams']['learning_rate'] = st.sidebar.slider(
        "Tasa de Aprendizaje",
        min_value=0.0001,
        max_value=0.01,
        value=0.001,
        step=0.0001,
        format="%.4g",  # Mostrar hasta 4 decimales
        help="Velocidad con la que el modelo ajusta sus pesos durante el entrenamiento."
    )
    st.session_state['hyperparams']['epochs'] = st.sidebar.number_input(
        "N칰mero de 칄pocas",
        min_value=1,
        max_value=50,
        value=5,
        step=1,
        help="N칰mero de veces que el modelo ver치 todo el conjunto de datos durante el entrenamiento."
    )
    st.session_state['hyperparams']['batch_size'] = st.sidebar.number_input(
        "Tama침o de Batch",
        min_value=8,
        max_value=128,
        value=32,
        step=8,
        help="Cantidad de muestras procesadas antes de actualizar los pesos del modelo."
    )
    st.session_state['hyperparams']['loss_function'] = st.sidebar.selectbox(
        "Funci칩n de P칠rdida",
        ["categorical_crossentropy", "mean_squared_error"],
        help="M칠trica utilizada para evaluar qu칠 tan bien se est치 entrenando el modelo."
    )


# Configuraci칩n de Early Stopping
elif tabs == "Early Stopping":
    st.sidebar.subheader("Early Stopping")
    enable_early_stopping = st.sidebar.checkbox(
        "Habilitar Early Stopping",
        value=False,
        help="Detiene el entrenamiento si no hay mejora en la m칠trica monitoreada despu칠s de un n칰mero determinado de 칠pocas."
    )
    if enable_early_stopping:
        patience = st.sidebar.number_input(
            "Patience (N칰mero de 칠pocas sin mejora)",
            min_value=1,
            max_value=20,
            value=3,
            step=1,
            help="N칰mero de 칠pocas sin mejora antes de detener el entrenamiento."
        )
        monitor_metric = st.sidebar.selectbox(
            "M칠trica a Monitorear",
            ["val_loss", "val_accuracy"],
            index=0,
            help="M칠trica que se monitorear치 para decidir si detener el entrenamiento."
        )



# Checkbox para mostrar/ocultar m칠tricas (siempre disponible)
st.session_state['show_metrics'] = st.checkbox(
    "Mostrar Gr치ficos de M칠tricas",
    value=st.session_state.get('show_metrics', False),
    disabled=st.session_state['training_in_progress']  # Deshabilitar si est치 entrenando
)


# Funci칩n para resetear el estado de entrenamiento
def reset_training_state():
    st.session_state['training_in_progress'] = True
    st.session_state['training_finished'] = False
    st.session_state['logs'] = []  # Limpiar logs solo al comenzar nuevo entrenamiento
    st.session_state['modelDownload'] = None  # Limpiar modelo previo
    dynamic_placeholder.empty()  # Limpiar placeholders
    preview_placeholder.empty()


# Mostrar preview solo si no est치 entrenando
if not st.session_state['training_in_progress']:
    preview_graph(st.session_state['layer_config'], preview_placeholder)

# Bot칩n para iniciar el entrenamiento
if not st.session_state['training_in_progress'] and not st.session_state['training_finished']:
    if st.button("Comenzar entrenamiento"):
        reset_training_state()  # Resetear estados antes de iniciar
        st.rerun()

# Bot칩n para cancelar el entrenamiento
if st.session_state['training_in_progress']:
    col_cancel, col_info = st.columns([1, 3])
    with col_cancel:
        if st.button("Cancelar Entrenamiento"):
            st.session_state['training_in_progress'] = False
            st.session_state['training_finished'] = False
            st.session_state['logs'].append("Entrenamiento cancelado por el usuario.")
            dynamic_placeholder.text("Entrenamiento cancelado.")
            st.rerun()

# Mostrar progreso del entrenamiento
if st.session_state['training_in_progress']:
    train_model(st.session_state['layer_config'], st.session_state['hyperparams'], preview_placeholder, dynamic_placeholder)
    st.session_state['training_in_progress'] = False
    st.session_state['training_finished'] = True
    st.rerun()


# Mostrar el bot칩n de guardar modelo una vez terminado el entrenamiento
if st.session_state['training_finished']:
    st.success("Entrenamiento finalizado con 칠xito.")
    st.text_area(
        "Logs del Entrenamiento",
        "\n".join(st.session_state['logs']),
        height=300,
        key="final_logs"
    )    
    if st.button("Guardar Modelo"):
        st.session_state['modelDownload'].save("trained_model.h5")
        with open("trained_model.h5", "rb") as file:
            st.download_button("Descargar Modelo", file, file_name="trained_model.h5")
    
    # Mostrar el preview del gr치fico despu칠s de finalizar
    preview_graph(st.session_state['layer_config'], preview_placeholder)
    
    # Bot칩n para reiniciar el entrenamiento
    if st.button("Comenzar Entrenamiento"):
        reset_training_state()
        st.rerun()
