import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import plotly.graph_objects as go
import math
import numpy as np
import time
import pandas as pd

from threading import Thread
from matplotlib.animation import FuncAnimation
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam, SGD, RMSprop
from keras.callbacks import EarlyStopping
from keras.utils import to_categorical
from keras.regularizers import l2
from sklearn.datasets import load_iris, load_breast_cancer, load_digits, load_wine, fetch_california_housing, load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer


# Ajustar el ancho del sidebar con CSS personalizado
st.markdown("""
    <style>
    /* Ajustar el ancho del sidebar */
    [data-testid="stSidebar"] {
        width: 300px; /* Cambia el valor seg煤n lo desees */
    }
    /* Ajustar el contenido del sidebar para que se ajuste al ancho */
    [data-testid="stSidebar"] .css-1d391kg {
        width: 300px; /* Cambia el valor para que coincida con el ancho del sidebar */
    }
    </style>
    """, unsafe_allow_html=True)



# Config

def new_line():
    st.write("\n")
 
# with st.sidebar:
#    st.image("./assets/sb-quick.png",  use_container_width=True)


st.markdown("<h1 style='text-align: center; '>NeuraTrain</h1>", unsafe_allow_html=True)
st.markdown("КModelo de regresi贸n/clasificaci贸n para datos tabulares(csv)", unsafe_allow_html=True)
st.markdown("Utiliza el panel lateral para comenzar a crear tu red neuronal artifical!", unsafe_allow_html=True)

st.divider()


# Inicializar estado de la sesi贸n
if 'graph_positions' not in st.session_state:
    st.session_state['graph_positions'] = None
if 'graph' not in st.session_state:
    st.session_state['graph'] = None
if 'layer_config' not in st.session_state:
    st.session_state['layer_config'] = []
if 'hyperparams' not in st.session_state:
    st.session_state['hyperparams'] = {
        'optimizer': 'Adam',
        'learning_rate': 0.001,
        'epochs': 5,
        'batch_size': 32,
        'loss_function': 'categorical_crossentropy'
    }
if 'logs' not in st.session_state:
    st.session_state['logs'] = []
if 'training_in_progress' not in st.session_state:
    st.session_state['training_in_progress'] = False
if 'selected_dataset' not in st.session_state:
    st.session_state['selected_dataset'] = 'Iris'
if 'training_finished' not in st.session_state:
    st.session_state['training_finished'] = False
if 'modelDownload' not in st.session_state:
    st.session_state['modelDownload'] = None
# Inicializar estado de la sesi贸n para el bot贸n de m茅tricas
if 'show_metrics' not in st.session_state:
    st.session_state['show_metrics'] = False    
if 'train_ratio' not in st.session_state:
    st.session_state['train_ratio'] = 80  # Valor predeterminado para el entrenamiento
if 'val_ratio' not in st.session_state:
    st.session_state['val_ratio'] = 10  # Valor predeterminado para la validaci贸n
if 'test_ratio' not in st.session_state:
    st.session_state['test_ratio'] = 10  # Valor predeterminado para la prueba

if "dataset_loaded" not in st.session_state:
    st.session_state["dataset_loaded"] = False
if "full_dataset" not in st.session_state:
    st.session_state["full_dataset"] = False
if "X_original" not in st.session_state:
    st.session_state["X_original"] = None
if "y_original" not in st.session_state:
    st.session_state["y_original"] = None
if "columns_removed" not in st.session_state:
    st.session_state["columns_removed"] = False
if "missing_handled" not in st.session_state:
    st.session_state["missing_handled"] = False
if "categorical_encoded" not in st.session_state:
    st.session_state["categorical_encoded"] = False
if "scaling_done" not in st.session_state:
    st.session_state["scaling_done"] = False
if "dataset_split" not in st.session_state:
    st.session_state["dataset_split"] = False

if 'loss_values' not in st.session_state:
    st.session_state['loss_values'] = []
if 'accuracy_values' not in st.session_state:
    st.session_state['accuracy_values'] = []
if 'val_loss_values' not in st.session_state:
    st.session_state['val_loss_values'] = []
if 'val_accuracy_values' not in st.session_state:
    st.session_state['val_accuracy_values'] = []
if 'problem_type' not in st.session_state:
    st.session_state['problem_type'] = "Clasificaci贸n"  # Valor predeterminado
if 'y_test' not in st.session_state:
    st.session_state['y_test'] = None
if 'split_type' not in st.session_state:
    st.session_state['split_type'] = "Entrenamiento y Prueba"  # Valor predeterminado1

if 'scaler_y' not in st.session_state:
    st.session_state['scaler_y'] = None

if 'scaler_X' not in st.session_state:
    st.session_state['scaler_X'] = None

if 'early_stopping' not in st.session_state:
    st.session_state['early_stopping'] = {
        "enabled": False,
        "patience": 0,
        "monitor": "val_loss"
    }


if 'num_classes' not in st.session_state:
    st.session_state['num_classes'] = 0

if 'splits' not in st.session_state:
    st.session_state['splits'] = []

if 'disabled_button_train' not in st.session_state:
    st.session_state['disabled_button_train'] = False

if 'f1_score' not in st.session_state:
    st.session_state['f1_score'] = 0
if  'precision_score' not in st.session_state:
    st.session_state['precision_score'] = 0
if 'recall_score' not in st.session_state:
    st.session_state['recall_score'] = 0

# Inyectar CSS para capturar el color de fondo
st.markdown(
    """
    <style>
    .stApp {
        background-color: var(--background-color);
    }
    </style>
    """,
    unsafe_allow_html=True
)


# Capturar el color de fondo actual
def get_background_color():
    # Verificar si es modo claro o oscuro
    return st.session_state.get("background_color", "#ffffff")




# Mostrar gr谩fico y bot贸n para entrenar
preview_placeholder = st.empty()
dynamic_placeholder = st.empty()

# Funci贸n para inicializar configuraci贸n de capas seg煤n el dataset
def initialize_layer_config(dataset):
    # Verificar si el dataset ya fue inicializado
    if "initialized_dataset" not in st.session_state or st.session_state["initialized_dataset"] != dataset:
        st.session_state["initialized_dataset"] = dataset  # Marcar el dataset como inicializado

        if dataset == 'CIFAR-10':
            # Capas base espec铆ficas de CIFAR-10
            st.session_state['layer_config'] = [
                {"name": "Conv2D1", "type": "Conv2D", "filters": 32, "kernel_size": 3, "activation": "relu", "base": True},
                {"name": "Conv2D2", "type": "Conv2D", "filters": 64, "kernel_size": 3, "activation": "relu", "base": True},
                {"name": "MaxPooling2D1", "type": "MaxPooling2D", "pool_size": 2, "base": True},
                {"name": "Flatten1", "type": "Flatten", "base": True},
                {"name": "Dense1", "type": "Dense", "neurons": 128, "activation": "relu", "base": True},
                {"name": "Dropout1", "type": "Dropout", "dropout_rate": 0.5, "base": True},
            ]
        # Reiniciar capas intermedias al cambiar dataset
        st.session_state['num_intermediate_layers'] = 0


# Funci贸n para manejar actualizaci贸n de capas intermedias
def update_intermediate_layers():
    num_layers = st.session_state['num_intermediate_layers']
    current_intermediate_layers = len([
        layer for layer in st.session_state['layer_config']
        if not layer.get("base", False)
    ])

    # Actualizar solo si hay un cambio
    if num_layers != current_intermediate_layers:
        # Eliminar capas intermedias actuales
        st.session_state['layer_config'] = [
            layer for layer in st.session_state['layer_config'] if layer.get("base", False)
        ]

        # Agregar nuevas capas intermedias
        for i in range(num_layers):
            st.session_state['layer_config'].insert(
                -2,  # Insertar antes de las capas finales (Flatten y Output)
                {
                    "name": f"IntermediateLayer{i + 1}",
                    "type": "Dense",
                    "neurons": 64,
                    "activation": "relu",
                    "base": False,
                }
            )


from sklearn.preprocessing import StandardScaler

# Cargar y dividir dataset
from sklearn.preprocessing import StandardScaler
def load_dataset(name, problem_type):
    # Cargar dataset completo con nombres de columnas correctos
    if name == "Iris":
        data = load_iris(as_frame=True)
        df = data.data
        df["species"] = pd.Series(data.target).map(dict(enumerate(data.target_names)))
    elif name == "Wine":
        data = load_wine(as_frame=True)
        df = data.data
        df["wine_class"] = pd.Series(data.target).map(dict(enumerate(data.target_names)))
    elif name == "Breast Cancer":
        data = load_breast_cancer(as_frame=True)
        df = data.data
        df["diagnosis"] = data.target
    elif name == "Digits":
        data = load_digits(as_frame=False)
        df = pd.DataFrame(data.data, columns=[f"pixel_{i}" for i in range(data.data.shape[1])])
        df["digit"] = data.target
    elif name == "Boston Housing":
        data = fetch_california_housing(as_frame=True)
        df = data.data
        df["median_value"] = data.target
    elif name == "Diabetes":
        data = load_diabetes(as_frame=True)
        df = data.data
        df["disease_progression"] = data.target
    else:
        raise ValueError("Dataset no soportado")

    return df



def initialize_graph(layers):
    """
    Inicializa una representaci贸n de la red neuronal con Input y Output 煤nicos.
    """
    if not layers:
        st.session_state['graph'] = []
        return

    # Redefinir gr谩fico con Input, capas intermedias y Output
    st.session_state['graph'] = [
        {'name': 'Input', 'num_neurons': 1, 'type': 'Input', 'layer_index': 0}
    ]

    for i, layer in enumerate(layers):
        st.session_state['graph'].append({
            'name': layer['name'],
            'num_neurons': layer.get('neurons', 4),  # Valor predeterminado
            'type': layer['type'],
            'layer_index': i + 1
        })

    st.session_state['graph'].append({
        'name': 'Output', 'num_neurons': 1, 'type': 'Output', 'layer_index': len(layers) + 1
    })



def generate_graph_fig(layers, active_layer=None, epoch=None, neurons_per_point=12, animated_layer=None, progress_step=0):
    """
    Genera el gr谩fico unificado para preview y din谩mico con animaci贸n.
    """
    fig = go.Figure()
    layer_positions = {}

    background_color = get_background_color()

    # Determinar tama帽o de texto basado en la cantidad de capas
    num_layers = len(layers)
    text_size = 12 if num_layers <= 10 else max(8, 12 - (num_layers - 10) // 2)

    # Calcular posiciones y conexiones primero
    for i, layer in enumerate(st.session_state['graph']):
        x_position = i * 300
        if layer['type'] in ["Dense", "Input", "Output"]:
            total_neurons = layer['num_neurons']
            num_points = math.ceil(total_neurons / neurons_per_point)
            y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]
        else:
            y_positions = [0]  # Una sola posici贸n para capas simb贸licas

        # Guardar posiciones
        for j, y in enumerate(y_positions):
            layer_positions[(i, j)] = (x_position, y)

    # Dibujar conexiones primero (en el fondo)
    for i in range(len(st.session_state['graph']) - 1):
        for j1 in layer_positions:
            if j1[0] == i:
                for j2 in layer_positions:
                    if j2[0] == i + 1:
                        x0, y0 = layer_positions[j1]
                        x1, y1 = layer_positions[j2]
                        is_current_connection = i == animated_layer
                        line_color = "lightblue" if not is_current_connection else f"rgba(0, 200, 200, {0.5 + progress_step / 2})"
                        line_width = 2 if not is_current_connection else 3
                        fig.add_trace(go.Scatter(
                            x=[x0, x1], y=[y0, y1],
                            mode="lines",
                            line=dict(width=line_width, color=line_color),
                            hoverinfo="none"
                        ))

    # Dibujar formas (capas) encima
    for i, layer in enumerate(st.session_state['graph']):
        x_position = i * 300
        if layer['type'] in ["Dense", "Input", "Output"]:
            total_neurons = layer['num_neurons']
            num_points = math.ceil(total_neurons / neurons_per_point)
            y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]
            if layer['type'] != "Dense":   
                text1=[f"{layer['name']}"]
                # Dibujar neuronas como puntos
                node_color = "gray" if i != animated_layer else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                for j, y in enumerate(y_positions):
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[y],
                        mode="markers",
                        marker=dict(size=20, color=node_color),
                        hoverinfo="none",
                        text=text1
                    ))
            else:
                text1=[f"{layer['type']}<br>{layer['num_neurons']} Neuronas"]
                # Dibujar neuronas como puntos
                node_color = "blue" if i != animated_layer else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                for j, y in enumerate(y_positions):
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[y],
                        mode="markers",
                        marker=dict(size=10, color=node_color),
                        hoverinfo="none",
                        text=text1
                    ))



        else:
            # Capas sin neuronas: formas simb贸licas
            if layer['type'] == "Conv2D":
                color = "green"
            elif layer['type'] == "MaxPooling2D":
                color = "purple"
            elif layer['type'] == "Flatten":
                color = "gray"
            elif layer['type'] == "Dropout":
                color = "orange"
            elif layer['type'] == "BatchNormalization":
                color = "pink"
            else:
                color = "black"

            fig.add_trace(go.Scatter(
                x=[x_position], y=[0],
                mode="markers",
                marker=dict(size=20, color=color),
                hoverinfo="none",
                text=[f"{layer['name']}<br>{layer['type']}"]
            ))

        # Etiquetas para Input/Output y capas
        label_text = (
            f"{layer['type']}"
            if layer['type'] in ["Dense", "Input", "Output"]
            else layer['type']
        )

        # Ajustar posici贸n de etiquetas alternadas
        if i % 2 == 0:
            label_y_offset = max(y_positions) + 1.5  # Posici贸n arriba
            label_position = "top center"
        else:
            label_y_offset = min(y_positions) - 2.5  # Posici贸n abajo (m谩s alejado)

        fig.add_trace(go.Scatter(
            x=[x_position], 
            y=[label_y_offset],
            mode="text",
            text=label_text,
            textposition=label_position,
            textfont=dict(size=text_size),
            hoverinfo="none"
        ))

    # Configuraci贸n del gr谩fico
    title = f"Training Progress - Epoch {epoch + 1}" if epoch is not None else "Network Architecture Preview"
    fig.update_layout(
        title=title,
        showlegend=False,
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        plot_bgcolor=background_color,  # Fondo del gr谩fico
        paper_bgcolor=background_color,  # Fondo del 谩rea alrededor del gr谩fico
        margin=dict(l=10, r=10, t=30, b=10)
    )

    return fig



def update_graph_with_smooth_color_transition(layers, epoch, placeholder, neurons_per_point=12, animation_steps=30):
    """
    Muestra una animaci贸n visual fluida de flujo entre capas mediante cambios progresivos de color.
    """
    total_layers = len(layers)
    background_color = get_background_color()

    for layer_index in range(total_layers - 1):  # No incluye la capa final
        for step in range(animation_steps):
            progress_step = step / animation_steps  # Progreso gradual del color
            
            fig = go.Figure()
            layer_positions = {}

            # Calcular posiciones de capas
            for i, layer in enumerate(layers):
                x_position = i * 300
                total_neurons = layer['num_neurons'] if layer['type'] in ["Dense", "Input", "Output"] else 1

                # Calcular cu谩ntos puntos representar
                num_points = math.ceil(total_neurons / neurons_per_point)
                y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]

                # Guardar posiciones de la capa
                for j, y in enumerate(y_positions):
                    layer_positions[(i, j)] = (x_position, y)

            # Dibujar conexiones primero (fondo)
            for i in range(total_layers - 1):  # No conecta despu茅s de Output
                for j1 in layer_positions:
                    if j1[0] == i:
                        for j2 in layer_positions:
                            if j2[0] == i + 1:
                                x0, y0 = layer_positions[j1]
                                x1, y1 = layer_positions[j2]

                                # Color de las conexiones
                                if i == layer_index:
                                    line_color = f"rgba(255, {int(progress_step * 255)}, 0, 1)"  # De naranja a azul
                                else:
                                    line_color = "lightblue"

                                fig.add_trace(go.Scatter(
                                    x=[x0, x1], y=[y0, y1],
                                    mode="lines",
                                    line=dict(width=2, color=line_color),
                                    hoverinfo="none"
                                ))

            # Dibujar puntos o formas de las capas (frente)
            for i, layer in enumerate(layers):
                x_position = i * 300
                total_neurons = layer['num_neurons'] if layer['type'] in ["Dense", "Input", "Output"] else 1
                num_points = math.ceil(total_neurons / neurons_per_point)
                y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]

                # Determinar color y forma de la capa
                node_color = "blue" if i != layer_index and i != layer_index + 1 else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                if layer['type'] in ["Dense"]:
                    # Dibujar nodos (puntos)
                    for j, y in enumerate(y_positions):
                        fig.add_trace(go.Scatter(
                            x=[x_position], y=[y],
                            mode="markers",
                            marker=dict(size=10, color=node_color),
                            hoverinfo="none"
                        ))
                else:
                    # Dibujar formas simb贸licas para otras capas
                    color = {
                        "Conv2D": ( "green"),
                        "MaxPooling2D": ( "purple"),
                        "Flatten": ("gray"),
                        "Dropout": ("orange"),
                        "BatchNormalization": ("pink")
                    }.get(layer['type'], ("black"))
                    
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[-1],
                        mode="markers",
                        marker=dict(size=20, color=color),
                        hoverinfo="none"
                    ))

                # A帽adir etiquetas
                label_text = (
                    f"{layer['type']}"
                    if layer['type'] in ["Dense", "Input", "Output"]
                    else layer['type']
                )

                # Ajustar posici贸n de etiquetas alternadas
                if i % 2 == 0:
                    label_y_offset = max(y_positions) + 1.5  # Posici贸n arriba
                    label_position = "top center"
                else:
                    label_y_offset = min(y_positions) - 2.5  # Posici贸n abajo (m谩s alejado)

                fig.add_trace(go.Scatter(
                    x=[x_position], 
                    y=[label_y_offset],
                    mode="text",
                    text=label_text,
                    textposition=label_position,
                    hoverinfo="none"
                ))

            # Configuraci贸n del gr谩fico
            fig.update_layout(
                title=f"Progreso del entrenamiento - poca {epoch + 1}" if epoch is not None else "Arquitectura del modelo",
                showlegend=False,
                xaxis=dict(visible=False),
                yaxis=dict(visible=False),
                plot_bgcolor=background_color,  # Fondo del gr谩fico
                paper_bgcolor=background_color,  # Fondo del 谩rea alrededor del gr谩fico
                margin=dict(l=10, r=10, t=30, b=10)
            )

            # Renderizar el gr谩fico
            placeholder.plotly_chart(
                fig,
                use_container_width=True,
                key=f"training_{epoch}_{layer_index}_{step}_{time.time()}"
            )
            time.sleep(0.03)  # Ajusta para controlar la fluidez




def preview_graph(layers, placeholder, neurons_per_point=12):
    """
    Genera un gr谩fico est谩tico con Input, capas y Output.
    """
    initialize_graph(layers)
    if not st.session_state['graph']:
        placeholder.empty()
        st.warning("No hay capas configuradas. Agregue una capa para visualizar el gr谩fico.")
        return

    fig = generate_graph_fig(layers, neurons_per_point=neurons_per_point)
    placeholder.plotly_chart(fig, use_container_width=True, key=f"preview_{time.time()}")





    
# Funci贸n para registrar logs
def log_event(log_placeholder, message):
    st.session_state['logs'].append(message)
    log_placeholder.text_area("Registro del entrenamiento", "\n".join(st.session_state['logs']), height=300)









# Funci贸n para entrenar el modelo con gr谩ficos din谩micos
from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix,ConfusionMatrixDisplay
import pandas as pd



def train_model_classification(layers, hyperparams, preview_placeholder, dynamic_placeholder):
    # Limpiar logs y placeholders
    st.session_state['logs'] = []
    st.session_state['loss_values'] = []
    st.session_state['accuracy_values'] = []
    st.session_state['val_loss_values'] = []
    st.session_state['val_accuracy_values'] = []

    preview_placeholder.empty()

    # Configuraci贸n de contenedores
    col_dynamic, col_metrics = st.columns([6, 1])
    dynamic_graph_placeholder = col_dynamic.empty()
    log_placeholder = st.empty()

    # Validar splits (ya existente)
    if not st.session_state.get("dataset_split", False):
        st.error("El dataset no est谩 configurado correctamente.")
        return

    splits = st.session_state['splits']
    if len(splits) == 4:
        X_train, X_test, y_train, y_test = splits
        X_val, y_val = None, None
    elif len(splits) == 6:
        X_train, X_val, X_test, y_train, y_val, y_test = splits
    else:
        st.error("La divisi贸n del dataset no es v谩lida. Reconfigura el dataset.")
        return


    # Configurar Early Stopping si est谩 habilitado
    callbacks = []
    early_stopping_config = st.session_state.get('early_stopping', {})
    if early_stopping_config.get("enabled", False):
        patience = early_stopping_config.get("patience", 0)
        monitor_metric = early_stopping_config.get("monitor", "val_loss")
        early_stopping = EarlyStopping(
            monitor=monitor_metric,
            patience=patience,
            restore_best_weights=True,
            verbose=1
        )
        callbacks.append(early_stopping)
        log_event(log_placeholder, f"Early Stopping habilitado: Monitoreando '{monitor_metric}' con paciencia de {patience} 茅pocas.")



    # Validar dimensiones antes del entrenamiento
    if len(st.session_state["splits"][3]) != len(st.session_state["splits"][3]):
        st.error("Dimensiones inconsistentes entre X_test y y_test_original antes del entrenamiento.")
        st.stop()

    num_classes = len(np.unique(y_train))
    y_train = to_categorical(y_train, num_classes)
    if y_val is not None:
        y_val = to_categorical(y_val, num_classes)
    y_test_original = y_test
    y_test = to_categorical(y_test, num_classes)

    if X_test.isnull().values.any():
        st.error("X_test contiene valores NaN. Revisa el preprocesamiento.")
        st.stop()



    # Crear modelo
    input_shape = (X_train.shape[1],)
    model = Sequential()
    for i, layer in enumerate(layers):
        if layer['type'] == "Dense":
            model.add(Dense(layer['neurons'], activation=layer['activation'], input_shape=input_shape if i == 0 else None,kernel_regularizer=l2(layer['l2']) if layer.get('enable_l2', False) else None))
        elif layer['type'] == "Dropout":
            model.add(Dropout(layer['dropout_rate']))

    # Configurar optimizador y compilar modelo
    optimizer = Adam(learning_rate=hyperparams['learning_rate'])
    model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

    # Entrenar modelo
    for epoch in range(hyperparams['epochs']):
        log_event(log_placeholder, f"poca {epoch + 1}/{hyperparams['epochs']} iniciada.")

        if X_val is not None and y_val is not None:
            # Con conjunto de validaci贸n  
            history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                                batch_size=hyperparams['batch_size'], epochs=1, verbose=0, callbacks=callbacks)
        else:
            # Sin conjunto de validaci贸n
            history = model.fit(X_train, y_train, batch_size=hyperparams['batch_size'],
                                epochs=1, verbose=0, callbacks=callbacks)

        # Registrar m茅tricas
        train_loss = history.history['loss'][0]
        train_accuracy = history.history['accuracy'][0]
        log_event(log_placeholder, f"poca {epoch + 1}: P茅rdida en entrenamiento: {train_loss:.4f}")
        log_event(log_placeholder, f"poca {epoch + 1}: Precisi贸n en entrenamiento: {train_accuracy:.4f}")

        if X_val is not None and y_val is not None:
            val_loss = history.history['val_loss'][0]
            val_accuracy = history.history['val_accuracy'][0]
            log_event(log_placeholder, f"poca {epoch + 1}: P茅rdida en validaci贸n: {val_loss:.4f}")
            log_event(log_placeholder, f"poca {epoch + 1}: Precisi贸n en validaci贸n: {val_accuracy:.4f}")

        st.session_state['loss_values'].append(train_loss)
        st.session_state['accuracy_values'].append(train_accuracy)
        if X_val is not None and y_val is not None:
            st.session_state['val_loss_values'].append(val_loss)
            st.session_state['val_accuracy_values'].append(val_accuracy)

        # Actualizar visualizaci贸n din谩mica
        update_graph_with_smooth_color_transition(
            st.session_state['graph'], epoch, dynamic_graph_placeholder, neurons_per_point=10, animation_steps=30
        )

    # M茅tricas finales
    st.write(f"Dimensiones de X_test: {X_test.shape}")
    st.write(f"Dimensiones de y_test_original: {y_test_original.shape}")


    if np.isnan(X_test.values).any():
        st.error("X_test contiene valores NaN. Revisa el preprocesamiento.")
        st.stop()

    st.session_state["modelDownload"] = model
    st.success("Entrenamiento finalizado con 茅xito.")

def train_model_regression(layers, hyperparams, preview_placeholder, dynamic_placeholder):
    # Limpiar logs y placeholders
    st.session_state['logs'] = []
    st.session_state['loss_values'] = []
    st.session_state['mae_values'] = []
    st.session_state['val_loss_values'] = []
    st.session_state['val_mae_values'] = []

    preview_placeholder.empty()

    # Configuraci贸n de contenedores
    col_dynamic, col_metrics = st.columns([6, 1])
    dynamic_graph_placeholder = col_dynamic.empty()
    log_placeholder = st.empty()

    # Validar configuraci贸n del dataset
    if not st.session_state.get("dataset_split", False):
        st.error("El dataset no est谩 configurado correctamente. Config煤ralo antes de entrenar.")
        return

    # Recuperar splits
    splits = st.session_state['splits']
    if len(splits) == 4:
        X_train, X_test, y_train, y_test = splits
        X_val, y_val = None, None
    elif len(splits) == 6:
        X_train, X_val, X_test, y_train, y_val, y_test = splits
    else:
        st.error("La divisi贸n del dataset no es v谩lida. Reconfigura el dataset.")
        return


    # Configurar Early Stopping si est谩 habilitado
    callbacks = []
    early_stopping_config = st.session_state.get('early_stopping', {})
    if early_stopping_config.get("enabled", False):
        patience = early_stopping_config.get("patience", 0)
        monitor_metric = early_stopping_config.get("monitor", "val_loss")
        early_stopping = EarlyStopping(
            monitor=monitor_metric,
            patience=patience,
            restore_best_weights=True,
            verbose=1
        )
        callbacks.append(early_stopping)
        log_event(log_placeholder, f"Early Stopping habilitado: Monitoreando '{monitor_metric}' con paciencia de {patience} 茅pocas.")


    # Crear modelo
    input_shape = (X_train.shape[1],)
    model = Sequential()
    for i, layer in enumerate(layers):
        if layer['type'] == "Dense":
            model.add(Dense(layer['neurons'], activation=layer['activation'],
                            input_shape=input_shape if i == 0 else None,
                            kernel_regularizer=l2(layer['l2']) if layer.get('enable_l2', False) else None))
        elif layer['type'] == "Dropout":
            model.add(Dropout(layer['dropout_rate']))

    # Configurar optimizador y compilar modelo
    optimizer = Adam(learning_rate=hyperparams['learning_rate'])
    model.compile(optimizer=optimizer, loss="mean_squared_error", metrics=["mae"])

    # Entrenar modelo
    for epoch in range(hyperparams['epochs']):
        log_event(log_placeholder, f"poca {epoch + 1}/{hyperparams['epochs']} iniciada.")

        if X_val is not None and y_val is not None:
            history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                                batch_size=hyperparams['batch_size'], epochs=1, verbose=0, callbacks=callbacks)
        else:
            history = model.fit(X_train, y_train, batch_size=hyperparams['batch_size'],
                                epochs=1, verbose=0, callbacks=callbacks)

        # Registrar m茅tricas de entrenamiento
        train_loss = history.history['loss'][0]
        train_mae = history.history['mae'][0]
        log_event(log_placeholder, f"Entrenamiento - P茅rdida (MSE): {train_loss:.4f}, Precisi贸n (MAE): {train_mae:.4f}")

        # Registrar m茅tricas de validaci贸n, si aplican
        if X_val is not None:
            val_loss = history.history['val_loss'][0]
            val_mae = history.history['val_mae'][0]
            log_event(log_placeholder, f"Validaci贸n - P茅rdida (MSE): {val_loss:.4f}, Precisi贸n (MAE): {val_mae:.4f}")

        # Actualizar m茅tricas para gr谩ficos
        st.session_state['loss_values'].append(train_loss)
        st.session_state['mae_values'].append(train_mae)
        if X_val is not None:
            st.session_state['val_loss_values'].append(val_loss)
            st.session_state['val_mae_values'].append(val_mae)

        # Actualizar visualizaci贸n din谩mica
        update_graph_with_smooth_color_transition(
            st.session_state['graph'], epoch, dynamic_graph_placeholder, neurons_per_point=10, animation_steps=30
        )

    # Guardar modelo
    st.session_state["modelDownload"] = model
    st.success("Entrenamiento finalizado con 茅xito.")




current_intermediate_layers = max(0, len(st.session_state['layer_config']) - 6)

# Inicializar estado para el n煤mero de capas
if 'num_layers_selected' not in st.session_state:
    st.session_state['num_layers_selected'] = len(st.session_state['layer_config'])
if 'previous_layer_config' not in st.session_state:
    st.session_state['previous_layer_config'] = []
# Inicializar el estado para capas intermedias si no existe
if 'num_intermediate_layers' not in st.session_state:
    st.session_state['num_intermediate_layers'] = current_intermediate_layers

# Funci贸n para manejar la actualizaci贸n de capas
def update_layer_config():
    num_layers = st.session_state['num_layers_selected']
    current_num_layers = len(st.session_state['layer_config'])

    if num_layers > current_num_layers:
        # A帽adir capas
        for i in range(current_num_layers, num_layers):
            st.session_state['layer_config'].append({
                "name": f"Layer{i + 1}",
                "type": "Dense",
                "neurons": 64,
            })
    elif num_layers < current_num_layers:
        # Reducir capas
        st.session_state['layer_config'] = st.session_state['layer_config'][:num_layers]

    # Guardar una copia del estado actual para evitar rebotes
    st.session_state['previous_layer_config'] = st.session_state['layer_config'][:]



# Diccionario de arquitecturas predefinidas
architectures = {
    "Iris": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 16, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 3, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ],
        "Deep MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 16, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense4", "type": "Dense", "neurons": 3, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Wine": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 3, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Breast Cancer": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 16, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 2, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Digits": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 10, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Boston Housing": {
        "Simple Regression": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Output", "type": "Dense", "neurons": 1, "activation": "linear", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Diabetes": {
        "Simple Regression": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Output", "type": "Dense", "neurons": 1, "activation": "linear", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    }
}


# Funci贸n para manejar el cambio de dataset
if "selected_dataset_previous" not in st.session_state or st.session_state["selected_dataset_previous"] != st.session_state["selected_dataset"]:
    st.session_state["selected_dataset_previous"] = st.session_state["selected_dataset"]
    
    # Reinicia configuraci贸n de capas y arquitectura
    initialize_layer_config(st.session_state["selected_dataset"])



from sklearn.metrics import mean_squared_error





# Configuraci贸n del Dataset
from sklearn.impute import SimpleImputer
import numpy as np




# Funci贸n principal para configurar el dataset
def configure_dataset():
    st.sidebar.header("Configuraci贸n del Dataset")

    # Paso 1: Selecci贸n del dataset
    st.sidebar.subheader("Paso 1: Selecci贸n del Dataset")
    problem_type = st.sidebar.selectbox(
        "Seleccione el tipo de problema",
        ["Clasificaci贸n", "Regresi贸n"],
        key="problem_type_selectbox",
        help="Clasificaci贸n es para predecir categor铆as, como tipos de flores. Regresi贸n es para predecir valores continuos, como precios de casas."
    )
    st.sidebar.info("""
        **Definici贸n**: El dataset es el conjunto de datos que ser谩 utilizado para entrenar y evaluar el modelo. 

        **Sugerencia**: Selecciona un tipo de problema (clasificaci贸n o regresi贸n) y el dataset apropiado para el tipo de an谩lisis que deseas realizar. 
        Por ejemplo:
        - **Clasificaci贸n**: Usar谩s el modelo para predecir categor铆as, como tipos de flores.
        - **Regresi贸n**: Usar谩s el modelo para predecir valores continuos, como precios de casas.
        """)

    st.session_state['problem_type'] = problem_type

    dataset_name = st.sidebar.selectbox(
        "Seleccione el dataset",
        ["Iris", "Wine", "Breast Cancer", "Digits"] if problem_type == "Clasificaci贸n" else ["Boston Housing", "Diabetes"],
        key="dataset_name_selectbox",
        help="Elija el dataset apropiado seg煤n el tipo de problema que seleccion贸 anteriormente."
    )

    st.session_state['selected_dataset'] = dataset_name

    if st.sidebar.button("Cargar Dataset", key="load_dataset_button",):
    # Reiniciar el estado relacionado con el dataset al cambiarlo
        if st.session_state.get("dataset_loaded", False):
            st.session_state["dataset_loaded"] = False
            st.session_state.pop("full_dataset", None)
            st.session_state.pop("target_variable", None)
            st.session_state.pop("X_original", None)
            st.session_state.pop("y_original", None)
            st.session_state.pop("splits", None)
            st.session_state["dataset_split"] = False

    # Cargar el nuevo dataset
    df = load_dataset(dataset_name, problem_type)
    if df.empty:
        st.sidebar.error("El dataset cargado est谩 vac铆o. Seleccione un dataset v谩lido.")
        st.stop()

    # Guardar el dataset en el estado de la sesi贸n
    st.session_state["dataset_loaded"] = True
    st.session_state["full_dataset"] = df
    st.success("Dataset cargado con 茅xito.")
    st.sidebar.write("Vista previa del dataset:", df.head())

    
    # Paso 2: Selecci贸n de la variable objetivo
    if st.session_state.get("dataset_loaded", False):
        st.sidebar.subheader("Paso 2: Selecci贸n de la Variable Objetivo")
        target_variable = st.sidebar.selectbox(
            "Seleccione la variable objetivo:",
            st.session_state["full_dataset"].columns,
            key="target_variable_selectbox",
            help="Seleccione la columna que desea predecir. Por ejemplo, en un dataset de casas, puede ser el precio."
        )
        if target_variable != st.session_state.get("target_variable", None):
            st.session_state["target_variable"] = target_variable
            st.session_state["X_original"] = st.session_state["full_dataset"].drop(columns=[target_variable]).copy()
            st.session_state["y_original"] = st.session_state["full_dataset"][target_variable].copy()

        st.sidebar.success(f"Variable objetivo seleccionada: {st.session_state['target_variable']}")
        st.sidebar.write("Caracter铆sticas (X):", st.session_state["X_original"].head())
        # Verificar si y_original es un numpy.ndarray y convertirlo en un DataFrame/Series antes de usar .head()
        if isinstance(st.session_state["y_original"], np.ndarray):
            y_display = pd.Series(st.session_state["y_original"].ravel())
        else:
            y_display = st.session_state["y_original"]

        # Mostrar la vista previa de la variable objetivo
        st.sidebar.write("Variable objetivo (y):", y_display.head())
                # Paso 3: Eliminaci贸n de columnas

    if st.session_state.get("target_variable"):
        st.sidebar.subheader("Paso 3: Eliminaci贸n de Columnas")
        selected_columns = st.sidebar.multiselect(
            "Seleccione las columnas a eliminar",
            st.session_state["X_original"].columns,
            key="columns_to_drop_multiselect",
            help="Elimine columnas irrelevantes o redundantes, como IDs o nombres que no aporten al an谩lisis."
        )
        if st.sidebar.button("Aplicar Eliminaci贸n", key="apply_column_removal_button"):
            if target_variable in selected_columns:
                st.sidebar.error("La variable objetivo no puede ser eliminada.")
                st.stop()
            st.session_state["X_original"] = st.session_state["X_original"].drop(columns=selected_columns).copy()
            if st.session_state["X_original"].shape[1] == 0:
                st.sidebar.error("No quedan columnas en el dataset despu茅s de la eliminaci贸n.")
                st.stop()
            st.session_state["columns_removed"] = True
            st.success("Columnas eliminadas con 茅xito.")
            st.sidebar.write("Vista previa del dataset actualizado:", st.session_state["X_original"].head())

        # Paso 4: Manejo de valores nulos
        st.sidebar.subheader("Paso 4: Manejo de Valores Nulos")
        X = st.session_state["X_original"]
        nulos_totales = X.isnull().sum().sum()

        if nulos_totales == 0:
            st.sidebar.success("No existen valores nulos en el dataset.")
            st.session_state["missing_handled"] = True
        else:
            numeric_null_option = st.sidebar.selectbox(
                "Valores Nulos - Num茅ricos",
                ["Eliminar filas", "Reemplazar con 0", "Reemplazar con la media", "Reemplazar con la mediana"],
                key="numeric_null_option_selectbox",
                help="Elija c贸mo manejar valores nulos en columnas num茅ricas. Por ejemplo, reemplazar con la media es 煤til en la mayor铆a de los casos."
            )
            categorical_null_option = st.sidebar.selectbox(
                "Valores Nulos - Categ贸ricos",
                ["Eliminar filas", "Reemplazar con 'Ninguno'", "Reemplazar con la moda"],
                key="categorical_null_option_selectbox",
                help="Elija c贸mo manejar valores nulos en columnas categ贸ricas. Por ejemplo, reemplazar con la moda es com煤n en problemas de clasificaci贸n."
            )
            if st.sidebar.button("Aplicar Manejo de Nulos", key="apply_null_handling_button"):
                X_cleaned = handle_nulls(st.session_state["X_original"], numeric_null_option, categorical_null_option)
                st.session_state["X_original"] = X_cleaned
                st.session_state["missing_handled"] = True
                st.sidebar.success("Manejo de valores nulos aplicado con 茅xito.")
                st.sidebar.write("Vista previa del dataset actualizado:", X_cleaned.head())

        # Paso 5: Codificaci贸n de variables categ贸ricas
        if st.session_state.get("missing_handled", False):
            st.sidebar.subheader("Paso 5: Codificaci贸n de Variables Categ贸ricas")
            categorical_cols = st.session_state["X_original"].select_dtypes(include=["object", "category"]).columns

            # Codificar columnas categ贸ricas en X
            if len(categorical_cols) == 0:
                st.sidebar.success("No existen columnas categ贸ricas en las caracter铆sticas (X).")
                st.session_state["categorical_encoded"] = True
            else:
                encoding_option = st.sidebar.selectbox(
                    "M茅todo de Codificaci贸n para caracter铆sticas (X):",
                    ["One-Hot Encoding", "Label Encoding"],
                    key="encoding_option_selectbox",
                    help="Use One-Hot Encoding para categor铆as sin orden (como colores). Use Label Encoding para categor铆as ordenadas (como niveles de educaci贸n)."
                )
                if st.sidebar.button("Aplicar Codificaci贸n en X", key="apply_encoding_button"):
                    X_encoded = encode_categorical(st.session_state["X_original"], encoding_option)
                    st.session_state["X_original"] = X_encoded
                    st.session_state["categorical_encoded"] = True
                    st.sidebar.success("Codificaci贸n aplicada con 茅xito a las caracter铆sticas (X).")
                    st.sidebar.write("Vista previa del dataset actualizado (X):", X_encoded.head())

            # Codificar la variable objetivo (y) si es categ贸rica y el problema es de clasificaci贸n
            if st.session_state["problem_type"] == "Clasificaci贸n" and st.session_state["y_original"].dtype == "object":
                st.sidebar.subheader("Codificaci贸n de la Variable Objetivo (y)")
                if st.sidebar.button("Aplicar Codificaci贸n en y", key="apply_y_encoding_button"):
                    label_encoder = LabelEncoder()
                    st.session_state["y_original"] = label_encoder.fit_transform(st.session_state["y_original"])
                    st.session_state["label_encoder"] = label_encoder
                    st.sidebar.success("Codificaci贸n aplicada con 茅xito a la variable objetivo (y).")
                    st.sidebar.write("Vista previa de la variable objetivo codificada (y):", st.session_state["y_original"][:5])

        # Paso 6: Normalizaci贸n y escalamiento
        if st.session_state.get("categorical_encoded", False):
            st.sidebar.subheader("Paso 6: Normalizaci贸n y Escalamiento")
            numeric_columns = st.session_state["X_original"].select_dtypes(include=["float64", "int64"]).columns
            scaling_option = st.sidebar.selectbox(
                "M茅todo de Escalamiento:",
                ["StandardScaler", "MinMaxScaler"],
                key="scaling_option_selectbox",
                help="StandardScaler es 煤til para datos con distribuci贸n normal. MinMaxScaler es 煤til para datos en rangos espec铆ficos."
            )
            if st.sidebar.button("Aplicar Escalamiento", key="apply_scaling_button",help="Aplica un escalamiento a la variable objetivo para mejorar el rendimiento del modelo en problemas de regresi贸n."):
                numeric_columns = st.session_state["X_original"].select_dtypes(include=["float64", "int64"]).columns
                scaler_x = StandardScaler() if scaling_option == "StandardScaler" else MinMaxScaler()
                scaled_data = scaler_x.fit_transform(st.session_state["X_original"][numeric_columns])
                st.session_state["X_original"].loc[:, numeric_columns] = scaled_data
                st.session_state["scaler_x"] = scaler_x
                # Escalamiento de la variable objetivo (y) en caso de regresi贸n
                if st.session_state["problem_type"] == "Regresi贸n":
                    st.sidebar.subheader("Escalamiento de la Variable Objetivo (y)")
                    scaler_y = StandardScaler() if scaling_option == "StandardScaler" else MinMaxScaler()
                    y_scaled = scaler_y.fit_transform(st.session_state["y_original"].values.reshape(-1, 1))
                    st.session_state["y_original"] = y_scaled
                    st.session_state["scaler_y"] = scaler_y  # Guardar el escalador para revertir predicciones
                    st.sidebar.success("Escalamiento aplicado a la variable objetivo (y).")
                    st.sidebar.write("Vista previa de la variable objetivo escalada:", st.session_state["y_original"][:5])                 
                st.session_state["scaling_done"] = True
                st.sidebar.success("Escalamiento aplicado con 茅xito.")
                st.sidebar.write("Vista previa del dataset escalado:", st.session_state["X_original"].head())

        # Validar que y_original no haya sido alterado
        if st.session_state["problem_type"] == "Clasificaci贸n" and st.session_state["y_original"].dtype not in [np.int32, np.int64, np.float32, np.float64]:
            st.write(st.session_state["y_original"].dtype)
            st.sidebar.error("La variable objetivo debe seguir siendo num茅rica despu茅s del escalamiento. Verifica el flujo.")
            st.stop()

        # Paso 7: Divisi贸n del dataset
        if st.session_state.get("scaling_done", False):
            st.sidebar.subheader("Paso 7: Divisi贸n del Dataset")
            
            split_type = st.sidebar.selectbox(
                "Tipo de Divisi贸n:",
                ["Entrenamiento y Prueba", "Entrenamiento, Validaci贸n y Prueba"],
                key="split_type_selectbox",
                help="Entrenamiento y Prueba es una divisi贸n b谩sica. Agregar Validaci贸n ayuda a ajustar el modelo y prevenir sobreajuste."
            )
            st.session_state["split_type"] = split_type
            train_ratio = st.sidebar.slider("Entrenamiento (%)", 10, 90, 70, key="train_ratio_slider",help="Seleccione el porcentaje de datos que se usar谩n para entrenar el modelo. Usualmente es entre 70-80%.")

            # Calcular proporciones para validaci贸n y prueba
            if split_type == "Entrenamiento, Validaci贸n y Prueba":
                val_ratio = st.sidebar.slider("Validaci贸n (%)", 5, 50, 15, key="val_ratio_slider",help="El porcentaje restante se divide entre validaci贸n y prueba. Aseg煤rate de dejar suficientes datos para cada conjunto.")
                test_ratio = 100 - train_ratio - val_ratio
            else:
                val_ratio = 0
                test_ratio = 100 - train_ratio


            st.sidebar.text(f"Prueba (%): {test_ratio}")
            
            # Bot贸n para aplicar divisi贸n
            if st.sidebar.button("Aplicar Divisi贸n", key="apply_split_button") and not st.session_state.get("dataset_split", False):
                if split_type == "Entrenamiento y Prueba":
                    # Divisi贸n simple: Entrenamiento y Prueba
                    X_train, X_test, y_train, y_test = train_test_split(
                        st.session_state["X_original"], st.session_state["y_original"],
                        test_size=test_ratio / 100, random_state=42
                    )
                    # Guardar y_test_original antes de cualquier transformaci贸n
                    st.session_state['y_test_original'] = y_test.copy()
                    st.session_state["splits"] = (X_train, X_test, y_train, y_test)

                elif split_type == "Entrenamiento, Validaci贸n y Prueba":
                    # Divisi贸n avanzada: Entrenamiento, Validaci贸n y Prueba
                    X_train, X_temp, y_train, y_temp = train_test_split(
                        st.session_state["X_original"], st.session_state["y_original"],
                        test_size=(val_ratio + test_ratio) / 100, random_state=42
                    )
                    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)
                    X_val, X_test, y_val, y_test = train_test_split(
                        X_temp, y_temp, test_size=1 - val_ratio_adjusted, random_state=42
                    )
                    # Despu茅s de la divisi贸n

                    st.session_state['y_test_original'] = y_test.copy()
                    
                    st.session_state["splits"] = (X_train, X_val, X_test, y_train, y_val, y_test)
                    # Verificar dimensiones despu茅s de la divisi贸n
                    if len(X_test) != len(y_test):
                        st.error("Dimensiones inconsistentes entre X_test y y_test despu茅s de la divisi贸n.")

                st.session_state['train_ratio'] = train_ratio
                st.session_state['val_ratio'] = val_ratio
                st.session_state['test_ratio'] = test_ratio


                # Verificaci贸n de dimensiones
                splits = st.session_state["splits"]
                
                if len(splits) == 4:
                    X_train, X_test, y_train, y_test = splits
                elif len(splits) == 6:
                    X_train, X_val, X_test, y_train, y_val, y_test = splits
                
                if len(X_test) != len(y_test):
                    st.error("Dimensiones inconsistentes entre X_test y y_test despu茅s de la divisi贸n.")
                    st.stop()
                if problem_type == "Clasificaci贸n":
                    num_classes = len(np.unique(y_train))
                    st.session_state['num_classes'] = num_classes
                # Guardar copia de y_test_original para referencia
                st.session_state["y_test_original"] = y_test.copy()

                st.session_state["dataset_split"] = True
                st.sidebar.success("Divisi贸n del dataset realizada con 茅xito.")


    # Mensaje final
    if st.session_state.get("dataset_split", False):
        st.sidebar.header("隆Configuraci贸n Completada!")
        st.sidebar.success("El dataset est谩 listo. Ahora puedes proceder a entrenar el modelo.")




# Funci贸n para manejar valores nulos
def handle_nulls(X, numeric_null_option, categorical_null_option):
    """
    Maneja los valores nulos en los datos para columnas num茅ricas y categ贸ricas.

    Args:
        X (pd.DataFrame): Datos de entrada.
        numeric_null_option (str): Opci贸n seleccionada para manejar valores nulos num茅ricos.
        categorical_null_option (str): Opci贸n seleccionada para manejar valores nulos categ贸ricos.

    Returns:
        pd.DataFrame: DataFrame con valores nulos gestionados.
    """
    from sklearn.impute import SimpleImputer

    # Copiar el DataFrame para no modificar el original
    X_cleaned = X.copy()

    # Manejo de columnas num茅ricas
    numeric_cols = X_cleaned.select_dtypes(include=['float64', 'int64']).columns
    if not numeric_cols.empty:
        if numeric_null_option == "Eliminar filas":
            X_cleaned.dropna(subset=numeric_cols, inplace=True)
        elif numeric_null_option == "Reemplazar con 0":
            X_cleaned[numeric_cols] = X_cleaned[numeric_cols].fillna(0)
        elif numeric_null_option == "Reemplazar con la media":
            imputer = SimpleImputer(strategy='mean')
            X_cleaned[numeric_cols] = imputer.fit_transform(X_cleaned[numeric_cols])
        elif numeric_null_option == "Reemplazar con la mediana":
            imputer = SimpleImputer(strategy='median')
            X_cleaned[numeric_cols] = imputer.fit_transform(X_cleaned[numeric_cols])
    else:
        st.sidebar.success("No hay valores nulos en las columnas num茅ricas.")

    # Manejo de columnas categ贸ricas
    categorical_cols = X_cleaned.select_dtypes(include=['object', 'category']).columns
    if not categorical_cols.empty:
        if categorical_null_option == "Eliminar filas":
            X_cleaned.dropna(subset=categorical_cols, inplace=True)
        elif categorical_null_option == "Reemplazar con 'Ninguno'":
            X_cleaned[categorical_cols] = X_cleaned[categorical_cols].fillna('Ninguno')
        elif categorical_null_option == "Reemplazar con la moda":
            imputer = SimpleImputer(strategy='most_frequent')
            X_cleaned[categorical_cols] = imputer.fit_transform(X_cleaned[categorical_cols])
    else:
        st.sidebar.success("No hay valores nulos en las columnas categ贸ricas.")

    return X_cleaned




# Funci贸n para codificar variables categ贸ricas
def encode_categorical(X, encoding_option):
    """
    Codifica las variables categ贸ricas seg煤n la opci贸n seleccionada: One-Hot Encoding o Label Encoding.

    Args:
        X (pd.DataFrame): Datos de entrada.
        encoding_option (str): M茅todo de codificaci贸n seleccionado ("One-Hot Encoding" o "Label Encoding").

    Returns:
        pd.DataFrame: DataFrame transformado con la codificaci贸n aplicada.
    """
    # Identificar columnas categ贸ricas
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns

    if not categorical_cols.empty:
        if encoding_option == "One-Hot Encoding":
            # Aplicar One-Hot Encoding
            X = pd.get_dummies(X, columns=categorical_cols)
        elif encoding_option == "Label Encoding":
            # Aplicar Label Encoding
            from sklearn.preprocessing import LabelEncoder
            label_encoder = LabelEncoder()
            for col in categorical_cols:
                X[col] = label_encoder.fit_transform(X[col].astype(str))
        else:
            raise ValueError(f"M茅todo de codificaci贸n no soportado: {encoding_option}")
    else:
        st.sidebar.success("No hay variables categ贸ricas en el dataset.")

    return X



def validate_layer_config(layer_config, problem_type):
    warnings = []
    output_layer = layer_config[-1]  # ltima capa
    num_classes = st.session_state['num_classes']
    # Validar que la 煤ltima capa sea adecuada para el problema
        
    if st.session_state['splits'] == []:  
        warnings.append("Primero debes dividir el dataset antes de entrenar el modelo.")
        st.session_state['disabled_button_train'] = True
        return warnings
    if problem_type == "Clasificaci贸n":
        if num_classes == 2:  # Clasificaci贸n Binaria
            if output_layer['neurons'] != 1 or output_layer['activation'] != "sigmoid":
                warnings.append("Para clasificaci贸n binaria, la capa de salida debe tener 1 neurona con activaci贸n 'sigmoid'.")
        elif num_classes > 2:  # Clasificaci贸n Multiclase
            if output_layer['neurons'] != num_classes or output_layer['activation'] != "softmax":
                warnings.append(f"Para clasificaci贸n multiclase, la capa de salida debe tener {num_classes} neuronas con activaci贸n 'softmax'.")
    elif problem_type == "Regresi贸n":
        if output_layer['neurons'] != 1 or output_layer['activation'] != "linear":
            warnings.append("Para problemas de regresi贸n, la capa de salida debe tener 1 neurona con activaci贸n 'linear'.")
    elif problem_type == "Clasificaci贸n Multietiqueta":
        if output_layer['neurons'] != num_classes or output_layer['activation'] != "sigmoid":
            warnings.append(f"Para clasificaci贸n multietiqueta, la capa de salida debe tener {num_classes} neuronas con activaci贸n 'sigmoid'.")

    # Validar uso adecuado de capas Flatten
    flatten_count = sum(1 for layer in layer_config if layer['type'] == "Flatten")
    if flatten_count > 1:
        warnings.append("Se detectaron m煤ltiples capas 'Flatten'. Generalmente s贸lo se necesita una.")
    elif flatten_count == 0 and any(layer['type'] in ["Conv2D", "MaxPooling2D"] for layer in layer_config):
        warnings.append("Se requiere al menos una capa 'Flatten' despu茅s de 'Conv2D' o 'MaxPooling2D' para conectar con capas densas.")

    # Validar uso de Dropout en redes profundas
    if len(layer_config) > 5 and not any(layer['type'] == "Dropout" for layer in layer_config):
        warnings.append("Se recomienda incluir capas 'Dropout' para prevenir el sobreajuste en redes profundas.")

    # Validar configuraci贸n de Conv2D y MaxPooling2D
    if any(layer['type'] == "Conv2D" for layer in layer_config) and not any(layer['type'] == "MaxPooling2D" for layer in layer_config):
        warnings.append("Se recomienda agregar una capa 'MaxPooling2D' despu茅s de 'Conv2D' para reducir dimensiones.")

    # Validar que Conv2D tenga un n煤mero razonable de filtros
    for layer in layer_config:
        if layer['type'] == "Conv2D" and layer.get('filters', 0) < 8:
            warnings.append("Las capas 'Conv2D' deber铆an tener al menos 8 filtros para ser efectivas.")

    # Validar que Dropout tenga valores razonables
    for layer in layer_config:
        if layer['type'] == "Dropout" and not (0.0 < layer.get('dropout_rate', 0.0) <= 0.9):
            warnings.append("La capa 'Dropout' debe tener una tasa entre 0.1 y 0.9.")

    # Validar que las capas Dense tengan un n煤mero positivo de neuronas
    for layer in layer_config:
        if layer['type'] == "Dense" and layer.get('neurons', 0) <= 0:
            warnings.append("Las capas 'Dense' deben tener un n煤mero positivo de neuronas.")

    # Validar que la primera capa sea adecuada para el dataset
    if problem_type == "Clasificaci贸n" and layer_config[0]['type'] != "Dense":
        warnings.append("La primera capa debe ser de tipo 'Dense' para manejar correctamente los datos de entrada en problemas de clasificaci贸n.")
    elif problem_type == "Regresi贸n" and layer_config[0]['type'] != "Dense":
        warnings.append("La primera capa debe ser de tipo 'Dense' para manejar correctamente los datos de entrada en problemas de regresi贸n.")

    # Validar que BatchNormalization no sea la 煤ltima capa
    if layer_config[-1]['type'] == "BatchNormalization":
        warnings.append("La 煤ltima capa no debe ser 'BatchNormalization'. Considera moverla antes de la capa de salida.")

    # Validar que las dimensiones de salida sean compatibles con el problema
    if problem_type == "Clasificaci贸n":
        num_classes = st.session_state.get('num_classes', 0)
        if num_classes > 2 and layer_config[-1].get('neurons', 1) != num_classes:
            warnings.append(f"La 煤ltima capa debe tener {num_classes} neuronas para coincidir con el n煤mero de clases en el problema de clasificaci贸n.")
    elif problem_type == "Regresi贸n":
        if layer_config[-1].get('neurons', 1) != 1:
            warnings.append("La 煤ltima capa debe tener exactamente 1 neurona para problemas de regresi贸n.")

    # Validar que MaxPooling2D tenga un tama帽o de pool razonable
    for layer in layer_config:
        if layer['type'] == "MaxPooling2D" and layer.get('pool_size', 1) <= 0:
            warnings.append("La capa 'MaxPooling2D' debe tener un tama帽o de pool mayor a 0.")

    return warnings


# Configuraci贸n con pesta帽as
tabs = st.sidebar.radio("Configuraci贸n:", ["Dataset","Capas", "Hiperpar谩metros", "Early Stopping"],disabled=st.session_state['disable_other_tabs'] if 'disable_other_tabs' in st.session_state else False)

# Validar configuraci贸n de capas
if st.session_state['layer_config'] != []: 
    warnings = validate_layer_config(st.session_state['layer_config'], st.session_state['problem_type'])
    # Mostrar advertencias al usuario
    if warnings:
        st.sidebar.warning("锔 Se detectaron posibles problemas en la configuraci贸n de las capas:")
        for warning in warnings:
            st.sidebar.write(f"- {warning}")
    else:
        st.sidebar.success(" La configuraci贸n de capas parece adecuada para el problema seleccionado.")

# Configuraci贸n del Dataset
if tabs == "Dataset":
   # Configuraci贸n del Dataset
    configure_dataset()



# Visualizar ejemplos del dataset
elif tabs == "Capas":
    st.sidebar.subheader("Configuraci贸n de Capas",help="Definici贸n: La configuraci贸n de capas es un paso clave para dise帽ar una red neuronal, definiendo c贸mo procesar谩 los datos y aprender谩 durante el entrenamiento. Sugerencia: Si no est谩s seguro de c贸mo dise帽ar la arquitectura, selecciona una de las arquitecturas predefinidas para tu dataset y ajusta las capas seg煤n sea necesario.")
    st.sidebar.info("""
    **Definici贸n**: Cada capa en una red neuronal tiene una funci贸n espec铆fica para procesar los datos. Las configuraciones permiten personalizar c贸mo se comporta cada capa.

    **Sugerencia**:
    - Comienza con pocas capas si no est谩s seguro de c贸mo dise帽ar la arquitectura.
    - Usa **Dropout** si notas que el modelo se ajusta demasiado a los datos de entrenamiento.
    - Agrega **BatchNormalization** para estabilizar el entrenamiento si utilizas redes profundas.
    """)

    # Seleccionar arquitectura base
    st.sidebar.subheader("Arquitectura")
    selected_architecture = st.sidebar.selectbox(
        "Seleccione una Arquitectura",
        options=list(architectures[st.session_state['selected_dataset']].keys()),
        help="Elija la arquitectura del modelo basada en el dataset seleccionado."
    )

    # Aplicar la arquitectura seleccionada
    if st.sidebar.button("Aplicar Arquitectura"):
        # Obtener las capas de la arquitectura seleccionada
        selected_architecture_layers = architectures[st.session_state['selected_dataset']][selected_architecture]

        # Reiniciar las configuraciones de capas
        st.session_state['layer_config'] = selected_architecture_layers[:]
        st.session_state['num_layers_selected'] = len(selected_architecture_layers)

        # Validar tipos de capas seg煤n el dataset
        valid_types = ["Dense", "Dropout"] if st.session_state['selected_dataset'] in ['MNIST'] else [
            "Dense", "Dropout", "Conv2D", "MaxPooling2D", "Flatten", "BatchNormalization"
        ]

        for layer in st.session_state['layer_config']:
            if layer['type'] not in valid_types:
                st.warning(f"Tipo de capa no v谩lido detectado: {layer['type']}.")
                st.session_state['layer_config'] = []
                break

        st.sidebar.success(f"Arquitectura '{selected_architecture}' aplicada correctamente.")

    # Validar cambios externos en configuraciones
    if st.session_state['layer_config'] != st.session_state.get('previous_layer_config', []):
        st.session_state['previous_layer_config'] = st.session_state['layer_config'][:]

    # Mostrar configuraci贸n personalizada de capas
    st.sidebar.subheader("Capas Personalizadas")
    st.sidebar.number_input(
        "N煤mero de Capas",
        min_value=0,
        max_value=30,
        value=st.session_state['num_layers_selected'],
        step=1,
        key='num_layers_selected',
        help="N煤mero total de capas que desea incluir en su modelo.",
        on_change=update_layer_config
    )

    # Validar si la configuraci贸n actual de capas es v谩lida
    valid_types = ["Dense", "Dropout", "Conv2D", "MaxPooling2D", "Flatten", "BatchNormalization"]
    invalid_layers = any(layer.get("type") not in valid_types for layer in st.session_state['layer_config'])

    if invalid_layers:
        st.session_state['layer_config'] = []
        st.warning("La configuraci贸n previa de capas no es v谩lida para el dataset seleccionado. Las capas se han reiniciado.")

    # Configuraci贸n de cada capa
    for i, layer in enumerate(st.session_state['layer_config']):
        with st.sidebar.expander(f"Configuraci贸n de la Capa {i + 1}", expanded=True):
            layer['type'] = st.selectbox(
                "Tipo de Capa",
                valid_types,
                index=valid_types.index(layer.get("type", "Dense")),
                key=f"layer_type_{i}",
                help="""Seleccione el tipo de capa que desea a帽adir. Cada tipo tiene un prop贸sito espec铆fico en la red neuronal:
                - **Dense**: Totalmente conectada, 煤til para patrones complejos.
                - **Dropout**: Reduce el sobreajuste al desactivar neuronas aleatoriamente.
                - **Conv2D**: Extrae patrones espaciales en im谩genes.
                - **MaxPooling2D**: Reduce dimensiones seleccionando caracter铆sticas clave.
                - **Flatten**: Convierte datos multidimensionales a un solo vector.
                - **BatchNormalization**: Normaliza las salidas para estabilizar y acelerar el entrenamiento.""",
                on_change=update_layer_config
            )

            # Configuraci贸n espec铆fica para cada tipo de capa
            if layer['type'] == "Dense":
                layer['neurons'] = st.number_input(
                    "N煤mero de Neuronas",
                    min_value=1,
                    value=layer.get('neurons', 64),
                    key=f"neurons_{i}",
                    help="Define cu谩ntas unidades tendr谩 esta capa para aprender patrones complejos. Un mayor n煤mero de neuronas puede capturar m谩s informaci贸n, pero podr铆a incrementar el tiempo de entrenamiento.",
                    on_change=update_layer_config
                )
                layer['activation'] = st.selectbox(
                    "Activaci贸n",
                    ["relu", "sigmoid", "tanh", "softmax","linear"],
                    index=["relu", "sigmoid", "tanh", "softmax","linear"].index(layer.get('activation', "relu")),
                    key=f"activation_{i}",
                    help="""Funci贸n que determina c贸mo se transforman las salidas de esta capa.
                    - **ReLU**: Ideal para muchas capas, evita valores negativos.
                    - **Sigmoid**: Convierte salidas a un rango entre 0 y 1, 煤til para clasificaci贸n binaria.
                    - **Tanh**: Rango entre -1 y 1, 煤til para datos centrados.
                    - **Softmax**: Convierte salidas a probabilidades, ideal para clasificaci贸n multicategor铆a.
                    - **Linear**: Mantiene las salidas sin cambios, 煤til en regresi贸n.""",
                    on_change=update_layer_config
                )
                # Checkbox para habilitar regularizaci贸n L2
                layer['enable_l2'] = st.checkbox(
                    "Habilitar Regularizaci贸n L2",
                    value=layer.get('enable_l2', False),
                    key=f"enable_l2_{i}",
                    help="Penaliza grandes valores de los pesos para evitar el sobreajuste. Util铆zalo si notas que el modelo se ajusta demasiado a los datos de entrenamiento.",
                    on_change=update_layer_config
                )
                if layer['enable_l2']:
                    # Slider para definir el coeficiente L2 si est谩 habilitado
                    layer['l2'] = st.slider(
                        "Regularizaci贸n L2",
                        min_value=0.001,
                        max_value=0.1,
                        value=layer.get('l2', 0.01),
                        step=0.001,
                        key=f"l3_{i}",
                        help="Controla la intensidad de la penalizaci贸n en los pesos. Valores m谩s altos aplican m谩s restricci贸n.",
                        on_change=update_layer_config
                    )
            elif layer['type'] == "Dropout":
                layer['dropout_rate'] = st.slider(
                    "Tasa de Dropout",
                    0.0,
                    0.9,
                    value=layer.get('dropout_rate', 0.2),
                    step=0.1,
                    key=f"dropout_{i}",
                    help="Porcentaje de neuronas que se desactivar谩n aleatoriamente durante el entrenamiento. Ayuda a prevenir el sobreajuste y mejora la generalizaci贸n del modelo.",
                    on_change=update_layer_config
                )
            elif layer['type'] == "Conv2D":
                layer['filters'] = st.number_input(
                    "N煤mero de Filtros",
                    min_value=1,
                    value=layer.get('filters', 32),
                    key=f"filters_{i}",
                    help="N煤mero de detectores de caracter铆sticas que analizar谩n las entradas. M谩s filtros pueden detectar patrones m谩s variados, pero incrementan el tiempo de entrenamiento.",
                    on_change=update_layer_config
                )
                layer['kernel_size'] = st.slider(
                    "Tama帽o del Kernel",
                    min_value=1,
                    max_value=5,
                    on_change=update_layer_config,
                    value=layer.get('kernel_size', 3),
                    step=1,
                    key=f"kernel_{i}",
                    help="Tama帽o del filtro que se mueve sobre la entrada. Por ejemplo, un kernel de 3x3 analiza peque帽os fragmentos de la entrada a la vez."
                )
                layer['activation'] = st.selectbox(
                    "Activaci贸n",
                    ["relu", "sigmoid", "tanh", "softmax","linear"],
                    index=["relu", "sigmoid", "tanh", "softmax","linear"].index(layer.get('activation', "relu")),
                    key=f"activation_conv_{i}",
                    on_change=update_layer_config,
                    help="""Funci贸n que transforma las salidas de la convoluci贸n.
                        - **ReLU**: Com煤nmente usada en capas convolucionales.
                        - **Sigmoid** y **Tanh**: tiles en casos espec铆ficos como segmentaci贸n de im谩genes."""
                    )
            elif layer['type'] == "MaxPooling2D":
                layer['pool_size'] = st.slider(
                    "Tama帽o del Pool",
                    min_value=1,
                    max_value=5,
                    on_change=update_layer_config,
                    value=layer.get('pool_size', 2),
                    step=1,
                    key=f"pool_size_{i}",
                    help="Tama帽o de la ventana utilizada para reducir las dimensiones. Por ejemplo, un tama帽o de pool de 2x2 seleccionar谩 el valor m谩ximo en cada 谩rea de 2x2 p铆xeles."
                )
            elif layer['type'] == "Flatten":
                on_change=update_layer_config
                st.info("Capa que convierte entradas multidimensionales en un 煤nico vector. Esto es necesario para conectar capas convolucionales con capas densas.")
            elif layer['type'] == "BatchNormalization":
                on_change=update_layer_config
                st.info("Capa que normaliza las salidas de la capa anterior para estabilizar el entrenamiento. Es especialmente 煤til en redes profundas para acelerar la convergencia.")

            # Actualizar el nombre de la capa
            layer['name'] = f"{layer['type']}{i + 1}"



# Configurar hiperpar谩metros
elif tabs == "Hiperpar谩metros":
    st.sidebar.subheader("Hiperpar谩metros")
    st.session_state['hyperparams']['optimizer'] = st.sidebar.selectbox(
        "Optimizador",
        ["Adam", "SGD", "RMSprop"],
        help="Elija el optimizador que se utilizar谩 para ajustar los pesos del modelo."
    )
    st.session_state['hyperparams']['learning_rate'] = st.sidebar.slider(
        "Tasa de Aprendizaje",
        min_value=0.0001,
        max_value=0.01,
        value=0.001,
        step=0.0001,
        format="%.4g",  # Mostrar hasta 4 decimales
        help="Velocidad con la que el modelo ajusta sus pesos durante el entrenamiento."
    )
    st.session_state['hyperparams']['epochs'] = st.sidebar.number_input(
        "N煤mero de pocas",
        min_value=1,
        max_value=50,
        value=5,
        step=1,
        help="N煤mero de veces que el modelo ver谩 todo el conjunto de datos durante el entrenamiento."
    )
    st.session_state['hyperparams']['batch_size'] = st.sidebar.number_input(
        "Tama帽o de Batch",
        min_value=8,
        max_value=128,
        value=32,
        step=8,
        help="Cantidad de muestras procesadas antes de actualizar los pesos del modelo."
    )
    st.session_state['hyperparams']['loss_function'] = st.sidebar.selectbox(
        "Funci贸n de P茅rdida",
        ["categorical_crossentropy", "mean_squared_error"],
        help="M茅trica utilizada para evaluar qu茅 tan bien se est谩 entrenando el modelo."
    )


elif tabs == "Early Stopping":
    st.sidebar.subheader("Early Stopping")

    # Verificar si el conjunto de validaci贸n est谩 disponible
    has_validation_data = len(st.session_state.get('splits', [])) == 6

    enable_early_stopping = st.sidebar.checkbox(
        "Habilitar Early Stopping",
        value=False,
        help="Detiene el entrenamiento si no hay mejora en la m茅trica monitoreada despu茅s de un n煤mero determinado de 茅pocas.",
        disabled=not has_validation_data  # Deshabilitar si no hay validaci贸n
    )

    if not has_validation_data:
        st.sidebar.warning("El conjunto de validaci贸n no est谩 configurado. Early Stopping requiere un conjunto de validaci贸n.")

    if enable_early_stopping and has_validation_data:
        patience = st.sidebar.number_input(
            "Patience (N煤mero de 茅pocas sin mejora)",
            min_value=1,
            max_value=20,
            value=3,
            step=1,
            help="N煤mero de 茅pocas sin mejora antes de detener el entrenamiento."
        )
        monitor_metric = st.sidebar.selectbox(
            "M茅trica a Monitorear",
            ["val_loss", "val_accuracy"],
            index=0,
            help="M茅trica que se monitorear谩 para decidir si detener el entrenamiento."
        )
        st.session_state['early_stopping'] = {
            "enabled": enable_early_stopping,
            "patience": patience,
            "monitor": monitor_metric
        }
    else:
        st.session_state['early_stopping'] = {
            "enabled": False
        }



# Checkbox para mostrar/ocultar m茅tricas (siempre disponible)
st.session_state['show_metrics'] = st.checkbox(
    "Mostrar Gr谩ficos en tiempo de entrenamiento",
    value=st.session_state.get('show_metrics', False),
    disabled=st.session_state['training_in_progress']  # Deshabilitar si est谩 entrenando
)


# Funci贸n para resetear el estado de entrenamiento
def reset_training_state():
    st.session_state['training_in_progress'] = True
    st.session_state['training_finished'] = False
    st.session_state['logs'] = []  # Limpiar logs solo al comenzar nuevo entrenamiento
    st.session_state['modelDownload'] = None  # Limpiar modelo previo
    dynamic_placeholder.empty()  # Limpiar placeholders
    preview_placeholder.empty()


# Mostrar preview solo si no est谩 entrenando
if not st.session_state['training_in_progress']:
    preview_graph(st.session_state['layer_config'], preview_placeholder)

# Bot贸n para iniciar el entrenamiento
if not st.session_state['training_in_progress'] and not st.session_state['training_finished']:
    if st.button("Comenzar entrenamiento", disabled = st.session_state['disabled_button_train']):
        reset_training_state()  # Resetear estados antes de iniciar
        st.rerun()

# Bot贸n para cancelar el entrenamiento
if st.session_state['training_in_progress']:
    col_cancel, col_info = st.columns([1, 3])
    with col_cancel:
        if st.button("Cancelar Entrenamiento"):
            st.session_state['training_in_progress'] = False
            st.session_state['training_finished'] = False
            st.session_state['logs'].append("Entrenamiento cancelado por el usuario.")
            dynamic_placeholder.text("Entrenamiento cancelado.")
            st.rerun()

# Mostrar progreso del entrenamiento
if st.session_state['training_in_progress']:
    st.write("Tipo de problema seleccionado:", st.session_state.get('problem_type'))
    if st.session_state.get('problem_type') == "Clasificaci贸n":
        train_model_classification(
            st.session_state['layer_config'],
            st.session_state['hyperparams'],
            preview_placeholder,
            dynamic_placeholder
        )
    elif st.session_state.get('problem_type') == "Regresi贸n":
        train_model_regression(
            st.session_state['layer_config'],
            st.session_state['hyperparams'],
            preview_placeholder,
            dynamic_placeholder
        )
    else:
        st.error("El tipo de problema no est谩 configurado correctamente.")
        st.stop()

    st.session_state['training_in_progress'] = False
    st.session_state['training_finished'] = True
    st.rerun()


# Mostrar el bot贸n de guardar modelo una vez terminado el entrenamiento
if st.session_state['training_finished']:
    st.text_area(
        "Logs del Entrenamiento",
        "\n".join(st.session_state['logs']),
        height=300,
        key="final_logs"
    )

    # Bot贸n para guardar el modelo
    if st.button("Guardar Modelo"):
        st.session_state['modelDownload'].save("trained_model.h5")
        with open("trained_model.h5", "rb") as file:
            st.download_button("Descargar Modelo", file, file_name="trained_model.h5")
    # Bot贸n para reiniciar el entrenamiento
    if st.button("Comenzar Entrenamiento"):
        reset_training_state()
        st.rerun()
# Mostrar gr谩ficos detallados de m茅tricas bajo un checkbox
    if st.checkbox("Mostrar Gr谩ficos de M茅tricas finales", key="show_final_metrics"):
        st.header(" M茅tricas finales")

        col1, col2 = st.columns(2)  # Dividir gr谩ficos en columnas para mejor organizaci贸n

        if st.session_state['problem_type'] == "Clasificaci贸n":
            # Gr谩fico de p茅rdida
            with col1:
                st.write("#### P茅rdida por poca")
                with st.expander("驴Qu茅 significa este gr谩fico?"):
                    st.write("Este gr谩fico muestra c贸mo la p茅rdida (error) cambia en cada 茅poca. Una tendencia descendente indica que el modelo est谩 aprendiendo y ajust谩ndose mejor a los datos.")
                fig_loss = go.Figure()
                fig_loss.add_trace(go.Scatter(
                    x=list(range(1, len(st.session_state['loss_values']) + 1)),
                    y=st.session_state['loss_values'],
                    mode='lines+markers',
                    name="P茅rdida de Entrenamiento",
                    line=dict(color='blue')
                ))
                if st.session_state['val_loss_values']:
                    fig_loss.add_trace(go.Scatter(
                        x=list(range(1, len(st.session_state['val_loss_values']) + 1)),
                        y=st.session_state['val_loss_values'],
                        mode='lines+markers',
                        name="P茅rdida de Validaci贸n",
                        line=dict(color='red', dash='dash')
                    ))
                fig_loss.update_layout(
                    title="P茅rdida por poca",
                    xaxis_title="poca",
                    yaxis_title="P茅rdida",
                    legend_title="Tipo"
                )
                st.plotly_chart(fig_loss, use_container_width=True)

            # Gr谩fico de precisi贸n
            with col2:
                if st.session_state['accuracy_values']:
                    st.write(
                        "#### Precisi贸n por poca",
                    )
                    with st.expander("驴Qu茅 significa este gr谩fico?"):
                        st.write("Este gr谩fico muestra c贸mo la precisi贸n del modelo cambia en cada 茅poca. Una tendencia ascendente indica que el modelo est谩 mejorando en sus predicciones.")
                    fig_acc = go.Figure()
                    fig_acc.add_trace(go.Scatter(
                        x=list(range(1, len(st.session_state['accuracy_values']) + 1)),
                        y=st.session_state['accuracy_values'],
                        mode='lines+markers',
                        name="Precisi贸n de Entrenamiento",
                        line=dict(color='green')
                    ))
                    if st.session_state['val_accuracy_values']:
                        fig_acc.add_trace(go.Scatter(
                            x=list(range(1, len(st.session_state['val_accuracy_values']) + 1)),
                            y=st.session_state['val_accuracy_values'],
                            mode='lines+markers',
                            name="Precisi贸n de Validaci贸n",
                            line=dict(color='orange', dash='dash')
                        ))
                    fig_acc.update_layout(
                        title="Precisi贸n por poca",
                        xaxis_title="poca",
                        yaxis_title="Precisi贸n",
                        legend_title="Tipo"
                    )
                    st.plotly_chart(fig_acc, use_container_width=True)


        elif st.session_state['problem_type'] == "Regresi贸n":
            # Gr谩fico de P茅rdida (MSE)
            with col1:
                st.write("#### P茅rdida por poca (MSE)")
                with st.expander("驴Qu茅 significa este gr谩fico?"):
                    st.write("Este gr谩fico muestra c贸mo cambia el error cuadr谩tico medio (MSE) durante el entrenamiento. Una p茅rdida menor indica que el modelo se est谩 ajustando mejor a los datos.")
                fig_loss = go.Figure()
                fig_loss.add_trace(go.Scatter(
                    x=list(range(1, len(st.session_state['loss_values']) + 1)),
                    y=st.session_state['loss_values'],
                    mode='lines+markers',
                    name="P茅rdida de Entrenamiento (MSE)",
                    line=dict(color='blue')
                ))
                if st.session_state['val_loss_values']:
                    fig_loss.add_trace(go.Scatter(
                        x=list(range(1, len(st.session_state['val_loss_values']) + 1)),
                        y=st.session_state['val_loss_values'],
                        mode='lines+markers',
                        name="P茅rdida de Validaci贸n (MSE)",
                        line=dict(color='red', dash='dash')
                    ))
                fig_loss.update_layout(
                    title="P茅rdida por poca (MSE)",
                    xaxis_title="poca",
                    yaxis_title="P茅rdida (MSE)",
                    legend_title="Tipo"
                )
                st.plotly_chart(fig_loss, use_container_width=True)

            # Gr谩fico de Precisi贸n (MAE)
            with col2:
                st.write("#### Precisi贸n por poca (MAE)")
                with st.expander("驴Qu茅 significa este gr谩fico?"):
                    st.write("Este gr谩fico muestra c贸mo cambia el error absoluto medio (MAE) durante el entrenamiento. Un MAE menor indica que las predicciones del modelo est谩n m谩s cerca de los valores reales.")
        
                fig_mae = go.Figure()
                fig_mae.add_trace(go.Scatter(
                    x=list(range(1, len(st.session_state['mae_values']) + 1)),
                    y=st.session_state['mae_values'],
                    mode='lines+markers',
                    name="Precisi贸n de Entrenamiento (MAE)",
                    line=dict(color='green')
                ))
                if st.session_state['val_mae_values']:
                    fig_mae.add_trace(go.Scatter(
                        x=list(range(1, len(st.session_state['val_mae_values']) + 1)),
                        y=st.session_state['val_mae_values'],
                        mode='lines+markers',
                        name="Precisi贸n de Validaci贸n (MAE)",
                        line=dict(color='orange', dash='dash')
                    ))
                fig_mae.update_layout(
                    title="Precisi贸n por poca (MAE)",
                    xaxis_title="poca",
                    yaxis_title="Precisi贸n (MAE)",
                    legend_title="Tipo"
                )
                st.plotly_chart(fig_mae, use_container_width=True)

        # M茅tricas finales seg煤n el tipo de problema
        if st.session_state['problem_type'] == "Clasificaci贸n":
            # Verificar que el modelo exista
            if "modelDownload" not in st.session_state or st.session_state["modelDownload"] is None:
                st.error("El modelo no est谩 entrenado. Por favor, entrena el modelo antes de intentar predecir.")
                st.stop()

            # Cargar el modelo desde session_state
            model = st.session_state["modelDownload"]

            # Datos de prueba
            split_type = st.session_state["split_type"]

            if split_type == "Entrenamiento y Prueba":
                X_test = st.session_state["splits"][1]
                y_test_original = st.session_state["splits"][3]
            elif split_type == "Entrenamiento, Validaci贸n y Prueba":
                X_test = st.session_state["splits"][2]
                y_test_original = st.session_state["splits"][5]
            else:
                st.error("Tipo de divisi贸n desconocido. Revisa la configuraci贸n.")
                st.stop()





            # Predicci贸n
            predictions = model.predict(X_test)
            y_pred = np.argmax(predictions, axis=1)

            # Validar dimensiones despu茅s de predecir
            if len(y_pred) != len(y_test_original):
                st.error(f"Dimensiones inconsistentes: y_pred tiene {len(y_pred)} muestras, pero y_test_original tiene {len(y_test_original)}.")
                st.stop()

            # Calcular m茅tricas
            f1 = f1_score(y_test_original, y_pred, average='weighted')
            st.session_state['f1_score'] = f1
            precision = precision_score(y_test_original, y_pred, average='weighted')
            st.session_state['precision_score'] = precision
            recall = recall_score(y_test_original, y_pred, average='weighted')
            st.session_state['recall_score'] = recall

            # Visualizaci贸n de la matriz de confusi贸n
            cm = confusion_matrix(y_test_original, y_pred)
            st.session_state['confusion_matrix'] = cm
            st.write("#### Matriz de confusi贸n:")
            with st.expander("驴Qu茅 significa esta matriz?"):
                st.write("""
                Una matriz de confusi贸n compara las predicciones del modelo con los valores reales. Cada celda representa:
                
                - **Fila**: La clase real.
                - **Columna**: La clase predicha.
                
                Interpretaci贸n:
                - **Diagonal principal**: Cantidad de predicciones correctas.
                - **Fuera de la diagonal**: Errores de predicci贸n (predicciones incorrectas).
                
                Por ejemplo:
                - Un valor alto en la diagonal principal indica que el modelo predijo correctamente para esa clase.
                - Un valor alto fuera de la diagonal indica errores espec铆ficos entre dos clases.
                """)
            fig, ax = plt.subplots()
            disp = ConfusionMatrixDisplay(confusion_matrix=cm)
            disp.plot(ax=ax)
            st.pyplot(fig)

            # Comparar predicciones con etiquetas reales
            st.write("Comparaci贸n de predicciones individuales:")
            for i in range(min(5, len(y_test_original))):  # Mostrar m谩ximo 5 ejemplos
                if isinstance(y_test_original, np.ndarray):  # Si es un array de NumPy
                    real_value = y_test_original[i]
                else:  # Si es un DataFrame o Serie de Pandas
                    real_value = y_test_original.iloc[i]
                
                st.write(f"Ejemplo {i+1}: Real: {real_value}, Predicci贸n: {y_pred[i]}")

            # Mostrar m茅tricas
            # Mostrar m茅tricas finales de clasificaci贸n
            st.write("#### M茅tricas Finales")
            st.text_input(
                "**F1 Score:**",
                f"{f1:.4f}",
                help="El F1 Score es una medida de equilibrio entre precisi贸n y recall, especialmente 煤til en datasets desbalanceados. Valores cercanos a 1 indican un buen rendimiento."
            )
            st.text_input(
                "**Precisi贸n (Precision):**",
                f"{precision:.4f}",
                help="La precisi贸n indica qu茅 proporci贸n de las predicciones positivas es correcta. Es clave en problemas donde los falsos positivos son costosos."
            )
            st.text_input(
                "**Recall:**",
                f"{recall:.4f}",
                help="El recall indica qu茅 proporci贸n de los casos positivos reales fue correctamente identificada. Es clave cuando los falsos negativos son costosos."
            )

        else:  # Para regresi贸n
            final_loss = st.session_state['loss_values'][-1]
            final_mae = st.session_state['mae_values'][-1]  # Precisi贸n final (MAE)
            # M茅tricas finales de regresi贸n
            st.text_input(
                "**P茅rdida Final (MSE):**",
                f"{final_loss:.4f}",
                help="El Mean Squared Error (MSE) mide el error promedio cuadr谩tico entre los valores reales y las predicciones. Valores m谩s bajos indican mayor precisi贸n."
            )
            st.text_input(
                "**Precisi贸n Final (MAE):**",
                f"{final_mae:.4f}",
                help="El Mean Absolute Error (MAE) mide el error promedio absoluto entre los valores reales y las predicciones. Es m谩s robusto frente a valores at铆picos en comparaci贸n con el MSE."
            )
            if st.session_state['val_loss_values']:
                final_val_loss = st.session_state['val_loss_values'][-1]
                final_val_mae = st.session_state['val_mae_values'][-1]  # Precisi贸n validaci贸n (MAE)
                st.write(f"**P茅rdida Validaci贸n Final (MSE):** {final_val_loss:.4f}")
                st.write(f"**Precisi贸n Validaci贸n Final (MAE):** {final_val_mae:.4f}")

            # Gr谩fico de m茅tricas finales (regresi贸n)
            fig_metrics = go.Figure()
            fig_metrics.add_trace(go.Bar(
                name="P茅rdida Final (MSE)",
                x=["P茅rdida"],
                y=[final_loss],
                marker_color='blue'
            ))
            fig_metrics.add_trace(go.Bar(
                name="Precisi贸n Final (MAE)",
                x=["Precisi贸n"],
                y=[final_mae],
                marker_color='green'
            ))
            if 'val_loss_values' in st.session_state and st.session_state['val_loss_values']:
                fig_metrics.add_trace(go.Bar(
                    name="P茅rdida Validaci贸n (MSE)",
                    x=["P茅rdida Validaci贸n"],
                    y=[final_val_loss],
                    marker_color='red'
                ))
                fig_metrics.add_trace(go.Bar(
                    name="Precisi贸n Validaci贸n (MAE)",
                    x=["Precisi贸n Validaci贸n"],
                    y=[final_val_mae],
                    marker_color='orange'
                ))
            fig_metrics.update_layout(
                title="M茅tricas Finales (Regresi贸n)",
                barmode="group",
                xaxis_title="M茅trica",
                yaxis_title="Valor"
            )
            st.plotly_chart(fig_metrics, use_container_width=True)

            # Comparar predicciones con valores reales (prueba)
            model = st.session_state["modelDownload"]
            split_type = st.session_state["split_type"]

            if split_type == "Entrenamiento y Prueba":
                X_test = st.session_state["splits"][1]
                y_test_original = st.session_state["splits"][3]
            elif split_type == "Entrenamiento, Validaci贸n y Prueba":
                X_test = st.session_state["splits"][2]
                y_test_original = st.session_state["splits"][5]
            else:
                st.error("Tipo de divisi贸n desconocido. Revisa la configuraci贸n.")
                st.stop()

            # Revertir escalado si se aplic贸
            if "scaler_y" in st.session_state and st.session_state["scaler_y"] is not None:
                scaler = st.session_state["scaler_y"]
                y_test_original = scaler.inverse_transform(y_test_original.reshape(-1, 1)).flatten()
                y_pred = scaler.inverse_transform(model.predict(X_test).reshape(-1, 1)).flatten()
            else:
                y_pred = model.predict(X_test).flatten()
                y_test_original = y_test_original.flatten()

            # Comparaci贸n de predicciones con valores reales
            st.write(
                "#### Comparaci贸n de Predicciones con Valores Reales",
            )
            with st.expander("驴Qu茅 significa este gr谩fico?"):
                st.write("""
                Este gr谩fico compara los valores reales (lo que deber铆amos obtener) con las predicciones realizadas por el modelo.
                
                Interpretaci贸n:
                - **L铆nea Azul (Valores Reales)**: Representa los valores verdaderos del dataset de prueba.
                - **L铆nea Verde (Predicciones)**: Representa los valores que el modelo ha predicho.
                
                Un buen modelo deber铆a tener las l铆neas muy cercanas, lo que indica que las predicciones se aproximan bien a los valores reales.
                
                驴Qu茅 buscar?
                - Si las l铆neas est谩n muy separadas, podr铆a ser una se帽al de que el modelo no est谩 generalizando bien.
                - Si las l铆neas se cruzan constantemente, indica un modelo que sigue las tendencias correctamente.
                """)
            for i in range(min(5, len(y_test_original))):  # Mostrar m谩ximo 5 ejemplos
                st.write(f"Ejemplo {i+1}: Real: {float(y_test_original[i]):.4f}, Predicci贸n: {float(y_pred[i]):.4f}")

            # Gr谩fico de comparaci贸n
            fig_comparison = go.Figure()
            fig_comparison.add_trace(go.Scatter(
                x=list(range(len(y_test_original))),
                y=y_test_original,
                mode='lines+markers',
                name="Real",
                line=dict(color='blue')
            ))
            fig_comparison.add_trace(go.Scatter(
                x=list(range(len(y_pred))),
                y=y_pred,
                mode='lines+markers',
                name="Predicci贸n",
                line=dict(color='green', dash='dash')
            ))
            fig_comparison.update_layout(
                title="Comparaci贸n de Predicciones y Valores Reales",
                xaxis_title="ndice de Prueba",
                yaxis_title="Valor",
                legend_title="Tipo"
            )
            st.plotly_chart(fig_comparison, use_container_width=True)

# Probar el modelo guardado
if st.session_state["training_finished"] and st.session_state["modelDownload"]:
    
    if st.checkbox("Probar modelo", key="Test_model"):

        st.header(" Probar el Modelo")

        # Mostrar instrucciones para el usuario
        st.write("Modifique los valores de las caracter铆sticas para probar el modelo y observe c贸mo cambian las predicciones.")

        # Recuperar el modelo entrenado
        model = st.session_state["modelDownload"]

        # Determinar las caracter铆sticas del dataset
        if "splits" in st.session_state:
            if st.session_state["split_type"] == "Entrenamiento y Prueba":
                X_test = st.session_state["splits"][1]  # Tomar X_test para pruebas
                y_test = st.session_state["splits"][3]  # Tomar y_test para pruebas
            elif st.session_state["split_type"] == "Entrenamiento, Validaci贸n y Prueba":
                X_test = st.session_state["splits"][2]  # Tomar X_test para pruebas      
                y_test = st.session_state["splits"][5]  # Tomar y_test para pruebas      
        else:
            st.error("No se encontr贸 el conjunto de datos de prueba.")
            st.stop()

        # Crear sliders para cada caracter铆stica (en su forma original)
        input_data = {}
        original_X_test = X_test.copy()  # Copia del conjunto de prueba original

        # Revertir escalamiento de X_test si se aplic贸 escalamiento
        if "scaler_x" in st.session_state and st.session_state["scaler_x"] is not None:
            scaler = st.session_state["scaler_x"]
              # Validar si las dimensiones son compatibles
            if scaler.scale_.shape[0] == X_test.shape[1]:
                original_X_test[:] = scaler.inverse_transform(X_test)
            else:
                st.error("Dimensiones incompatibles entre el escalador y el conjunto de prueba.")
                st.stop()

        # Configurar sliders basados en los valores originales
        for col in X_test.columns:
            min_val = original_X_test[col].min()
            max_val = original_X_test[col].max()
            default_val = original_X_test[col].iloc[0]

            input_data[col] = st.slider(
                f"Modificar {col}",
                float(min_val),
                float(max_val),
                float(default_val),
                step=(max_val - min_val) / 100  # Paso del slider
            )

        # Convertir los datos de entrada a un DataFrame
        input_df = pd.DataFrame([input_data])

        # Escalar los datos de entrada nuevamente para la predicci贸n si se aplic贸 escalamiento
        if "scaler_x" in st.session_state and st.session_state["scaler_x"] is not None:
            input_df = st.session_state["scaler_x"].transform(input_df)

        # Generar predicci贸n
        prediction = model.predict(input_df)

        # Mostrar resultados seg煤n el tipo de problema
        if st.session_state["problem_type"] == "Clasificaci贸n":
            # Si es clasificaci贸n, revertir las etiquetas codificadas si aplica
            if "label_encoder" in st.session_state:
                predicted_class = st.session_state["label_encoder"].inverse_transform([np.argmax(prediction, axis=1)[0]])[0]
            else:
                predicted_class = np.argmax(prediction, axis=1)[0]

            # Obtener el nombre de la columna objetivo
            target_column_name = st.session_state.get("target_variable", "Variable Objetivo")

            # Mostrar la predicci贸n con el nombre de la columna objetivo
            st.subheader(f"Predicci贸n de la variable '{target_column_name}'")
            st.write(f"Clase predicha: {predicted_class}")

            # Si el modelo permite probabilidades, mostrarlas
            if hasattr(model, "predict_proba"):
                st.write("Probabilidades:")
                for idx, prob in enumerate(prediction[0]):
                    st.write(f"Clase {idx}: {prob:.4f}")

        elif st.session_state["problem_type"] == "Regresi贸n":
            # Revertir el escalamiento del valor predicho si aplica
            if "scaler_y" in st.session_state:
                prediction = st.session_state["scaler_y"].inverse_transform(prediction.reshape(-1, 1)).flatten()

            # Obtener el nombre de la columna objetivo
            target_column_name = st.session_state.get("target_variable", "Variable Objetivo")

            # Mostrar la predicci贸n con el nombre de la columna objetivo
            st.subheader(f"Predicci贸n de la variable '{target_column_name}'")
            st.write(f"Valor predicho: {prediction[0]:.4f}")

        if st.session_state["problem_type"] == "Clasificaci贸n":
            if "label_encoder" in st.session_state:
                y_test_original = st.session_state["label_encoder"].inverse_transform(y_test)
            else:
                y_test_original = y_test

            st.write("### Valores reales disponibles:")

            if isinstance(y_test_original, np.ndarray):  # Si es un array de NumPy
                unique_values = np.unique(y_test_original)  # Obtener valores 煤nicos
            else:  # Si es un DataFrame o Serie de Pandas
                unique_values = y_test_original.unique()  # Obtener valores 煤nicos

            # Mostrar valores 煤nicos en una lista ordenada
            st.write(f"#### Valores 煤nicos encontrados ({len(unique_values)}):")
            st.markdown("  \n".join([f"- **{val}**" for val in unique_values]))
        elif st.session_state["problem_type"] == "Regresi贸n":
            y_test_original = y_test

            # Revertir el escalamiento de los valores reales si aplica
            if "scaler_y" in st.session_state:
                y_test_original = st.session_state["scaler_y"].inverse_transform(y_test_original.reshape(-1, 1)).flatten()

            st.write("Primeros 5 valores reales disponibles:")
            st.write(y_test_original[:5])



if st.session_state["training_finished"] and st.session_state["modelDownload"]:
    import json
    # Funci贸n para convertir objetos no serializables a JSON
    def serialize_object(obj):
        if isinstance(obj, pd.DataFrame):
            return obj.to_dict(orient="list")  # Convertir DataFrame a diccionario
        elif isinstance(obj, np.ndarray):
            return obj.tolist()  # Convertir ndarray a lista
        elif isinstance(obj, list):
            return [serialize_object(item) for item in obj]  # Procesar listas recursivamente
        elif isinstance(obj, dict):
            return {key: serialize_object(value) for key, value in obj.items()}  # Procesar diccionarios recursivamente
        return obj  # Devolver el objeto original si es compatible

    # Bot贸n para exportar configuraci贸n
    if st.button("Exportar Configuraci贸n del Modelo"):
        # Determinar tipo de divisi贸n
        splits = st.session_state.get("splits", [])
        split_type = "Entrenamiento y Prueba" if len(splits) == 4 else "Entrenamiento, Validaci贸n y Prueba"

        # Recopilar m茅tricas seg煤n el tipo de problema
        problem_type = st.session_state.get("problem_type", "Clasificaci贸n")
        metrics = {
            "Clasificaci贸n": {
                "loss_values": st.session_state.get("loss_values"),
                "accuracy_values": st.session_state.get("accuracy_values"),
                "val_loss_values": st.session_state.get("val_loss_values"),
                "val_accuracy_values": st.session_state.get("val_accuracy_values"),
                "confusion_matrix": st.session_state.get("confusion_matrix"),
                "f1_score": st.session_state.get("f1_score"),
                "precision": st.session_state.get("precision_score"),
                "recall": st.session_state.get("recall_score"),
            },
            "Regresi贸n": {
                "loss_values": st.session_state.get("loss_values"),
                "mae_values": st.session_state.get("mae_values"),
                "val_loss_values": st.session_state.get("val_loss_values"),
                "val_mae_values": st.session_state.get("val_mae_values"),
                "mse": st.session_state.get("mse"),
                "rmse": st.session_state.get("rmse"),
            }
        }.get(problem_type, {})

        # Recopilar datos relevantes
        config_data = {
            "dataset": {
                "selected_dataset": st.session_state.get("selected_dataset"),
                "train_ratio": st.session_state.get("train_ratio"),
                "val_ratio": st.session_state.get("val_ratio"),
                "test_ratio": st.session_state.get("test_ratio"),
                "split_type": split_type,
                "target_variable": st.session_state.get("target_variable"),
                "columns_removed": st.session_state.get("columns_removed"),
                "missing_handled": st.session_state.get("missing_handled"),
                "categorical_encoded": st.session_state.get("categorical_encoded"),
                "scaling_done": st.session_state.get("scaling_done"),
            },
            "layer_config": st.session_state.get("layer_config"),
            "hyperparams": st.session_state.get("hyperparams"),
            "early_stopping": st.session_state.get("early_stopping"),
            "metrics": serialize_object(metrics),
        }

        # Serializar el objeto de configuraci贸n
        serialized_data = serialize_object(config_data)

        # Convertir a JSON y mostrar
        config_json = json.dumps(serialized_data, indent=4)
        st.json(json.loads(config_json))  # Mostrar configuraci贸n en la app

        serialized_data = serialize_object(config_data)
        st.session_state['serialized_results'] = serialized_data
        # Descargar archivo JSON
        st.download_button(
            label="Descargar Configuraci贸n como JSON",
            data=config_json,
            file_name="model_config.json",
            mime="application/json"
        )