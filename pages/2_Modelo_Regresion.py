import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import plotly.graph_objects as go
import math
import numpy as np
import time
import pandas as pd

from threading import Thread
from matplotlib.animation import FuncAnimation
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten
from keras.optimizers import Adam, SGD, RMSprop
from keras.datasets import mnist, fashion_mnist, cifar10
from keras.utils import to_categorical
from keras.regularizers import l2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.datasets import load_iris, load_breast_cancer, load_digits, load_wine, fetch_california_housing, load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer

# Config

def new_line():
    st.write("\n")
 
# with st.sidebar:
#    st.image("./assets/sb-quick.png",  use_container_width=True)


st.markdown("<h1 style='text-align: center; '>NeuraTrain</h1>", unsafe_allow_html=True)
st.markdown("КModelo de regresi贸n/clasificaci贸n para datos tabulares(csv)", unsafe_allow_html=True)
st.markdown("Utiliza el panel lateral para comenzar a crear tu red neuronal artifical!", unsafe_allow_html=True)

st.divider()


# Inicializar estado de la sesi贸n
if 'graph_positions' not in st.session_state:
    st.session_state['graph_positions'] = None
if 'graph' not in st.session_state:
    st.session_state['graph'] = None
if 'layer_config' not in st.session_state:
    st.session_state['layer_config'] = []
if 'hyperparams' not in st.session_state:
    st.session_state['hyperparams'] = {
        'optimizer': 'Adam',
        'learning_rate': 0.001,
        'epochs': 5,
        'batch_size': 32,
        'loss_function': 'categorical_crossentropy'
    }
if 'logs' not in st.session_state:
    st.session_state['logs'] = []
if 'training_in_progress' not in st.session_state:
    st.session_state['training_in_progress'] = False
if 'selected_dataset' not in st.session_state:
    st.session_state['selected_dataset'] = 'Iris'
if 'training_finished' not in st.session_state:
    st.session_state['training_finished'] = False
if 'modelDownload' not in st.session_state:
    st.session_state['modelDownload'] = None
# Inicializar estado de la sesi贸n para el bot贸n de m茅tricas
if 'show_metrics' not in st.session_state:
    st.session_state['show_metrics'] = False    
if 'train_ratio' not in st.session_state:
    st.session_state['train_ratio'] = 80  # Valor predeterminado para el entrenamiento
if 'val_ratio' not in st.session_state:
    st.session_state['val_ratio'] = 10  # Valor predeterminado para la validaci贸n
if 'test_ratio' not in st.session_state:
    st.session_state['test_ratio'] = 10  # Valor predeterminado para la prueba

if "dataset_loaded" not in st.session_state:
    st.session_state["dataset_loaded"] = False
if "full_dataset" not in st.session_state:
    st.session_state["full_dataset"] = False
if "X_original" not in st.session_state:
    st.session_state["X_original"] = None
if "y_original" not in st.session_state:
    st.session_state["y_original"] = None
if "columns_removed" not in st.session_state:
    st.session_state["columns_removed"] = False
if "missing_handled" not in st.session_state:
    st.session_state["missing_handled"] = False
if "categorical_encoded" not in st.session_state:
    st.session_state["categorical_encoded"] = False
if "scaling_done" not in st.session_state:
    st.session_state["scaling_done"] = False
if "dataset_split" not in st.session_state:
    st.session_state["dataset_split"] = False

if 'loss_values' not in st.session_state:
    st.session_state['loss_values'] = []
if 'accuracy_values' not in st.session_state:
    st.session_state['accuracy_values'] = []
if 'val_loss_values' not in st.session_state:
    st.session_state['val_loss_values'] = []
if 'val_accuracy_values' not in st.session_state:
    st.session_state['val_accuracy_values'] = []
if 'problem_type' not in st.session_state:
    st.session_state['problem_type'] = "Clasificaci贸n"  # Valor predeterminado
if 'y_test' not in st.session_state:
    st.session_state['y_test'] = None
if 'split_type' not in st.session_state:
    st.session_state['split_type'] = "Entrenamiento y Prueba"  # Valor predeterminado1

# Inyectar CSS para capturar el color de fondo
st.markdown(
    """
    <style>
    .stApp {
        background-color: var(--background-color);
    }
    </style>
    """,
    unsafe_allow_html=True
)


# Capturar el color de fondo actual
def get_background_color():
    # Verificar si es modo claro o oscuro
    return st.session_state.get("background_color", "#ffffff")




# Mostrar gr谩fico y bot贸n para entrenar
preview_placeholder = st.empty()
dynamic_placeholder = st.empty()

# Funci贸n para inicializar configuraci贸n de capas seg煤n el dataset
def initialize_layer_config(dataset):
    # Verificar si el dataset ya fue inicializado
    if "initialized_dataset" not in st.session_state or st.session_state["initialized_dataset"] != dataset:
        st.session_state["initialized_dataset"] = dataset  # Marcar el dataset como inicializado

        if dataset == 'CIFAR-10':
            # Capas base espec铆ficas de CIFAR-10
            st.session_state['layer_config'] = [
                {"name": "Conv2D1", "type": "Conv2D", "filters": 32, "kernel_size": 3, "activation": "relu", "base": True},
                {"name": "Conv2D2", "type": "Conv2D", "filters": 64, "kernel_size": 3, "activation": "relu", "base": True},
                {"name": "MaxPooling2D1", "type": "MaxPooling2D", "pool_size": 2, "base": True},
                {"name": "Flatten1", "type": "Flatten", "base": True},
                {"name": "Dense1", "type": "Dense", "neurons": 128, "activation": "relu", "base": True},
                {"name": "Dropout1", "type": "Dropout", "dropout_rate": 0.5, "base": True},
            ]
        # Reiniciar capas intermedias al cambiar dataset
        st.session_state['num_intermediate_layers'] = 0


# Funci贸n para manejar actualizaci贸n de capas intermedias
def update_intermediate_layers():
    num_layers = st.session_state['num_intermediate_layers']
    current_intermediate_layers = len([
        layer for layer in st.session_state['layer_config']
        if not layer.get("base", False)
    ])

    # Actualizar solo si hay un cambio
    if num_layers != current_intermediate_layers:
        # Eliminar capas intermedias actuales
        st.session_state['layer_config'] = [
            layer for layer in st.session_state['layer_config'] if layer.get("base", False)
        ]

        # Agregar nuevas capas intermedias
        for i in range(num_layers):
            st.session_state['layer_config'].insert(
                -2,  # Insertar antes de las capas finales (Flatten y Output)
                {
                    "name": f"IntermediateLayer{i + 1}",
                    "type": "Dense",
                    "neurons": 64,
                    "activation": "relu",
                    "base": False,
                }
            )


from sklearn.preprocessing import StandardScaler

# Cargar y dividir dataset
from sklearn.preprocessing import StandardScaler
def load_dataset(name, problem_type):
    # Cargar dataset completo con nombres de columnas correctos
    if name == "Iris":
        data = load_iris(as_frame=True)
        df = data.data
        df["species"] = pd.Series(data.target).map(dict(enumerate(data.target_names)))
    elif name == "Wine":
        data = load_wine(as_frame=True)
        df = data.data
        df["wine_class"] = pd.Series(data.target).map(dict(enumerate(data.target_names)))
    elif name == "Breast Cancer":
        data = load_breast_cancer(as_frame=True)
        df = data.data
        df["diagnosis"] = data.target
    elif name == "Digits":
        data = load_digits(as_frame=False)
        df = pd.DataFrame(data.data, columns=[f"pixel_{i}" for i in range(data.data.shape[1])])
        df["digit"] = data.target
    elif name == "Boston Housing":
        data = fetch_california_housing(as_frame=True)
        df = data.data
        df["median_value"] = data.target
    elif name == "Diabetes":
        data = load_diabetes(as_frame=True)
        df = data.data
        df["disease_progression"] = data.target
    else:
        raise ValueError("Dataset no soportado")

    return df



def initialize_graph(layers):
    """
    Inicializa una representaci贸n de la red neuronal con Input y Output 煤nicos.
    """
    if not layers:
        st.session_state['graph'] = []
        return

    # Redefinir gr谩fico con Input, capas intermedias y Output
    st.session_state['graph'] = [
        {'name': 'Input', 'num_neurons': 1, 'type': 'Input', 'layer_index': 0}
    ]

    for i, layer in enumerate(layers):
        st.session_state['graph'].append({
            'name': layer['name'],
            'num_neurons': layer.get('neurons', 4),  # Valor predeterminado
            'type': layer['type'],
            'layer_index': i + 1
        })

    st.session_state['graph'].append({
        'name': 'Output', 'num_neurons': 1, 'type': 'Output', 'layer_index': len(layers) + 1
    })



def generate_graph_fig(layers, active_layer=None, epoch=None, neurons_per_point=12, animated_layer=None, progress_step=0):
    """
    Genera el gr谩fico unificado para preview y din谩mico con animaci贸n.
    """
    fig = go.Figure()
    layer_positions = {}

    background_color = get_background_color()

    # Determinar tama帽o de texto basado en la cantidad de capas
    num_layers = len(layers)
    text_size = 12 if num_layers <= 10 else max(8, 12 - (num_layers - 10) // 2)

    # Calcular posiciones y conexiones primero
    for i, layer in enumerate(st.session_state['graph']):
        x_position = i * 300
        if layer['type'] in ["Dense", "Input", "Output"]:
            total_neurons = layer['num_neurons']
            num_points = math.ceil(total_neurons / neurons_per_point)
            y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]
        else:
            y_positions = [0]  # Una sola posici贸n para capas simb贸licas

        # Guardar posiciones
        for j, y in enumerate(y_positions):
            layer_positions[(i, j)] = (x_position, y)

    # Dibujar conexiones primero (en el fondo)
    for i in range(len(st.session_state['graph']) - 1):
        for j1 in layer_positions:
            if j1[0] == i:
                for j2 in layer_positions:
                    if j2[0] == i + 1:
                        x0, y0 = layer_positions[j1]
                        x1, y1 = layer_positions[j2]
                        is_current_connection = i == animated_layer
                        line_color = "lightblue" if not is_current_connection else f"rgba(0, 200, 200, {0.5 + progress_step / 2})"
                        line_width = 2 if not is_current_connection else 3
                        fig.add_trace(go.Scatter(
                            x=[x0, x1], y=[y0, y1],
                            mode="lines",
                            line=dict(width=line_width, color=line_color),
                            hoverinfo="none"
                        ))

    # Dibujar formas (capas) encima
    for i, layer in enumerate(st.session_state['graph']):
        x_position = i * 300
        if layer['type'] in ["Dense", "Input", "Output"]:
            total_neurons = layer['num_neurons']
            num_points = math.ceil(total_neurons / neurons_per_point)
            y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]
            if layer['type'] != "Dense":   
                text1=[f"{layer['name']}"]
                # Dibujar neuronas como puntos
                node_color = "gray" if i != animated_layer else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                for j, y in enumerate(y_positions):
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[y],
                        mode="markers",
                        marker=dict(size=20, color=node_color),
                        hoverinfo="none",
                        text=text1
                    ))
            else:
                text1=[f"{layer['type']}<br>{layer['num_neurons']} Neuronas"]
                # Dibujar neuronas como puntos
                node_color = "blue" if i != animated_layer else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                for j, y in enumerate(y_positions):
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[y],
                        mode="markers",
                        marker=dict(size=10, color=node_color),
                        hoverinfo="none",
                        text=text1
                    ))



        else:
            # Capas sin neuronas: formas simb贸licas
            if layer['type'] == "Conv2D":
                color = "green"
            elif layer['type'] == "MaxPooling2D":
                color = "purple"
            elif layer['type'] == "Flatten":
                color = "gray"
            elif layer['type'] == "Dropout":
                color = "orange"
            elif layer['type'] == "BatchNormalization":
                color = "pink"
            else:
                color = "black"

            fig.add_trace(go.Scatter(
                x=[x_position], y=[0],
                mode="markers",
                marker=dict(size=20, color=color),
                hoverinfo="none",
                text=[f"{layer['name']}<br>{layer['type']}"]
            ))

        # Etiquetas para Input/Output y capas
        label_text = (
            f"{layer['type']}"
            if layer['type'] in ["Dense", "Input", "Output"]
            else layer['type']
        )

        # Ajustar posici贸n de etiquetas alternadas
        if i % 2 == 0:
            label_y_offset = max(y_positions) + 1.5  # Posici贸n arriba
            label_position = "top center"
        else:
            label_y_offset = min(y_positions) - 2.5  # Posici贸n abajo (m谩s alejado)

        fig.add_trace(go.Scatter(
            x=[x_position], 
            y=[label_y_offset],
            mode="text",
            text=label_text,
            textposition=label_position,
            textfont=dict(size=text_size),
            hoverinfo="none"
        ))

    # Configuraci贸n del gr谩fico
    title = f"Training Progress - Epoch {epoch + 1}" if epoch is not None else "Network Architecture Preview"
    fig.update_layout(
        title=title,
        showlegend=False,
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        plot_bgcolor=background_color,  # Fondo del gr谩fico
        paper_bgcolor=background_color,  # Fondo del 谩rea alrededor del gr谩fico
        margin=dict(l=10, r=10, t=30, b=10)
    )

    return fig



def update_graph_with_smooth_color_transition(layers, epoch, placeholder, neurons_per_point=12, animation_steps=30):
    """
    Muestra una animaci贸n visual fluida de flujo entre capas mediante cambios progresivos de color.
    """
    total_layers = len(layers)
    background_color = get_background_color()

    for layer_index in range(total_layers - 1):  # No incluye la capa final
        for step in range(animation_steps):
            progress_step = step / animation_steps  # Progreso gradual del color
            
            fig = go.Figure()
            layer_positions = {}

            # Calcular posiciones de capas
            for i, layer in enumerate(layers):
                x_position = i * 300
                total_neurons = layer['num_neurons'] if layer['type'] in ["Dense", "Input", "Output"] else 1

                # Calcular cu谩ntos puntos representar
                num_points = math.ceil(total_neurons / neurons_per_point)
                y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]

                # Guardar posiciones de la capa
                for j, y in enumerate(y_positions):
                    layer_positions[(i, j)] = (x_position, y)

            # Dibujar conexiones primero (fondo)
            for i in range(total_layers - 1):  # No conecta despu茅s de Output
                for j1 in layer_positions:
                    if j1[0] == i:
                        for j2 in layer_positions:
                            if j2[0] == i + 1:
                                x0, y0 = layer_positions[j1]
                                x1, y1 = layer_positions[j2]

                                # Color de las conexiones
                                if i == layer_index:
                                    line_color = f"rgba(255, {int(progress_step * 255)}, 0, 1)"  # De naranja a azul
                                else:
                                    line_color = "lightblue"

                                fig.add_trace(go.Scatter(
                                    x=[x0, x1], y=[y0, y1],
                                    mode="lines",
                                    line=dict(width=2, color=line_color),
                                    hoverinfo="none"
                                ))

            # Dibujar puntos o formas de las capas (frente)
            for i, layer in enumerate(layers):
                x_position = i * 300
                total_neurons = layer['num_neurons'] if layer['type'] in ["Dense", "Input", "Output"] else 1
                num_points = math.ceil(total_neurons / neurons_per_point)
                y_positions = list(range(-num_points // 2, num_points // 2 + 1))[:num_points]

                # Determinar color y forma de la capa
                node_color = "blue" if i != layer_index and i != layer_index + 1 else f"rgba(255, 165, {255 - int(progress_step * 200)}, 1)"
                if layer['type'] in ["Dense"]:
                    # Dibujar nodos (puntos)
                    for j, y in enumerate(y_positions):
                        fig.add_trace(go.Scatter(
                            x=[x_position], y=[y],
                            mode="markers",
                            marker=dict(size=10, color=node_color),
                            hoverinfo="none"
                        ))
                else:
                    # Dibujar formas simb贸licas para otras capas
                    color = {
                        "Conv2D": ( "green"),
                        "MaxPooling2D": ( "purple"),
                        "Flatten": ("gray"),
                        "Dropout": ("orange"),
                        "BatchNormalization": ("pink")
                    }.get(layer['type'], ("black"))
                    
                    fig.add_trace(go.Scatter(
                        x=[x_position], y=[-1],
                        mode="markers",
                        marker=dict(size=20, color=color),
                        hoverinfo="none"
                    ))

                # A帽adir etiquetas
                label_text = (
                    f"{layer['type']}"
                    if layer['type'] in ["Dense", "Input", "Output"]
                    else layer['type']
                )

                # Ajustar posici贸n de etiquetas alternadas
                if i % 2 == 0:
                    label_y_offset = max(y_positions) + 1.5  # Posici贸n arriba
                    label_position = "top center"
                else:
                    label_y_offset = min(y_positions) - 2.5  # Posici贸n abajo (m谩s alejado)

                fig.add_trace(go.Scatter(
                    x=[x_position], 
                    y=[label_y_offset],
                    mode="text",
                    text=label_text,
                    textposition=label_position,
                    hoverinfo="none"
                ))

            # Configuraci贸n del gr谩fico
            fig.update_layout(
                title=f"Progreso del entrenamiento - poca {epoch + 1}" if epoch is not None else "Arquitectura del modelo",
                showlegend=False,
                xaxis=dict(visible=False),
                yaxis=dict(visible=False),
                plot_bgcolor=background_color,  # Fondo del gr谩fico
                paper_bgcolor=background_color,  # Fondo del 谩rea alrededor del gr谩fico
                margin=dict(l=10, r=10, t=30, b=10)
            )

            # Renderizar el gr谩fico
            placeholder.plotly_chart(
                fig,
                use_container_width=True,
                key=f"training_{epoch}_{layer_index}_{step}_{time.time()}"
            )
            time.sleep(0.03)  # Ajusta para controlar la fluidez




def preview_graph(layers, placeholder, neurons_per_point=12):
    """
    Genera un gr谩fico est谩tico con Input, capas y Output.
    """
    initialize_graph(layers)
    if not st.session_state['graph']:
        placeholder.empty()
        st.warning("No hay capas configuradas. Agregue una capa para visualizar el gr谩fico.")
        return

    fig = generate_graph_fig(layers, neurons_per_point=neurons_per_point)
    placeholder.plotly_chart(fig, use_container_width=True, key=f"preview_{time.time()}")





    
# Funci贸n para registrar logs
def log_event(log_placeholder, message):
    st.session_state['logs'].append(message)
    log_placeholder.text_area("Registro del entrenamiento", "\n".join(st.session_state['logs']), height=300)









# Funci贸n para entrenar el modelo con gr谩ficos din谩micos
from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix,ConfusionMatrixDisplay
import pandas as pd

def train_model(layers, hyperparams, preview_placeholder, dynamic_placeholder):
    # Limpiar logs y placeholders
    st.session_state['logs'] = []
    st.session_state['loss_values'] = []
    st.session_state['accuracy_values'] = []
    st.session_state['val_loss_values'] = []
    st.session_state['val_accuracy_values'] = []


    preview_placeholder.empty()
    

    # Configuraci贸n de contenedores para evitar desplazamiento
    col_dynamic, col_metrics = st.columns([6, 1])  # Ajusta proporciones

    # Placeholder para gr谩fico din谩mico (mantiene la posici贸n)
    dynamic_graph_placeholder = col_dynamic.empty()

    # Placeholder para m茅tricas y logs (separado del gr谩fico)
    metrics_placeholder = col_metrics.empty()
    log_placeholder = col_metrics.empty()
    
    loss_chart_placeholder = col_metrics.empty()
    accuracy_chart_placeholder = col_metrics.empty()

    # Recuperar datos configurados previamente
    if not st.session_state.get("dataset_split", False):
        st.error("El dataset no est谩 configurado correctamente. Config煤ralo antes de entrenar.")
        return

    splits = st.session_state['splits']
    if len(splits) == 4:  # Entrenamiento y prueba
        X_train, X_test, y_train, y_test = splits
        X_val, y_val = None, None
    elif len(splits) == 6:  # Entrenamiento, validaci贸n y prueba
        X_train, X_val, X_test, y_train, y_val, y_test = splits
    else:
        st.error("La divisi贸n del dataset no es v谩lida. Reconfigura el dataset.")
        return

    input_shape = (X_train.shape[1],)
    problem_type = st.session_state['problem_type']

    # Preparar etiquetas para clasificaci贸n
    if problem_type == "Clasificaci贸n":
        num_classes = len(np.unique(y_train))
        if np.any(y_train >= num_classes) or np.any(y_train < 0):
            st.error("Error: Hay etiquetas fuera del rango v谩lido para la clasificaci贸n.")
            return
        y_train = to_categorical(y_train, num_classes)
        if y_val is not None:
            y_val = to_categorical(y_val, num_classes)
        y_test_original = y_test  # Guardar etiquetas originales
        y_test = to_categorical(y_test, num_classes)

    # Crear el modelo
    model = Sequential()
    for i, layer in enumerate(layers):
        if layer['type'] == "Dense":
            # A帽adir regularizaci贸n L2
            regularizer = None
            if 'regularization' in layer and layer['regularization'] == 'l2':
                regularizer = tf.keras.regularizers.l2(layer['regularization_rate'])

            model.add(Dense(
                layer['neurons'],
                activation=layer['activation'],
                input_shape=input_shape if i == 0 else None
            ))
        elif layer['type'] == "Dropout":
            model.add(Dropout(layer['dropout_rate']))

    # Capa de salida
    #if problem_type == "Clasificaci贸n":
    #    model.add(Dense(num_classes, activation="softmax"))
    #    loss_function = "categorical_crossentropy"
    #else:  # Regresi贸n
    #    model.add(Dense(1, activation="linear"))
    #    loss_function = "mean_squared_error"

    # Configuraci贸n del optimizador
    optimizers = {
        "Adam": Adam(learning_rate=hyperparams['learning_rate']),
        "SGD": SGD(learning_rate=hyperparams['learning_rate']),
        "RMSprop": RMSprop(learning_rate=hyperparams['learning_rate'])
    }
    optimizer = optimizers[hyperparams['optimizer']]
    loss_function = hyperparams['loss_function']


    model.compile(
        optimizer=optimizer,
        loss=loss_function,
        metrics=["accuracy"] if problem_type == "Clasificaci贸n" else ["mae"]  # Precisi贸n para clasificaci贸n, MAE para regresi贸n
    )

    # Inicializaci贸n de m茅tricas
    loss_values = []
    accuracy_values = []
    val_loss_values = []
    val_accuracy_values = []

    # Entrenamiento por 茅pocas
    for epoch in range(hyperparams['epochs']):
        start_time = time.time()
        log_event(log_placeholder, f"poca {epoch + 1}/{hyperparams['epochs']} iniciada.")

        # Animaci贸n del gr谩fico din谩mico
        update_graph_with_smooth_color_transition(
            st.session_state['graph'],
            epoch,
            dynamic_graph_placeholder,
            neurons_per_point=12,
            animation_steps=15
        )

        # Entrenar el modelo
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val) if X_val is not None else None,
            batch_size=hyperparams['batch_size'],
            epochs=1,
            verbose=0
        )

        # Actualizar m茅tricas
        loss = history.history['loss'][0]
        accuracy = history.history.get('accuracy', [None])[0]
        val_loss = history.history.get('val_loss', [None])[0]
        val_accuracy = history.history.get('val_accuracy', [None])[0]

        st.session_state['loss_values'].append(loss)
        if accuracy is not None:
            st.session_state['accuracy_values'].append(accuracy)
        if val_loss is not None:
            st.session_state['val_loss_values'].append(val_loss)
        if val_accuracy is not None:
            st.session_state['val_accuracy_values'].append(val_accuracy)


        # Gr谩fico de p茅rdida (aplica a ambos tipos de problemas)
        with loss_chart_placeholder:
            fig_loss, ax_loss = plt.subplots(figsize=(4, 2))
            ax_loss.plot(range(1, len(st.session_state['loss_values']) + 1),
                        st.session_state['loss_values'], marker='o', color='blue', label="P茅rdida")
            if st.session_state['val_loss_values']:
                ax_loss.plot(range(1, len(st.session_state['val_loss_values']) + 1),
                            st.session_state['val_loss_values'], linestyle="--", color='red', label="Validaci贸n")
            ax_loss.set_title("P茅rdida")
            ax_loss.grid(True)
            ax_loss.legend()
            loss_chart_placeholder.pyplot(fig_loss, clear_figure=True)

        # Gr谩fico de precisi贸n (solo para clasificaci贸n)
        if problem_type == "Clasificaci贸n":
            with accuracy_chart_placeholder:
                if st.session_state['accuracy_values']:
                    fig_accuracy, ax_accuracy = plt.subplots(figsize=(4, 2))
                    ax_accuracy.plot(range(1, len(st.session_state['accuracy_values']) + 1),
                                    st.session_state['accuracy_values'], marker='o', color='green', label="Precisi贸n")
                    if st.session_state['val_accuracy_values']:
                        ax_accuracy.plot(range(1, len(st.session_state['val_accuracy_values']) + 1),
                                        st.session_state['val_accuracy_values'], linestyle="--", color='orange', label="Validaci贸n")
                    ax_accuracy.set_title("Precisi贸n")
                    ax_accuracy.grid(True)
                    ax_accuracy.legend()
                    accuracy_chart_placeholder.pyplot(fig_accuracy, clear_figure=True)
        else:
            with accuracy_chart_placeholder:
                if accuracy is not None:
                    fig_accuracy, ax_accuracy = plt.subplots(figsize=(4, 2))
                    ax_accuracy.plot(range(1, len(accuracy_values) + 1), accuracy_values, marker='o', color='green', label="Precisi贸n")
                    if val_accuracy is not None:
                        ax_accuracy.plot(range(1, len(val_accuracy_values) + 1), val_accuracy_values, linestyle="--", color='orange', label="Validaci贸n")
                    ax_accuracy.set_title("Precisi贸n")
                    ax_accuracy.grid(True)
                    ax_accuracy.legend()
                    accuracy_chart_placeholder.pyplot(fig_accuracy, clear_figure=True)

        # Verificar valores de las m茅tricas y asignar 'N/A' si son None
        loss_str = f"{loss:.4f}" if loss is not None else "N/A"
        accuracy_str = f"{accuracy:.4f}" if accuracy is not None else "N/A"
        val_loss_str = f"{val_loss:.4f}" if val_loss is not None else "N/A"
        val_accuracy_str = f"{val_accuracy:.4f}" if val_accuracy is not None else "N/A"

        # Log de fin de 茅poca
        elapsed_time = time.time() - start_time
        log_event(
            log_placeholder,
            f"poca {epoch + 1} completada en {elapsed_time:.2f} segundos. "
            f"P茅rdida: {loss_str}, Precisi贸n: {accuracy_str}, "
            f"P茅rdida Validaci贸n: {val_loss_str}, Precisi贸n Validaci贸n: {val_accuracy_str}"
        )

    # Calcular m茅tricas finales
    # Mostrar m茅tricas finales seg煤n el tipo de problema
    if problem_type == "Clasificaci贸n":
        y_pred = np.argmax(model.predict(X_test), axis=1)
        f1 = f1_score(y_test_original, y_pred, average='weighted')
        precision = precision_score(y_test_original, y_pred, average='weighted')
        recall = recall_score(y_test_original, y_pred, average='weighted')

        st.write("### M茅tricas Finales - Clasificaci贸n")
        st.write(f"**F1 Score:** {f1:.4f}")
        st.write(f"**Precisi贸n (Precision):** {precision:.4f}")
        st.write(f"**Recall:** {recall:.4f}")
        st.text(classification_report(y_test_original, y_pred))
    else:
        st.write("### M茅tricas Finales - Regresi贸n")
        final_loss = st.session_state['loss_values'][-1]
        st.write(f"**P茅rdida Final (MAE):** {final_loss:.4f}")
        if st.session_state['val_loss_values']:
            final_val_loss = st.session_state['val_loss_values'][-1]
            st.write(f"**P茅rdida Validaci贸n Final (MAE):** {final_val_loss:.4f}")



    # Guardar modelo entrenado
    st.session_state["modelDownload"] = model
    st.success("Entrenamiento finalizado con 茅xito.")





def train_model_classification(layers, hyperparams, preview_placeholder, dynamic_placeholder):
    # Limpiar logs y placeholders
    st.session_state['logs'] = []
    st.session_state['loss_values'] = []
    st.session_state['accuracy_values'] = []
    st.session_state['val_loss_values'] = []
    st.session_state['val_accuracy_values'] = []

    preview_placeholder.empty()

    # Configuraci贸n de contenedores
    col_dynamic, col_metrics = st.columns([6, 1])
    dynamic_graph_placeholder = col_dynamic.empty()
    log_placeholder = st.empty()

    # Validar splits (ya existente)
    if not st.session_state.get("dataset_split", False):
        st.error("El dataset no est谩 configurado correctamente.")
        return

    splits = st.session_state['splits']
    if len(splits) == 4:
        X_train, X_test, y_train, y_test = splits
        X_val, y_val = None, None
    elif len(splits) == 6:
        X_train, X_val, X_test, y_train, y_val, y_test = splits
    else:
        st.error("La divisi贸n del dataset no es v谩lida. Reconfigura el dataset.")
        return



    st.write("Validaci贸n inmediata de dimensiones despu茅s de la divisi贸n:")
    st.write(f"Dimensiones de X_train: {X_train.shape}")
    st.write(f"Dimensiones de X_val: {X_val.shape}")
    st.write(f"Dimensiones de X_test: {X_test.shape}")
    st.write(f"Dimensiones de y_train: {y_train.shape}")
    st.write(f"Dimensiones de y_val: {y_val.shape}")
    st.write(f"Dimensiones de y_test: {y_test.shape}")
    st.write(f"Dimensiones de y_test_original: {st.session_state["y_test_original"].shape}")

    # Validar dimensiones antes del entrenamiento
    if len(st.session_state["splits"][3]) != len(st.session_state["splits"][3]):
        st.error("Dimensiones inconsistentes entre X_test y y_test_original antes del entrenamiento.")
        st.stop()

    num_classes = len(np.unique(y_train))
    y_train = to_categorical(y_train, num_classes)
    if y_val is not None:
        y_val = to_categorical(y_val, num_classes)
    y_test_original = y_test
    y_test = to_categorical(y_test, num_classes)

    if X_test.isnull().values.any():
        st.error("X_test contiene valores NaN. Revisa el preprocesamiento.")
        st.stop()
    if X_test.shape[1] != 4:
        st.error(f"X_test tiene {X_test.shape[1]} columnas, pero el modelo espera 4 caracter铆sticas.")
        st.stop()



    # Crear modelo
    input_shape = (X_train.shape[1],)
    model = Sequential()
    for i, layer in enumerate(layers):
        if layer['type'] == "Dense":
            model.add(Dense(layer['neurons'], activation=layer['activation'], input_shape=input_shape if i == 0 else None,kernel_regularizer=l2(layer['l2']) if layer.get('enable_l2', False) else None))
        elif layer['type'] == "Dropout":
            model.add(Dropout(layer['dropout_rate']))

    st.write(f"Configuraci贸n de capas utilizada: {st.session_state['layer_config']}")
    # Configurar optimizador y compilar modelo
    optimizer = Adam(learning_rate=hyperparams['learning_rate'])
    model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

    # Entrenar modelo
    for epoch in range(hyperparams['epochs']):
        log_event(log_placeholder, f"poca {epoch + 1}/{hyperparams['epochs']} iniciada.")

        if X_val is not None and y_val is not None:
            # Con conjunto de validaci贸n
            history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                                batch_size=hyperparams['batch_size'], epochs=1, verbose=0)
        else:
            # Sin conjunto de validaci贸n
            history = model.fit(X_train, y_train, batch_size=hyperparams['batch_size'], 
                                epochs=1, verbose=0)

        # Registrar m茅tricas
        train_loss = history.history['loss'][0]
        train_accuracy = history.history['accuracy'][0]
        log_event(log_placeholder, f"poca {epoch + 1}: P茅rdida en entrenamiento: {train_loss:.4f}")
        log_event(log_placeholder, f"poca {epoch + 1}: Precisi贸n en entrenamiento: {train_accuracy:.4f}")

        if X_val is not None and y_val is not None:
            val_loss = history.history['val_loss'][0]
            val_accuracy = history.history['val_accuracy'][0]
            log_event(log_placeholder, f"poca {epoch + 1}: P茅rdida en validaci贸n: {val_loss:.4f}")
            log_event(log_placeholder, f"poca {epoch + 1}: Precisi贸n en validaci贸n: {val_accuracy:.4f}")

        st.session_state['loss_values'].append(train_loss)
        st.session_state['accuracy_values'].append(train_accuracy)
        if X_val is not None and y_val is not None:
            st.session_state['val_loss_values'].append(val_loss)
            st.session_state['val_accuracy_values'].append(val_accuracy)

        # Actualizar visualizaci贸n din谩mica
        update_graph_with_smooth_color_transition(
            st.session_state['graph'], epoch, dynamic_graph_placeholder, neurons_per_point=10, animation_steps=30
        )

    # M茅tricas finales
    st.write(f"Dimensiones de X_test: {X_test.shape}")
    st.write(f"Dimensiones de y_test_original: {y_test_original.shape}")


    if np.isnan(X_test.values).any():
        st.error("X_test contiene valores NaN. Revisa el preprocesamiento.")
        st.stop()

    st.session_state["modelDownload"] = model
    st.success("Entrenamiento finalizado con 茅xito.")


def train_model_regression(layers, hyperparams, preview_placeholder, dynamic_placeholder):
    # Limpiar logs y placeholders
    st.session_state['logs'] = []
    st.session_state['loss_values'] = []
    st.session_state['val_loss_values'] = []

    preview_placeholder.empty()

    # Configuraci贸n de contenedores
    col_dynamic, col_metrics = st.columns([6, 1])
    dynamic_graph_placeholder = col_dynamic.empty()
    log_placeholder = st.empty()

    # Validar configuraci贸n del dataset
    if not st.session_state.get("dataset_split", False):
        st.error("El dataset no est谩 configurado correctamente. Config煤ralo antes de entrenar.")
        return

    # Recuperar splits
    splits = st.session_state['splits']
    if len(splits) == 4:
        X_train, X_test, y_train, y_test = splits
        X_val, y_val = None, None
    elif len(splits) == 6:
        X_train, X_val, X_test, y_train, y_val, y_test = splits
    else:
        st.error("La divisi贸n del dataset no es v谩lida. Reconfigura el dataset.")
        return

    # Crear modelo
    input_shape = (X_train.shape[1],)
    model = Sequential()
    for i, layer in enumerate(layers):
        if layer['type'] == "Dense":
            model.add(Dense(layer['neurons'], activation=layer['activation'], input_shape=input_shape if i == 0 else None,kernel_regularizer=l2(layer['l2']) if layer.get('enable_l2', False) else None))
        elif layer['type'] == "Dropout":
            model.add(Dropout(layer['dropout_rate']))
    model.add(Dense(1, activation="linear"))

    # Configurar optimizador y compilar modelo
    optimizer = Adam(learning_rate=hyperparams['learning_rate'])
    model.compile(optimizer=optimizer, loss="mean_squared_error", metrics=["mae"])

    # Entrenar modelo
    for epoch in range(hyperparams['epochs']):
        log_event(log_placeholder, f"poca {epoch + 1}/{hyperparams['epochs']} iniciada.")

        if X_val is not None and y_val is not None:
            history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                                batch_size=hyperparams['batch_size'], epochs=1, verbose=0)
        else:
            history = model.fit(X_train, y_train, batch_size=hyperparams['batch_size'], 
                                epochs=1, verbose=0)

        # Registrar m茅tricas
        train_loss = history.history['loss'][0]
        train_mae = history.history['mae'][0]
        log_event(log_placeholder, f"poca {epoch + 1}: P茅rdida (MSE) en entrenamiento: {train_loss:.4f}")
        log_event(log_placeholder, f"poca {epoch + 1}: Error absoluto medio (MAE) en entrenamiento: {train_mae:.4f}")

        if X_val is not None:
            val_loss = history.history['val_loss'][0]
            val_mae = history.history['val_mae'][0]
            log_event(log_placeholder, f"poca {epoch + 1}: P茅rdida (MSE) en validaci贸n: {val_loss:.4f}")
            log_event(log_placeholder, f"poca {epoch + 1}: Error absoluto medio (MAE) en validaci贸n: {val_mae:.4f}")

        # Actualizar m茅tricas para gr谩ficos
        st.session_state['loss_values'].append(train_loss)
        if X_val is not None:
            st.session_state['val_loss_values'].append(val_loss)

        # Actualizar visualizaci贸n din谩mica
        update_graph_with_smooth_color_transition(
            st.session_state['graph'], epoch, dynamic_graph_placeholder, neurons_per_point=10, animation_steps=30
        )

    # M茅tricas finales
    final_loss = st.session_state['loss_values'][-1]
    st.write(f"P茅rdida Final (MSE): {final_loss:.4f}")
    st.session_state["modelDownload"] = model
    st.success("Entrenamiento finalizado con 茅xito.")








current_intermediate_layers = max(0, len(st.session_state['layer_config']) - 6)

# Inicializar estado para el n煤mero de capas
if 'num_layers_selected' not in st.session_state:
    st.session_state['num_layers_selected'] = len(st.session_state['layer_config'])
if 'previous_layer_config' not in st.session_state:
    st.session_state['previous_layer_config'] = []
# Inicializar el estado para capas intermedias si no existe
if 'num_intermediate_layers' not in st.session_state:
    st.session_state['num_intermediate_layers'] = current_intermediate_layers

# Funci贸n para manejar la actualizaci贸n de capas
def update_layer_config():
    num_layers = st.session_state['num_layers_selected']
    current_num_layers = len(st.session_state['layer_config'])

    if num_layers > current_num_layers:
        # A帽adir capas
        for i in range(current_num_layers, num_layers):
            st.session_state['layer_config'].append({
                "name": f"Layer{i + 1}",
                "type": "Dense",
                "neurons": 64,
                "dropout_rate": 0.2
            })
    elif num_layers < current_num_layers:
        # Reducir capas
        st.session_state['layer_config'] = st.session_state['layer_config'][:num_layers]

    # Guardar una copia del estado actual para evitar rebotes
    st.session_state['previous_layer_config'] = st.session_state['layer_config'][:]



# Diccionario de arquitecturas predefinidas
architectures = {
    "Iris": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 16, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 3, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ],
        "Deep MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 16, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense4", "type": "Dense", "neurons": 3, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Wine": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 3, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Breast Cancer": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 16, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 2, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Digits": {
        "Simple MLP": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense3", "type": "Dense", "neurons": 10, "activation": "softmax", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Boston Housing": {
        "Simple Regression": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Output", "type": "Dense", "neurons": 1, "activation": "linear", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    },
    "Diabetes": {
        "Simple Regression": [
            {"name": "Dense1", "type": "Dense", "neurons": 64, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Dense2", "type": "Dense", "neurons": 32, "activation": "relu", "base": True, "l2": 0.01, "enable_l2": True},
            {"name": "Output", "type": "Dense", "neurons": 1, "activation": "linear", "base": True, "l2": 0.0, "enable_l2": False}
        ]
    }
}


# Funci贸n para manejar el cambio de dataset
if "selected_dataset_previous" not in st.session_state or st.session_state["selected_dataset_previous"] != st.session_state["selected_dataset"]:
    st.session_state["selected_dataset_previous"] = st.session_state["selected_dataset"]
    
    # Reinicia configuraci贸n de capas y arquitectura
    initialize_layer_config(st.session_state["selected_dataset"])



from sklearn.metrics import mean_squared_error




# Configuraci贸n con pesta帽as
tabs = st.sidebar.radio("Configuraci贸n:", ["Dataset","Capas", "Hiperpar谩metros", "Early Stopping"])


# Configuraci贸n del Dataset
from sklearn.impute import SimpleImputer
import numpy as np



# Aplicar ajustes avanzados
def apply_advanced_settings(X_train, X_test, numeric_null_option, categorical_null_option, normalize_data, scaling_option, encoding_option):
    # Manejo de valores nulos - Num茅ricos
    numeric_cols = X_train.select_dtypes(include=np.number).columns
    if numeric_null_option == "Eliminar filas":
        X_train.dropna(subset=numeric_cols, inplace=True)
        X_test.dropna(subset=numeric_cols, inplace=True)
    elif numeric_null_option == "Reemplazar con 0":
        X_train[numeric_cols] = X_train[numeric_cols].fillna(0)
        X_test[numeric_cols] = X_test[numeric_cols].fillna(0)
    elif numeric_null_option == "Reemplazar con la media":
        imputer = SimpleImputer(strategy='mean')
        X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])
        X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])
    elif numeric_null_option == "Reemplazar con la mediana":
        imputer = SimpleImputer(strategy='median')
        X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])
        X_test[numeric_cols] = imputer.transform(X_test[numeric_cols])
    elif numeric_null_option == "Propagar adelante/atr谩s":
        X_train[numeric_cols] = X_train[numeric_cols].fillna(method='ffill').fillna(method='bfill')
        X_test[numeric_cols] = X_test[numeric_cols].fillna(method='ffill').fillna(method='bfill')

    # Manejo de valores nulos - Categ贸ricos
    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns
    if categorical_null_option == "Eliminar filas":
        X_train.dropna(subset=categorical_cols, inplace=True)
        X_test.dropna(subset=categorical_cols, inplace=True)
    elif categorical_null_option == "Reemplazar con 'Ninguno'":
        X_train[categorical_cols] = X_train[categorical_cols].fillna('Ninguno')
        X_test[categorical_cols] = X_test[categorical_cols].fillna('Ninguno')
    elif categorical_null_option == "Reemplazar con la moda":
        imputer = SimpleImputer(strategy='most_frequent')
        X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])
        X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])

    # Normalizaci贸n y escalamiento
    if normalize_data:
        scaler = StandardScaler() if scaling_option == "StandardScaler" else MinMaxScaler()
        X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
        X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])

    # Codificaci贸n de variables categ贸ricas
    if encoding_option == "One-Hot Encoding":
        X_train = pd.get_dummies(X_train, columns=categorical_cols)
        X_test = pd.get_dummies(X_test, columns=categorical_cols)
        X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)
    elif encoding_option == "Label Encoding":
        label_encoder = LabelEncoder()
        for col in categorical_cols:
            X_train[col] = label_encoder.fit_transform(X_train[col].astype(str))
            X_test[col] = label_encoder.transform(X_test[col].astype(str))

    return X_train, X_test





# Funci贸n principal para configurar el dataset
def configure_dataset():
    st.sidebar.header("Configuraci贸n del Dataset")

    # Paso 1: Selecci贸n del dataset
    st.sidebar.subheader("Paso 1: Selecci贸n del Dataset")
    problem_type = st.sidebar.selectbox(
        "Seleccione el tipo de problema",
        ["Clasificaci贸n", "Regresi贸n"],
        key="problem_type_selectbox"
    )

    st.session_state['problem_type'] = problem_type

    dataset_name = st.sidebar.selectbox(
        "Seleccione el dataset",
        ["Iris", "Wine", "Breast Cancer", "Digits"] if problem_type == "Clasificaci贸n" else ["Boston Housing", "Diabetes"],
        key="dataset_name_selectbox"
    )

    st.session_state['selected_dataset'] = dataset_name

    if st.sidebar.button("Cargar Dataset", key="load_dataset_button"):
    # Reiniciar el estado relacionado con el dataset al cambiarlo
        if st.session_state.get("dataset_loaded", False):
            st.session_state["dataset_loaded"] = False
            st.session_state.pop("full_dataset", None)
            st.session_state.pop("target_variable", None)
            st.session_state.pop("X_original", None)
            st.session_state.pop("y_original", None)
            st.session_state.pop("splits", None)
            st.session_state["dataset_split"] = False

    # Cargar el nuevo dataset
    df = load_dataset(dataset_name, problem_type)
    if df.empty:
        st.sidebar.error("El dataset cargado est谩 vac铆o. Seleccione un dataset v谩lido.")
        st.stop()

    # Guardar el dataset en el estado de la sesi贸n
    st.session_state["dataset_loaded"] = True
    st.session_state["full_dataset"] = df
    st.success("Dataset cargado con 茅xito.")
    st.sidebar.write("Vista previa del dataset:", df.head())

    
    # Paso 2: Selecci贸n de la variable objetivo
    if st.session_state.get("dataset_loaded", False):
        st.sidebar.subheader("Paso 2: Selecci贸n de la Variable Objetivo")
        target_variable = st.sidebar.selectbox(
            "Seleccione la variable objetivo:",
            st.session_state["full_dataset"].columns,
            key="target_variable_selectbox"
        )
        if target_variable != st.session_state.get("target_variable", None):
            st.session_state["target_variable"] = target_variable
            st.session_state["X_original"] = st.session_state["full_dataset"].drop(columns=[target_variable]).copy()
            st.session_state["y_original"] = st.session_state["full_dataset"][target_variable].copy()

        st.sidebar.success(f"Variable objetivo seleccionada: {st.session_state['target_variable']}")
        st.sidebar.write("Caracter铆sticas (X):", st.session_state["X_original"].head())
        # Verificar si y_original es un numpy.ndarray y convertirlo en un DataFrame/Series antes de usar .head()
        if isinstance(st.session_state["y_original"], np.ndarray):
            y_display = pd.Series(st.session_state["y_original"])
        else:
            y_display = st.session_state["y_original"]

        # Mostrar la vista previa de la variable objetivo
        st.sidebar.write("Variable objetivo (y):", y_display.head())
                # Paso 3: Eliminaci贸n de columnas

    if st.session_state.get("target_variable"):
        st.sidebar.subheader("Paso 3: Eliminaci贸n de Columnas")
        selected_columns = st.sidebar.multiselect(
            "Seleccione las columnas a eliminar",
            st.session_state["X_original"].columns,
            key="columns_to_drop_multiselect"
        )
        if st.sidebar.button("Aplicar Eliminaci贸n", key="apply_column_removal_button"):
            if target_variable in selected_columns:
                st.sidebar.error("La variable objetivo no puede ser eliminada.")
                st.stop()
            st.session_state["X_original"] = st.session_state["X_original"].drop(columns=selected_columns).copy()
            if st.session_state["X_original"].shape[1] == 0:
                st.sidebar.error("No quedan columnas en el dataset despu茅s de la eliminaci贸n.")
                st.stop()
            st.session_state["columns_removed"] = True
            st.success("Columnas eliminadas con 茅xito.")
            st.sidebar.write("Vista previa del dataset actualizado:", st.session_state["X_original"].head())

        # Paso 4: Manejo de valores nulos
        st.sidebar.subheader("Paso 4: Manejo de Valores Nulos")
        X = st.session_state["X_original"]
        nulos_totales = X.isnull().sum().sum()

        if nulos_totales == 0:
            st.sidebar.success("No existen valores nulos en el dataset.")
            st.session_state["missing_handled"] = True
        else:
            numeric_null_option = st.sidebar.selectbox(
                "Valores Nulos - Num茅ricos",
                ["Eliminar filas", "Reemplazar con 0", "Reemplazar con la media", "Reemplazar con la mediana"],
                key="numeric_null_option_selectbox"
            )
            categorical_null_option = st.sidebar.selectbox(
                "Valores Nulos - Categ贸ricos",
                ["Eliminar filas", "Reemplazar con 'Ninguno'", "Reemplazar con la moda"],
                key="categorical_null_option_selectbox"
            )
            if st.sidebar.button("Aplicar Manejo de Nulos", key="apply_null_handling_button"):
                X_cleaned = handle_nulls(st.session_state["X_original"], numeric_null_option, categorical_null_option)
                st.session_state["X_original"] = X_cleaned
                st.session_state["missing_handled"] = True
                st.sidebar.success("Manejo de valores nulos aplicado con 茅xito.")
                st.sidebar.write("Vista previa del dataset actualizado:", X_cleaned.head())

        # Paso 5: Codificaci贸n de variables categ贸ricas
        if st.session_state.get("missing_handled", False):
            st.sidebar.subheader("Paso 5: Codificaci贸n de Variables Categ贸ricas")
            categorical_cols = st.session_state["X_original"].select_dtypes(include=["object", "category"]).columns

            # Codificar columnas categ贸ricas en X
            if len(categorical_cols) == 0:
                st.sidebar.success("No existen columnas categ贸ricas en las caracter铆sticas (X).")
                st.session_state["categorical_encoded"] = True
            else:
                encoding_option = st.sidebar.selectbox(
                    "M茅todo de Codificaci贸n para caracter铆sticas (X):",
                    ["One-Hot Encoding", "Label Encoding"],
                    key="encoding_option_selectbox"
                )
                if st.sidebar.button("Aplicar Codificaci贸n en X", key="apply_encoding_button"):
                    X_encoded = encode_categorical(st.session_state["X_original"], encoding_option)
                    st.session_state["X_original"] = X_encoded
                    st.session_state["categorical_encoded"] = True
                    st.sidebar.success("Codificaci贸n aplicada con 茅xito a las caracter铆sticas (X).")
                    st.sidebar.write("Vista previa del dataset actualizado (X):", X_encoded.head())

            # Codificar la variable objetivo (y) si es categ贸rica y el problema es de clasificaci贸n
            if st.session_state["problem_type"] == "Clasificaci贸n" and st.session_state["y_original"].dtype == "object":
                st.sidebar.subheader("Codificaci贸n de la Variable Objetivo (y)")
                if st.sidebar.button("Aplicar Codificaci贸n en y", key="apply_y_encoding_button"):
                    label_encoder = LabelEncoder()
                    st.session_state["y_original"] = label_encoder.fit_transform(st.session_state["y_original"])
                    st.session_state["label_encoder"] = label_encoder
                    st.sidebar.success("Codificaci贸n aplicada con 茅xito a la variable objetivo (y).")
                    st.sidebar.write("Vista previa de la variable objetivo codificada (y):", st.session_state["y_original"][:5])

        # Paso 6: Normalizaci贸n y escalamiento
        if st.session_state.get("categorical_encoded", False):
            st.sidebar.subheader("Paso 6: Normalizaci贸n y Escalamiento")
            numeric_columns = st.session_state["X_original"].select_dtypes(include=["float64", "int64"]).columns
            scaling_option = st.sidebar.selectbox(
                "M茅todo de Escalamiento:",
                ["StandardScaler", "MinMaxScaler"],
                key="scaling_option_selectbox"
            )
            if st.sidebar.button("Aplicar Escalamiento", key="apply_scaling_button"):
                numeric_columns = st.session_state["X_original"].select_dtypes(include=["float64", "int64"]).columns
                scaler = StandardScaler() if scaling_option == "StandardScaler" else MinMaxScaler()
                scaled_data = scaler.fit_transform(st.session_state["X_original"][numeric_columns])
                st.session_state["X_original"].loc[:, numeric_columns] = scaled_data
                st.session_state["scaling_done"] = True
                st.sidebar.success("Escalamiento aplicado con 茅xito.")
                st.sidebar.write("Vista previa del dataset escalado:", st.session_state["X_original"].head())

        # Validar que y_original no haya sido alterado
        if st.session_state["problem_type"] == "Clasificaci贸n" and st.session_state["y_original"].dtype not in [np.int32, np.int64, np.float32, np.float64]:
            st.write(st.session_state["y_original"].dtype)
            st.sidebar.error("La variable objetivo debe seguir siendo num茅rica despu茅s del escalamiento. Verifica el flujo.")
            st.stop()

        # Paso 7: Divisi贸n del dataset
        if st.session_state.get("scaling_done", False):
            st.sidebar.subheader("Paso 7: Divisi贸n del Dataset")
            
            split_type = st.sidebar.selectbox(
                "Tipo de Divisi贸n:",
                ["Entrenamiento y Prueba", "Entrenamiento, Validaci贸n y Prueba"],
                key="split_type_selectbox"
            )
            st.session_state["split_type"] = split_type
            train_ratio = st.sidebar.slider("Entrenamiento (%)", 10, 90, 70, key="train_ratio_slider")

            # Calcular proporciones para validaci贸n y prueba
            if split_type == "Entrenamiento, Validaci贸n y Prueba":
                val_ratio = st.sidebar.slider("Validaci贸n (%)", 5, 50, 15, key="val_ratio_slider")
                test_ratio = 100 - train_ratio - val_ratio
            else:
                val_ratio = 0
                test_ratio = 100 - train_ratio

            st.sidebar.text(f"Prueba (%): {test_ratio}")
            
            # Bot贸n para aplicar divisi贸n
            if st.sidebar.button("Aplicar Divisi贸n", key="apply_split_button") and not st.session_state.get("dataset_split", False):
                if split_type == "Entrenamiento y Prueba":
                    # Divisi贸n simple: Entrenamiento y Prueba
                    X_train, X_test, y_train, y_test = train_test_split(
                        st.session_state["X_original"], st.session_state["y_original"],
                        test_size=test_ratio / 100, random_state=42
                    )
                    # Guardar y_test_original antes de cualquier transformaci贸n
                    st.session_state['y_test_original'] = y_test.copy()
                    st.session_state["splits"] = (X_train, X_test, y_train, y_test)

                elif split_type == "Entrenamiento, Validaci贸n y Prueba":
                    # Divisi贸n avanzada: Entrenamiento, Validaci贸n y Prueba
                    X_train, X_temp, y_train, y_temp = train_test_split(
                        st.session_state["X_original"], st.session_state["y_original"],
                        test_size=(val_ratio + test_ratio) / 100, random_state=42
                    )
                    val_ratio_adjusted = val_ratio / (val_ratio + test_ratio)
                    X_val, X_test, y_val, y_test = train_test_split(
                        X_temp, y_temp, test_size=1 - val_ratio_adjusted, random_state=42
                    )
                    # Despu茅s de la divisi贸n

                    st.session_state['y_test_original'] = y_test.copy()
                    
                    st.session_state["splits"] = (X_train, X_val, X_test, y_train, y_val, y_test)
                    # Verificar dimensiones despu茅s de la divisi贸n
                    if len(X_test) != len(y_test):
                        st.error("Dimensiones inconsistentes entre X_test y y_test despu茅s de la divisi贸n.")



                # Verificaci贸n de dimensiones
                splits = st.session_state["splits"]
                
                if len(splits) == 4:
                    X_train, X_test, y_train, y_test = splits
                elif len(splits) == 6:
                    X_train, X_val, X_test, y_train, y_val, y_test = splits

                if len(X_test) != len(y_test):
                    st.error("Dimensiones inconsistentes entre X_test y y_test despu茅s de la divisi贸n.")
                    st.stop()

                # Guardar copia de y_test_original para referencia
                st.session_state["y_test_original"] = y_test.copy()

                st.session_state["dataset_split"] = True
                st.sidebar.success("Divisi贸n del dataset realizada con 茅xito.")


    # Mensaje final
    if st.session_state.get("dataset_split", False):
        st.sidebar.header("隆Configuraci贸n Completada!")
        st.sidebar.success("El dataset est谩 listo. Ahora puedes proceder a entrenar el modelo.")




# Funci贸n para manejar valores nulos
def handle_nulls(X, numeric_null_option, categorical_null_option):
    """
    Maneja los valores nulos en los datos para columnas num茅ricas y categ贸ricas.

    Args:
        X (pd.DataFrame): Datos de entrada.
        numeric_null_option (str): Opci贸n seleccionada para manejar valores nulos num茅ricos.
        categorical_null_option (str): Opci贸n seleccionada para manejar valores nulos categ贸ricos.

    Returns:
        pd.DataFrame: DataFrame con valores nulos gestionados.
    """
    from sklearn.impute import SimpleImputer

    # Copiar el DataFrame para no modificar el original
    X_cleaned = X.copy()

    # Manejo de columnas num茅ricas
    numeric_cols = X_cleaned.select_dtypes(include=['float64', 'int64']).columns
    if not numeric_cols.empty:
        if numeric_null_option == "Eliminar filas":
            X_cleaned.dropna(subset=numeric_cols, inplace=True)
        elif numeric_null_option == "Reemplazar con 0":
            X_cleaned[numeric_cols] = X_cleaned[numeric_cols].fillna(0)
        elif numeric_null_option == "Reemplazar con la media":
            imputer = SimpleImputer(strategy='mean')
            X_cleaned[numeric_cols] = imputer.fit_transform(X_cleaned[numeric_cols])
        elif numeric_null_option == "Reemplazar con la mediana":
            imputer = SimpleImputer(strategy='median')
            X_cleaned[numeric_cols] = imputer.fit_transform(X_cleaned[numeric_cols])
    else:
        st.sidebar.success("No hay valores nulos en las columnas num茅ricas.")

    # Manejo de columnas categ贸ricas
    categorical_cols = X_cleaned.select_dtypes(include=['object', 'category']).columns
    if not categorical_cols.empty:
        if categorical_null_option == "Eliminar filas":
            X_cleaned.dropna(subset=categorical_cols, inplace=True)
        elif categorical_null_option == "Reemplazar con 'Ninguno'":
            X_cleaned[categorical_cols] = X_cleaned[categorical_cols].fillna('Ninguno')
        elif categorical_null_option == "Reemplazar con la moda":
            imputer = SimpleImputer(strategy='most_frequent')
            X_cleaned[categorical_cols] = imputer.fit_transform(X_cleaned[categorical_cols])
    else:
        st.sidebar.success("No hay valores nulos en las columnas categ贸ricas.")

    return X_cleaned




# Funci贸n para normalizar y escalar datos num茅ricos
def normalize_and_scale(X, scaling_option):
    """
    Normaliza y escala las columnas num茅ricas de un DataFrame.
    """
    # Selecciona solo columnas num茅ricas
    numeric_cols = X.select_dtypes(include=np.number).columns
    if numeric_cols.empty:
        raise ValueError("No hay columnas num茅ricas para escalar.")

    # Selecciona el tipo de escalador
    scaler = StandardScaler() if scaling_option == "StandardScaler" else MinMaxScaler()
    
    # Aplica el escalamiento
    X_scaled = X.copy()
    X_scaled[numeric_cols] = scaler.fit_transform(X[numeric_cols])
    
    return X_scaled



# Funci贸n para codificar variables categ贸ricas
def encode_categorical(X, encoding_option):
    """
    Codifica las variables categ贸ricas seg煤n la opci贸n seleccionada: One-Hot Encoding o Label Encoding.

    Args:
        X (pd.DataFrame): Datos de entrada.
        encoding_option (str): M茅todo de codificaci贸n seleccionado ("One-Hot Encoding" o "Label Encoding").

    Returns:
        pd.DataFrame: DataFrame transformado con la codificaci贸n aplicada.
    """
    # Identificar columnas categ贸ricas
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns

    if not categorical_cols.empty:
        if encoding_option == "One-Hot Encoding":
            # Aplicar One-Hot Encoding
            X = pd.get_dummies(X, columns=categorical_cols)
        elif encoding_option == "Label Encoding":
            # Aplicar Label Encoding
            from sklearn.preprocessing import LabelEncoder
            label_encoder = LabelEncoder()
            for col in categorical_cols:
                X[col] = label_encoder.fit_transform(X[col].astype(str))
        else:
            raise ValueError(f"M茅todo de codificaci贸n no soportado: {encoding_option}")
    else:
        st.sidebar.success("No hay variables categ贸ricas en el dataset.")

    return X

# Configuraci贸n del Dataset
if tabs == "Dataset":
   # Configuraci贸n del Dataset
    configure_dataset()



# Visualizar ejemplos del dataset
elif tabs == "Capas":
    st.sidebar.subheader("Configuraci贸n de Capas")

    # Seleccionar arquitectura base
    st.sidebar.subheader("Arquitectura")
    selected_architecture = st.sidebar.selectbox(
        "Seleccione una Arquitectura",
        options=list(architectures[st.session_state['selected_dataset']].keys()),
        help="Elija la arquitectura del modelo basada en el dataset seleccionado."
    )

    # Aplicar la arquitectura seleccionada
    if st.sidebar.button("Aplicar Arquitectura"):
        # Obtener las capas de la arquitectura seleccionada
        selected_architecture_layers = architectures[st.session_state['selected_dataset']][selected_architecture]

        # Reiniciar las configuraciones de capas
        st.session_state['layer_config'] = selected_architecture_layers[:]
        st.session_state['num_layers_selected'] = len(selected_architecture_layers)

        # Validar tipos de capas seg煤n el dataset
        valid_types = ["Dense", "Dropout"] if st.session_state['selected_dataset'] in ['MNIST'] else [
            "Dense", "Dropout", "Conv2D", "MaxPooling2D", "Flatten", "BatchNormalization"
        ]

        for layer in st.session_state['layer_config']:
            if layer['type'] not in valid_types:
                st.warning(f"Tipo de capa no v谩lido detectado: {layer['type']}.")
                st.session_state['layer_config'] = []
                break

        st.sidebar.success(f"Arquitectura '{selected_architecture}' aplicada correctamente.")

    # Validar cambios externos en configuraciones
    if st.session_state['layer_config'] != st.session_state.get('previous_layer_config', []):
        st.session_state['previous_layer_config'] = st.session_state['layer_config'][:]

    # Mostrar configuraci贸n personalizada de capas
    st.sidebar.subheader("Capas Personalizadas")
    st.sidebar.number_input(
        "N煤mero de Capas",
        min_value=0,
        max_value=30,
        value=st.session_state['num_layers_selected'],
        step=1,
        key='num_layers_selected',
        help="N煤mero total de capas que desea incluir en su modelo.",
        on_change=update_layer_config
    )

    # Validar si la configuraci贸n actual de capas es v谩lida
    valid_types = ["Dense", "Dropout", "Conv2D", "MaxPooling2D", "Flatten", "BatchNormalization"]
    invalid_layers = any(layer.get("type") not in valid_types for layer in st.session_state['layer_config'])

    if invalid_layers:
        st.session_state['layer_config'] = []
        st.warning("La configuraci贸n previa de capas no es v谩lida para el dataset seleccionado. Las capas se han reiniciado.")

    # Configuraci贸n de cada capa
    for i, layer in enumerate(st.session_state['layer_config']):
        with st.sidebar.expander(f"Configuraci贸n de la Capa {i + 1}", expanded=True):
            layer['type'] = st.selectbox(
                "Tipo de Capa",
                valid_types,
                index=valid_types.index(layer.get("type", "Dense")),
                key=f"layer_type_{i}",
                help="Seleccione el tipo de capa que desea a帽adir."
            )

            # Configuraci贸n espec铆fica para cada tipo de capa
            if layer['type'] == "Dense":
                layer['neurons'] = st.number_input(
                    "N煤mero de Neuronas",
                    min_value=1,
                    value=layer.get('neurons', 64),
                    key=f"neurons_{i}",
                    help="N煤mero de neuronas en la capa densa."
                )
                layer['activation'] = st.selectbox(
                    "Activaci贸n",
                    ["relu", "sigmoid", "tanh", "softmax","linear"],
                    index=["relu", "sigmoid", "tanh", "softmax","linear"].index(layer.get('activation', "relu")),
                    key=f"activation_{i}",
                    help="Funci贸n de activaci贸n para la capa densa."
                )
                # Checkbox para habilitar regularizaci贸n L2
                layer['enable_l2'] = st.checkbox(
                    "Habilitar Regularizaci贸n L2",
                    value=layer.get('enable_l2', False),
                    key=f"enable_l2_{i}",
                    help="Activa la regularizaci贸n L2 para esta capa."
                )
                if layer['enable_l2']:
                    # Slider para definir el coeficiente L2 si est谩 habilitado
                    layer['l2'] = st.slider(
                        "Regularizaci贸n L2",
                        min_value=0.0,
                        max_value=0.1,
                        value=layer.get('l2', 0.01),
                        step=0.01,
                        key=f"l2_{i}",
                        help="Coeficiente de regularizaci贸n L2 para los pesos de la capa densa."
                    )
            elif layer['type'] == "Dropout":
                layer['dropout_rate'] = st.slider(
                    "Tasa de Dropout",
                    0.0,
                    0.9,
                    value=layer.get('dropout_rate', 0.2),
                    step=0.1,
                    key=f"dropout_{i}",
                    help="Proporci贸n de unidades que se desactivar谩n aleatoriamente en esta capa."
                )
            elif layer['type'] == "Conv2D":
                layer['filters'] = st.number_input(
                    "N煤mero de Filtros",
                    min_value=1,
                    value=layer.get('filters', 32),
                    key=f"filters_{i}",
                    help="Define cu谩ntos filtros utilizar谩 esta capa de convoluci贸n."
                )
                layer['kernel_size'] = st.slider(
                    "Tama帽o del Kernel",
                    min_value=1,
                    max_value=5,
                    value=layer.get('kernel_size', 3),
                    step=1,
                    key=f"kernel_{i}",
                    help="Tama帽o del filtro o kernel que se mover谩 sobre la entrada."
                )
                layer['activation'] = st.selectbox(
                    "Activaci贸n",
                    ["relu", "sigmoid", "tanh", "softmax","linear"],
                    index=["relu", "sigmoid", "tanh", "softmax","linear"].index(layer.get('activation', "relu")),
                    key=f"activation_conv_{i}",
                    help="Funci贸n de activaci贸n para la capa de convoluci贸n."
                )
            elif layer['type'] == "MaxPooling2D":
                layer['pool_size'] = st.slider(
                    "Tama帽o del Pool",
                    min_value=1,
                    max_value=5,
                    value=layer.get('pool_size', 2),
                    step=1,
                    key=f"pool_size_{i}",
                    help="Tama帽o de la ventana para la operaci贸n de pooling."
                )
            elif layer['type'] == "Flatten":
                st.info("Capa que aplana la entrada para conectarla con capas densas.")
            elif layer['type'] == "BatchNormalization":
                st.info("Capa para normalizar las salidas de la capa anterior y estabilizar el entrenamiento.")

            # Actualizar el nombre de la capa
            layer['name'] = f"{layer['type']}{i + 1}"




# Configurar hiperpar谩metros
elif tabs == "Hiperpar谩metros":
    st.sidebar.subheader("Hiperpar谩metros")
    st.session_state['hyperparams']['optimizer'] = st.sidebar.selectbox(
        "Optimizador",
        ["Adam", "SGD", "RMSprop"],
        help="Elija el optimizador que se utilizar谩 para ajustar los pesos del modelo."
    )
    st.session_state['hyperparams']['learning_rate'] = st.sidebar.slider(
        "Tasa de Aprendizaje",
        min_value=0.0001,
        max_value=0.01,
        value=0.001,
        step=0.0001,
        format="%.4g",  # Mostrar hasta 4 decimales
        help="Velocidad con la que el modelo ajusta sus pesos durante el entrenamiento."
    )
    st.session_state['hyperparams']['epochs'] = st.sidebar.number_input(
        "N煤mero de pocas",
        min_value=1,
        max_value=50,
        value=5,
        step=1,
        help="N煤mero de veces que el modelo ver谩 todo el conjunto de datos durante el entrenamiento."
    )
    st.session_state['hyperparams']['batch_size'] = st.sidebar.number_input(
        "Tama帽o de Batch",
        min_value=8,
        max_value=128,
        value=32,
        step=8,
        help="Cantidad de muestras procesadas antes de actualizar los pesos del modelo."
    )
    st.session_state['hyperparams']['loss_function'] = st.sidebar.selectbox(
        "Funci贸n de P茅rdida",
        ["categorical_crossentropy", "mean_squared_error"],
        help="M茅trica utilizada para evaluar qu茅 tan bien se est谩 entrenando el modelo."
    )


# Configuraci贸n de Early Stopping
elif tabs == "Early Stopping":
    st.sidebar.subheader("Early Stopping")
    enable_early_stopping = st.sidebar.checkbox(
        "Habilitar Early Stopping",
        value=False,
        help="Detiene el entrenamiento si no hay mejora en la m茅trica monitoreada despu茅s de un n煤mero determinado de 茅pocas."
    )
    if enable_early_stopping:
        patience = st.sidebar.number_input(
            "Patience (N煤mero de 茅pocas sin mejora)",
            min_value=1,
            max_value=20,
            value=3,
            step=1,
            help="N煤mero de 茅pocas sin mejora antes de detener el entrenamiento."
        )
        monitor_metric = st.sidebar.selectbox(
            "M茅trica a Monitorear",
            ["val_loss", "val_accuracy"],
            index=0,
            help="M茅trica que se monitorear谩 para decidir si detener el entrenamiento."
        )



# Checkbox para mostrar/ocultar m茅tricas (siempre disponible)
st.session_state['show_metrics'] = st.checkbox(
    "Mostrar Gr谩ficos de M茅tricas",
    value=st.session_state.get('show_metrics', False),
    disabled=st.session_state['training_in_progress']  # Deshabilitar si est谩 entrenando
)


# Funci贸n para resetear el estado de entrenamiento
def reset_training_state():
    st.session_state['training_in_progress'] = True
    st.session_state['training_finished'] = False
    st.session_state['logs'] = []  # Limpiar logs solo al comenzar nuevo entrenamiento
    st.session_state['modelDownload'] = None  # Limpiar modelo previo
    dynamic_placeholder.empty()  # Limpiar placeholders
    preview_placeholder.empty()


# Mostrar preview solo si no est谩 entrenando
if not st.session_state['training_in_progress']:
    preview_graph(st.session_state['layer_config'], preview_placeholder)

# Bot贸n para iniciar el entrenamiento
if not st.session_state['training_in_progress'] and not st.session_state['training_finished']:
    if st.button("Comenzar entrenamiento"):
        reset_training_state()  # Resetear estados antes de iniciar
        st.rerun()

# Bot贸n para cancelar el entrenamiento
if st.session_state['training_in_progress']:
    col_cancel, col_info = st.columns([1, 3])
    with col_cancel:
        if st.button("Cancelar Entrenamiento"):
            st.session_state['training_in_progress'] = False
            st.session_state['training_finished'] = False
            st.session_state['logs'].append("Entrenamiento cancelado por el usuario.")
            dynamic_placeholder.text("Entrenamiento cancelado.")
            st.rerun()

# Mostrar progreso del entrenamiento
if st.session_state['training_in_progress']:
    st.write("Tipo de problema seleccionado:", st.session_state.get('problem_type'))
    if st.session_state.get('problem_type') == "Clasificaci贸n":
        train_model_classification(
            st.session_state['layer_config'],
            st.session_state['hyperparams'],
            preview_placeholder,
            dynamic_placeholder
        )
    elif st.session_state.get('problem_type') == "Regresi贸n":
        train_model_regression(
            st.session_state['layer_config'],
            st.session_state['hyperparams'],
            preview_placeholder,
            dynamic_placeholder
        )
    else:
        st.error("El tipo de problema no est谩 configurado correctamente.")
        st.stop()

    st.session_state['training_in_progress'] = False
    st.session_state['training_finished'] = True
    st.rerun()


# Mostrar el bot贸n de guardar modelo una vez terminado el entrenamiento
if st.session_state['training_finished']:
    st.text_area(
        "Logs del Entrenamiento",
        "\n".join(st.session_state['logs']),
        height=300,
        key="final_logs"
    )

    # Bot贸n para guardar el modelo
    if st.button("Guardar Modelo"):
        st.session_state['modelDownload'].save("trained_model.h5")
        with open("trained_model.h5", "rb") as file:
            st.download_button("Descargar Modelo", file, file_name="trained_model.h5")

    # Mostrar gr谩ficos detallados de m茅tricas bajo un checkbox
    if st.checkbox("Mostrar Gr谩ficos de M茅tricas finales", key="show_final_metrics"):
        col1, col2 = st.columns(2)  # Dividir gr谩ficos en columnas para mejor organizaci贸n

        # Gr谩fico de p茅rdida
        with col1:
            st.write("#### P茅rdida por poca")
            fig_loss = go.Figure()
            fig_loss.add_trace(go.Scatter(
                x=list(range(1, len(st.session_state['loss_values']) + 1)),
                y=st.session_state['loss_values'],
                mode='lines+markers',
                name="P茅rdida de Entrenamiento",
                line=dict(color='blue')
            ))
            if st.session_state['val_loss_values']:
                fig_loss.add_trace(go.Scatter(
                    x=list(range(1, len(st.session_state['val_loss_values']) + 1)),
                    y=st.session_state['val_loss_values'],
                    mode='lines+markers',
                    name="P茅rdida de Validaci贸n",
                    line=dict(color='red', dash='dash')
                ))
            fig_loss.update_layout(
                title="P茅rdida por poca",
                xaxis_title="poca",
                yaxis_title="P茅rdida",
                legend_title="Tipo"
            )
            st.plotly_chart(fig_loss, use_container_width=True)

        # Gr谩fico de precisi贸n
        with col2:
            if st.session_state['accuracy_values']:
                st.write("#### Precisi贸n por poca")
                fig_acc = go.Figure()
                fig_acc.add_trace(go.Scatter(
                    x=list(range(1, len(st.session_state['accuracy_values']) + 1)),
                    y=st.session_state['accuracy_values'],
                    mode='lines+markers',
                    name="Precisi贸n de Entrenamiento",
                    line=dict(color='green')
                ))
                if st.session_state['val_accuracy_values']:
                    fig_acc.add_trace(go.Scatter(
                        x=list(range(1, len(st.session_state['val_accuracy_values']) + 1)),
                        y=st.session_state['val_accuracy_values'],
                        mode='lines+markers',
                        name="Precisi贸n de Validaci贸n",
                        line=dict(color='orange', dash='dash')
                    ))
                fig_acc.update_layout(
                    title="Precisi贸n por poca",
                    xaxis_title="poca",
                    yaxis_title="Precisi贸n",
                    legend_title="Tipo"
                )
                st.plotly_chart(fig_acc, use_container_width=True)

        # M茅tricas finales seg煤n el tipo de problema
        st.write("#### M茅tricas Finales")
        if st.session_state['problem_type'] == "Clasificaci贸n":
            # Verificar que el modelo exista
            if "modelDownload" not in st.session_state or st.session_state["modelDownload"] is None:
                st.error("El modelo no est谩 entrenado. Por favor, entrena el modelo antes de intentar predecir.")
                st.stop()

            # Cargar el modelo desde session_state
            model = st.session_state["modelDownload"]

            # Datos de prueba
            split_type = st.session_state["split_type"]

            if split_type == "Entrenamiento y Prueba":
                X_test = st.session_state["splits"][1]
                y_test_original = st.session_state["splits"][3]
            elif split_type == "Entrenamiento, Validaci贸n y Prueba":
                X_test = st.session_state["splits"][2]
                y_test_original = st.session_state["splits"][5]
            else:
                st.error("Tipo de divisi贸n desconocido. Revisa la configuraci贸n.")
                st.stop()

            # Predicci贸n
            predictions = model.predict(X_test)
            y_pred = np.argmax(predictions, axis=1)

            # Validar dimensiones despu茅s de predecir
            if len(y_pred) != len(y_test_original):
                st.error(f"Dimensiones inconsistentes: y_pred tiene {len(y_pred)} muestras, pero y_test_original tiene {len(y_test_original)}.")
                st.stop()

            # Calcular m茅tricas
            f1 = f1_score(y_test_original, y_pred, average='weighted')
            precision = precision_score(y_test_original, y_pred, average='weighted')
            recall = recall_score(y_test_original, y_pred, average='weighted')

            # Visualizaci贸n de la matriz de confusi贸n
            cm = confusion_matrix(y_test_original, y_pred)
            st.write("Matriz de confusi贸n:")
            fig, ax = plt.subplots()
            disp = ConfusionMatrixDisplay(confusion_matrix=cm)
            disp.plot(ax=ax)
            st.pyplot(fig)

            # Comparar predicciones con etiquetas reales
            st.write("Comparaci贸n de predicciones individuales:")
            for i in range(min(5, len(y_test_original))):  # Mostrar m谩ximo 5 ejemplos
                st.write(f"Ejemplo {i+1}: Real: {y_test_original[i]}, Predicci贸n: {y_pred[i]}")

            # Mostrar m茅tricas
            st.write(f"**F1 Score:** {f1:.4f}")
            st.write(f"**Precisi贸n (Precision):** {precision:.4f}")
            st.write(f"**Recall:** {recall:.4f}")

        else:  # Para regresi贸n
            final_loss = st.session_state['loss_values'][-1]
            st.write(f"**P茅rdida Final (MAE):** {final_loss:.4f}")
            if st.session_state['val_loss_values']:
                final_val_loss = st.session_state['val_loss_values'][-1]
                st.write(f"**P茅rdida Validaci贸n Final (MAE):** {final_val_loss:.4f}")

            # Gr谩fico de m茅tricas finales (regresi贸n)
            fig_metrics = go.Figure()
            fig_metrics.add_trace(go.Bar(name="P茅rdida Final", x=["P茅rdida"], y=[final_loss], marker_color='blue'))
            if 'val_loss_values' in st.session_state and st.session_state['val_loss_values']:
                fig_metrics.add_trace(go.Bar(name="P茅rdida Validaci贸n", x=["P茅rdida Validaci贸n"], y=[final_val_loss], marker_color='red'))
            fig_metrics.update_layout(title="P茅rdidas Finales", barmode="group")
            st.plotly_chart(fig_metrics, use_container_width=True)

    # Bot贸n para reiniciar el entrenamiento
    if st.button("Comenzar Entrenamiento"):
        reset_training_state()
        st.rerun()
